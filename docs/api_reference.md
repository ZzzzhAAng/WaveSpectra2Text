# API å‚è€ƒæ–‡æ¡£

## ğŸ“š æ ¸å¿ƒæ¨¡å— API

### æ¨¡å‹åˆ›å»º

#### `create_model()`
åˆ›å»ºTransformeråºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚

```python
from wavespectra2text import create_model

model = create_model(
    vocab_size=14,           # è¯æ±‡è¡¨å¤§å°
    input_dim=513,           # è¾“å…¥ç‰¹å¾ç»´åº¦
    hidden_dim=256,          # éšè—å±‚ç»´åº¦
    encoder_layers=4,        # ç¼–ç å™¨å±‚æ•°
    decoder_layers=4,        # è§£ç å™¨å±‚æ•°
    dropout=0.1,             # Dropoutæ¯”ç‡
    device='cpu'             # è®¡ç®—è®¾å¤‡
)
```

**å‚æ•°:**
- `vocab_size` (int): è¯æ±‡è¡¨å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨vocab.vocab_size
- `input_dim` (int): è¾“å…¥é¢‘è°±ç‰¹å¾ç»´åº¦ï¼Œé»˜è®¤513
- `hidden_dim` (int): Transformeréšè—å±‚ç»´åº¦ï¼Œé»˜è®¤256
- `encoder_layers` (int): ç¼–ç å™¨å±‚æ•°ï¼Œé»˜è®¤4
- `decoder_layers` (int): è§£ç å™¨å±‚æ•°ï¼Œé»˜è®¤4
- `dropout` (float): Dropoutæ¯”ç‡ï¼Œé»˜è®¤0.1
- `device` (str): è®¡ç®—è®¾å¤‡ï¼Œé»˜è®¤'cpu'

**è¿”å›:**
- `Seq2SeqModel`: é…ç½®å¥½çš„æ¨¡å‹å®ä¾‹

### è¯æ±‡è¡¨ç®¡ç†

#### `vocab`
å…¨å±€è¯æ±‡è¡¨å®ä¾‹ï¼ŒåŒ…å«ä¸­æ–‡æ•°å­—1-10å’Œç‰¹æ®Šç¬¦å·ã€‚

```python
from wavespectra2text import vocab

# è·å–è¯æ±‡è¡¨å¤§å°
size = vocab.vocab_size  # 14

# ç¼–ç æ–‡æœ¬ä¸ºç´¢å¼•
indices = vocab.encode("ä¸€")  # [1, 2] (SOS + ä¸€ + EOS)

# è§£ç ç´¢å¼•ä¸ºæ–‡æœ¬
text = vocab.decode([1, 2, 3])  # "ä¸€"

# è·å–ç‰¹æ®Šç¬¦å·ç´¢å¼•
pad_idx = vocab.get_padding_idx()  # 0
sos_idx = vocab.get_sos_idx()      # 1
eos_idx = vocab.get_eos_idx()      # 2
unk_idx = vocab.get_unk_idx()      # 3
```

### æ¨ç†æ ¸å¿ƒ

#### `InferenceCore`
ç»Ÿä¸€çš„æ¨ç†æ ¸å¿ƒç±»ï¼Œæä¾›æ¨¡å‹åŠ è½½å’Œæ¨ç†åŠŸèƒ½ã€‚

```python
from wavespectra2text import InferenceCore

# åˆ›å»ºæ¨ç†æ ¸å¿ƒ
core = InferenceCore('checkpoints/best_model.pth', device='cpu')

# ä»éŸ³é¢‘æ–‡ä»¶æ¨ç†
result = core.infer_from_audio('audio.wav', method='auto')

# ä»é¢‘è°±ç‰¹å¾æ¨ç†
result = core.infer_from_spectrogram(spectrogram_array, method='beam')

# è·å–æ¨¡å‹ä¿¡æ¯
info = core.get_model_info()
```

**æ–¹æ³•:**
- `infer_from_audio(audio_path, method='auto', beam_size=3)`: ä»éŸ³é¢‘æ–‡ä»¶æ¨ç†
- `infer_from_spectrogram(spectrogram, method='auto', beam_size=3)`: ä»é¢‘è°±ç‰¹å¾æ¨ç†
- `get_model_info()`: è·å–æ¨¡å‹ä¿¡æ¯

## ğŸ¯ æ¨ç†æ¨¡å— API

### åŒè¾“å…¥è¯†åˆ«å™¨

#### `DualInputSpeechRecognizer`
æ”¯æŒéŸ³é¢‘å’Œé¢‘è°±ä¸¤ç§è¾“å…¥æ¨¡å¼çš„è¯­éŸ³è¯†åˆ«å™¨ã€‚

```python
from wavespectra2text import DualInputSpeechRecognizer

# åˆ›å»ºè¯†åˆ«å™¨
recognizer = DualInputSpeechRecognizer('checkpoints/best_model.pth', device='cpu')

# éŸ³é¢‘è¯†åˆ«
result = recognizer.recognize_from_audio('audio.wav', show_details=True)

# é¢‘è°±è¯†åˆ«
result = recognizer.recognize_from_spectrogram('spectrogram.npy', show_details=True)

# å†…å­˜æ•°ç»„è¯†åˆ«
result = recognizer.recognize_from_spectrogram_array(spectrogram_array)

# è‡ªåŠ¨æ¨¡å¼
result = recognizer.auto_recognize('input_file')
```

**æ–¹æ³•:**
- `recognize_from_audio(audio_path, show_details=True)`: ä»éŸ³é¢‘æ–‡ä»¶è¯†åˆ«
- `recognize_from_spectrogram(spectrogram_path, show_details=True)`: ä»é¢‘è°±æ–‡ä»¶è¯†åˆ«
- `recognize_from_spectrogram_array(spectrogram_array, show_details=True)`: ä»å†…å­˜æ•°ç»„è¯†åˆ«
- `auto_recognize(input_path, show_details=True)`: è‡ªåŠ¨è¯†åˆ«è¾“å…¥ç±»å‹

**è¿”å›ç»“æœæ ¼å¼:**
```python
{
    'text': 'è¯†åˆ«ç»“æœæ–‡æœ¬',
    'success': True,
    'processing_time': {
        'preprocessing': 2.5,  # é¢„å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
        'inference': 0.3,      # æ¨ç†æ—¶é—´ï¼ˆç§’ï¼‰
        'total': 2.8           # æ€»æ—¶é—´ï¼ˆç§’ï¼‰
    },
    'input_type': 'audio_file',
    'spectrogram_shape': (200, 513),
    'method': 'beam_search',
    'mode': 'audio',
    'error': None
}
```

## ğŸ‹ï¸ è®­ç»ƒæ¨¡å— API

### è®­ç»ƒå™¨åˆ›å»º

#### `create_trainer()`
åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹ã€‚

```python
from wavespectra2text import create_trainer

trainer = create_trainer(
    trainer_type='improved',  # 'simple', 'improved', 'large'
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    device=device,
    config=config
)

# å¼€å§‹è®­ç»ƒ
trainer.train(num_epochs=50, resume_path=None)
```

**è®­ç»ƒå™¨ç±»å‹:**
- `'simple'`: å°æ•°æ®é›†è®­ç»ƒå™¨ï¼Œé€‚åˆ1-50æ ·æœ¬
- `'improved'`: ä¸­ç­‰æ•°æ®é›†è®­ç»ƒå™¨ï¼Œé€‚åˆ50-200æ ·æœ¬
- `'large'`: å¤§æ•°æ®é›†è®­ç»ƒå™¨ï¼Œé€‚åˆ200+æ ·æœ¬

### é…ç½®ç®¡ç†

#### `get_default_config()`
è·å–ä¸åŒè§„æ¨¡çš„é»˜è®¤é…ç½®ã€‚

```python
from wavespectra2text.training.config import get_default_config

# è·å–ä¸åŒè§„æ¨¡é…ç½®
small_config = get_default_config('small')
medium_config = get_default_config('medium')
large_config = get_default_config('large')
xlarge_config = get_default_config('xlarge')
```

**é…ç½®è§„æ¨¡:**
- `'small'`: å°æ•°æ®é›†é…ç½® (batch_size=1, hidden_dim=64)
- `'medium'`: ä¸­ç­‰æ•°æ®é›†é…ç½® (batch_size=2, hidden_dim=128)
- `'large'`: å¤§æ•°æ®é›†é…ç½® (batch_size=4, hidden_dim=256)
- `'xlarge'`: è¶…å¤§æ•°æ®é›†é…ç½® (batch_size=8, hidden_dim=512)

## ğŸ“Š æ•°æ®å¤„ç† API

### æ•°æ®é›†åˆ›å»º

#### `AudioDataset`
æ”¯æŒå®æ—¶è®¡ç®—å’Œé¢„è®¡ç®—ä¸¤ç§æ¨¡å¼çš„éŸ³é¢‘æ•°æ®é›†ã€‚

```python
from wavespectra2text import AudioDataset

# å®æ—¶è®¡ç®—æ¨¡å¼
dataset = AudioDataset(
    labels_file='data/labels.csv',
    audio_dir='data/audio',
    mode='realtime'
)

# é¢„è®¡ç®—æ¨¡å¼
dataset = AudioDataset(
    labels_file='data/labels.csv',
    precomputed_dir='data/features',
    mode='precomputed'
)
```

### é¢„å¤„ç†å™¨

#### `PreprocessorFactory`
é¢„å¤„ç†å™¨å·¥å‚ï¼Œæ”¯æŒå¤šç§é¢„å¤„ç†ç­–ç•¥ã€‚

```python
from wavespectra2text import PreprocessorFactory

# åˆ›å»ºSTFTé¢‘è°±é¢„å¤„ç†å™¨
preprocessor = PreprocessorFactory.create(
    'spectrogram',
    sample_rate=48000,
    n_fft=1024,
    hop_length=512,
    max_length=200
)

# åˆ›å»ºMelé¢‘è°±é¢„å¤„ç†å™¨
mel_preprocessor = PreprocessorFactory.create(
    'mel_spectrogram',
    sample_rate=48000,
    n_fft=1024,
    hop_length=512,
    n_mels=128,
    max_length=200
)

# å¤„ç†éŸ³é¢‘æ–‡ä»¶
features = preprocessor.process('audio.wav')

# è·å–ç‰¹å¾å½¢çŠ¶
shape = preprocessor.get_feature_shape()
```

**å¯ç”¨é¢„å¤„ç†å™¨:**
- `'spectrogram'`: STFTé¢‘è°±é¢„å¤„ç†å™¨
- `'mel_spectrogram'`: Melé¢‘è°±é¢„å¤„ç†å™¨

### æ•°æ®å·¥å…·

#### `AudioProcessor`
éŸ³é¢‘å¤„ç†å·¥å…·ç±»ã€‚

```python
from wavespectra2text import AudioProcessor

processor = AudioProcessor(
    sample_rate=48000,
    n_fft=1024,
    hop_length=512,
    max_length=200
)

# æå–é¢‘è°±ç‰¹å¾
spectrogram = processor.extract_spectrogram('audio.wav')
```

## ğŸ”§ å·¥å…·æ¨¡å— API

### æ–‡ä»¶å·¥å…·

#### `FileUtils`
æ–‡ä»¶æ“ä½œå·¥å…·ç±»ã€‚

```python
from wavespectra2text import FileUtils

# æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
exists = FileUtils.file_exists('path/to/file')

# åˆ›å»ºç›®å½•
FileUtils.create_dir('path/to/dir')

# è·å–æ–‡ä»¶æ‰©å±•å
ext = FileUtils.get_file_extension('file.wav')
```

### æ ‡ç­¾ç®¡ç†

#### `LabelManager`
æ ‡ç­¾ç®¡ç†å·¥å…·ç±»ã€‚

```python
from wavespectra2text import LabelManager

manager = LabelManager('data/labels.csv')

# è·å–æ‰€æœ‰æ ‡ç­¾
labels = manager.get_all_labels()

# éªŒè¯æ ‡ç­¾æ–‡ä»¶
is_valid = manager.validate_labels()

# æ›´æ–°æ ‡ç­¾
manager.update_labels(new_labels)
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹

```python
from wavespectra2text import (
    create_model, vocab, create_trainer, AudioDataset,
    get_default_config
)
from torch.utils.data import DataLoader

# 1. åŠ è½½é…ç½®
config = get_default_config('medium')

# 2. åˆ›å»ºæ•°æ®é›†
dataset = AudioDataset(
    labels_file='data/labels.csv',
    audio_dir='data/audio',
    mode='realtime'
)

# 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)
val_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=False)

# 4. åˆ›å»ºæ¨¡å‹
model = create_model(
    vocab_size=vocab.vocab_size,
    hidden_dim=config['hidden_dim'],
    encoder_layers=config['encoder_layers'],
    decoder_layers=config['decoder_layers'],
    dropout=config['dropout']
)

# 5. åˆ›å»ºè®­ç»ƒå™¨
trainer = create_trainer('improved', model, train_loader, val_loader, 'cpu', config)

# 6. å¼€å§‹è®­ç»ƒ
trainer.train(config['num_epochs'])
```

### å®Œæ•´æ¨ç†æµç¨‹

```python
from wavespectra2text import DualInputSpeechRecognizer

# 1. åˆ›å»ºè¯†åˆ«å™¨
recognizer = DualInputSpeechRecognizer('checkpoints/best_model.pth')

# 2. éŸ³é¢‘è¯†åˆ«
audio_result = recognizer.recognize_from_audio('data/audio/test.wav')
print(f"éŸ³é¢‘è¯†åˆ«ç»“æœ: {audio_result['text']}")

# 3. é¢‘è°±è¯†åˆ«
spectrogram_result = recognizer.recognize_from_spectrogram('data/features/test.npy')
print(f"é¢‘è°±è¯†åˆ«ç»“æœ: {spectrogram_result['text']}")

# 4. è‡ªåŠ¨æ¨¡å¼
auto_result = recognizer.auto_recognize('data/audio/test.wav')
print(f"è‡ªåŠ¨è¯†åˆ«ç»“æœ: {auto_result['text']}")
```

## ğŸš¨ é”™è¯¯å¤„ç†

### å¸¸è§å¼‚å¸¸

- `FileNotFoundError`: æ–‡ä»¶ä¸å­˜åœ¨
- `ValueError`: å‚æ•°å€¼é”™è¯¯
- `RuntimeError`: è¿è¡Œæ—¶é”™è¯¯
- `ImportError`: æ¨¡å—å¯¼å…¥é”™è¯¯

### é”™è¯¯å¤„ç†ç¤ºä¾‹

```python
try:
    recognizer = DualInputSpeechRecognizer('checkpoints/best_model.pth')
    result = recognizer.recognize_from_audio('audio.wav')
    
    if result['success']:
        print(f"è¯†åˆ«æˆåŠŸ: {result['text']}")
    else:
        print(f"è¯†åˆ«å¤±è´¥: {result['error']}")
        
except FileNotFoundError as e:
    print(f"æ–‡ä»¶ä¸å­˜åœ¨: {e}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```
