# ç”¨æˆ·æŒ‡å—

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ¬¡ä½¿ç”¨

#### 1. å‡†å¤‡æ•°æ®
```bash
# å°†éŸ³é¢‘æ–‡ä»¶æ”¾å…¥data/audioç›®å½•
cp your_audio_files/*.wav data/audio/

# è¿è¡Œæ•°æ®è®¾ç½®è„šæœ¬
python scripts/setup_data.py
```

#### 2. ç¼–è¾‘æ ‡ç­¾æ–‡ä»¶
ç¼–è¾‘ `data/labels.csv` æ–‡ä»¶ï¼Œç¡®ä¿æ ‡ç­¾æ­£ç¡®ï¼š
```csv
filename,label
Chinese_Number_01.wav,ä¸€
Chinese_Number_02.wav,äºŒ
Chinese_Number_03.wav,ä¸‰
...
```

#### 3. å¼€å§‹è®­ç»ƒ
```bash
# å°æ•°æ®é›†è®­ç»ƒï¼ˆæ¨èæ–°æ‰‹ï¼‰
python scripts/train.py --scale small

# æŸ¥çœ‹è®­ç»ƒè¿›åº¦
tensorboard --logdir runs
```

#### 4. æµ‹è¯•è¯†åˆ«
```bash
# æµ‹è¯•éŸ³é¢‘è¯†åˆ«
python scripts/inference.py --model checkpoints/best_model.pth --input data/audio/Chinese_Number_01.wav

# æµ‹è¯•é¢‘è°±è¯†åˆ«ï¼ˆæ›´å¿«ï¼‰
python scripts/inference.py --model checkpoints/best_model.pth --input data/features/Chinese_Number_01.npy
```

## ğŸ“Š è®­ç»ƒæŒ‡å—

### é€‰æ‹©åˆé€‚çš„è®­ç»ƒè§„æ¨¡

| æ•°æ®é‡ | æ¨èè§„æ¨¡ | é…ç½®æ–‡ä»¶ | è®­ç»ƒæ—¶é—´ | å†…å­˜éœ€æ±‚ |
|--------|----------|----------|----------|----------|
| 1-50æ ·æœ¬ | `small` | `configs/small_dataset.yaml` | 5-15åˆ†é’Ÿ | 2GB |
| 50-200æ ·æœ¬ | `medium` | `configs/medium_dataset.yaml` | 15-30åˆ†é’Ÿ | 4GB |
| 200-1000æ ·æœ¬ | `large` | `configs/large_dataset.yaml` | 30-60åˆ†é’Ÿ | 8GB |
| 1000+æ ·æœ¬ | `xlarge` | `configs/xlarge_dataset.yaml` | 1-3å°æ—¶ | 16GB |

### è®­ç»ƒå‚æ•°è°ƒä¼˜

#### å°æ•°æ®é›†ä¼˜åŒ–
```bash
# ä½¿ç”¨å°æ•°æ®é›†é…ç½®
python scripts/train.py --config configs/small_dataset.yaml

# å…³é”®å‚æ•°ï¼š
# - batch_size: 1 (é¿å…è¿‡æ‹Ÿåˆ)
# - learning_rate: 1e-5 (å°å­¦ä¹ ç‡)
# - hidden_dim: 64 (å°æ¨¡å‹)
# - dropout: 0.5 (é«˜æ­£åˆ™åŒ–)
```

#### å¤§æ•°æ®é›†ä¼˜åŒ–
```bash
# ä½¿ç”¨å¤§æ•°æ®é›†é…ç½®
python scripts/train.py --config configs/large_dataset.yaml

# å…³é”®å‚æ•°ï¼š
# - batch_size: 4-8 (å¤§æ‰¹æ¬¡)
# - learning_rate: 1e-4 (æ­£å¸¸å­¦ä¹ ç‡)
# - hidden_dim: 256-512 (å¤§æ¨¡å‹)
# - dropout: 0.1-0.2 (ä½æ­£åˆ™åŒ–)
```

### è®­ç»ƒç›‘æ§

#### ä½¿ç”¨TensorBoard
```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir runs

# åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:6006
```

#### ç›‘æ§æŒ‡æ ‡
- **è®­ç»ƒæŸå¤±**: åº”è¯¥é€æ¸ä¸‹é™
- **éªŒè¯æŸå¤±**: åº”è¯¥ä¸è®­ç»ƒæŸå¤±åŒæ­¥ä¸‹é™
- **å­¦ä¹ ç‡**: æ ¹æ®è°ƒåº¦å™¨å˜åŒ–
- **å‡†ç¡®ç‡**: åº”è¯¥é€æ¸æå‡

### è®­ç»ƒæŠ€å·§

#### 1. æ•°æ®è´¨é‡
- ç¡®ä¿éŸ³é¢‘è´¨é‡è‰¯å¥½
- æ ‡ç­¾å‡†ç¡®æ— è¯¯
- æ•°æ®å¹³è¡¡ï¼ˆå„ç±»åˆ«æ ·æœ¬æ•°é‡ç›¸è¿‘ï¼‰

#### 2. è¿‡æ‹Ÿåˆå¤„ç†
```bash
# å¢åŠ dropout
# ç¼–è¾‘é…ç½®æ–‡ä»¶
dropout: 0.5  # ä»0.1å¢åŠ åˆ°0.5

# å‡å°‘æ¨¡å‹å¤§å°
hidden_dim: 64  # ä»256å‡å°‘åˆ°64
encoder_layers: 1  # ä»4å‡å°‘åˆ°1
decoder_layers: 1  # ä»4å‡å°‘åˆ°1
```

#### 3. æ¬ æ‹Ÿåˆå¤„ç†
```bash
# å‡å°‘dropout
dropout: 0.1  # ä»0.5å‡å°‘åˆ°0.1

# å¢åŠ æ¨¡å‹å¤§å°
hidden_dim: 256  # ä»64å¢åŠ åˆ°256
encoder_layers: 4  # ä»1å¢åŠ åˆ°4
decoder_layers: 4  # ä»1å¢åŠ åˆ°4

# å¢åŠ è®­ç»ƒè½®æ•°
num_epochs: 200  # ä»50å¢åŠ åˆ°200
```

## ğŸ¯ æ¨ç†æŒ‡å—

### è¾“å…¥æ¨¡å¼é€‰æ‹©

#### éŸ³é¢‘è¾“å…¥æ¨¡å¼
**é€‚ç”¨åœºæ™¯**: ä¸€èˆ¬ä½¿ç”¨ã€å¼€å‘æµ‹è¯•
```bash
python scripts/inference.py --model checkpoints/best_model.pth --input audio.wav --mode audio
```

**ç‰¹ç‚¹**:
- å®Œæ•´çš„éŸ³é¢‘å¤„ç†æµç¨‹
- é¢„å¤„ç†æ—¶é—´: 2-3ç§’
- æ¨ç†æ—¶é—´: 0.3-0.5ç§’
- æ€»æ—¶é—´: 2.5-3.5ç§’

#### é¢‘è°±è¾“å…¥æ¨¡å¼
**é€‚ç”¨åœºæ™¯**: é«˜æ€§èƒ½éœ€æ±‚ã€æ‰¹é‡å¤„ç†ã€å®æ—¶ç³»ç»Ÿ
```bash
python scripts/inference.py --model checkpoints/best_model.pth --input spectrogram.npy --mode spectrogram
```

**ç‰¹ç‚¹**:
- è·³è¿‡é¢„å¤„ç†æ­¥éª¤
- é¢„å¤„ç†æ—¶é—´: 0ç§’
- æ¨ç†æ—¶é—´: 0.3-0.5ç§’
- æ€»æ—¶é—´: 0.3-0.5ç§’

#### è‡ªåŠ¨æ¨¡å¼
**é€‚ç”¨åœºæ™¯**: ä¸ç¡®å®šè¾“å…¥ç±»å‹
```bash
python scripts/inference.py --model checkpoints/best_model.pth --input file --mode auto
```

**ç‰¹ç‚¹**:
- æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨åˆ¤æ–­
- `.wav`, `.mp3`, `.flac` â†’ éŸ³é¢‘æ¨¡å¼
- `.npy`, `.npz` â†’ é¢‘è°±æ¨¡å¼

### æ‰¹é‡æ¨ç†

#### é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶
```bash
# æ‰¹é‡é¢„å¤„ç†
python scripts/batch_preprocess.py --audio_dir data/audio --labels_file data/labels.csv --output_dir data/features
```

#### æ‰¹é‡æ¨ç†
```python
from wavespectra2text import DualInputSpeechRecognizer

recognizer = DualInputSpeechRecognizer('checkpoints/best_model.pth')

# æ‰¹é‡éŸ³é¢‘æ¨ç†
audio_files = ['audio1.wav', 'audio2.wav', 'audio3.wav']
results = []

for audio_file in audio_files:
    result = recognizer.recognize_from_audio(audio_file)
    results.append(result)
    print(f"{audio_file}: {result['text']}")
```

### æ€§èƒ½ä¼˜åŒ–

#### 1. ä½¿ç”¨GPU
```bash
# è®­ç»ƒæ—¶ä½¿ç”¨GPU
python scripts/train.py --scale medium --device cuda

# æ¨ç†æ—¶ä½¿ç”¨GPU
python scripts/inference.py --model checkpoints/best_model.pth --input audio.wav --device cuda
```

#### 2. é¢„è®¡ç®—ç‰¹å¾
```bash
# é¢„å¤„ç†æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
python scripts/batch_preprocess.py --audio_dir data/audio --labels_file data/labels.csv

# ä½¿ç”¨é¢„è®¡ç®—æ¨¡å¼è®­ç»ƒ
python scripts/train.py --scale medium --use_precomputed
```

#### 3. æ‰¹é‡å¤„ç†
```python
# ä½¿ç”¨æ‰¹é‡æ¨ç†
from wavespectra2text.core.inference import BatchInference

batch_inference = BatchInference(recognizer.inference_core)
results = batch_inference.infer_audio_batch(audio_files, show_progress=True)
```

## ğŸ”§ æ•°æ®ç®¡ç†

### æ•°æ®å‡†å¤‡

#### 1. éŸ³é¢‘æ–‡ä»¶è¦æ±‚
- **æ ¼å¼**: WAV, MP3, FLAC, M4A, AAC, OGG
- **é‡‡æ ·ç‡**: æ¨è48kHzï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨é‡é‡‡æ ·ï¼‰
- **æ—¶é•¿**: 1-10ç§’ï¼ˆæ¨è2-5ç§’ï¼‰
- **è´¨é‡**: æ¸…æ™°æ— å™ªå£°

#### 2. æ ‡ç­¾æ–‡ä»¶æ ¼å¼
```csv
filename,label
audio_01.wav,ä¸€
audio_02.wav,äºŒ
audio_03.wav,ä¸‰
...
```

#### 3. æ•°æ®éªŒè¯
```bash
# éªŒè¯æ•°æ®å®Œæ•´æ€§
python scripts/setup_data.py

# æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶
python -c "
import pandas as pd
df = pd.read_csv('data/labels.csv')
print(f'æ€»æ ·æœ¬æ•°: {len(df)}')
print(f'æ ‡ç­¾åˆ†å¸ƒ: {df[\"label\"].value_counts()}')
"
```

### æ•°æ®å¢å¼º

#### 1. éŸ³é¢‘å¢å¼º
```python
import librosa
import numpy as np

def add_noise(audio, noise_factor=0.005):
    """æ·»åŠ å™ªå£°"""
    noise = np.random.randn(len(audio))
    return audio + noise_factor * noise

def change_speed(audio, sr, speed_factor=1.2):
    """æ”¹å˜è¯­é€Ÿ"""
    return librosa.effects.time_stretch(audio, rate=speed_factor)

def change_pitch(audio, sr, pitch_factor=2):
    """æ”¹å˜éŸ³è°ƒ"""
    return librosa.effects.pitch_shift(audio, sr=sr, n_steps=pitch_factor)
```

#### 2. æ•°æ®å¹³è¡¡
```python
import pandas as pd
from collections import Counter

def balance_dataset(df, target_count=100):
    """å¹³è¡¡æ•°æ®é›†"""
    label_counts = Counter(df['label'])
    balanced_data = []
    
    for label, count in label_counts.items():
        if count < target_count:
            # é‡å¤é‡‡æ ·
            label_data = df[df['label'] == label]
            repeat_times = target_count // count + 1
            balanced_data.append(label_data.sample(n=target_count, replace=True))
        else:
            # éšæœºé‡‡æ ·
            label_data = df[df['label'] == label]
            balanced_data.append(label_data.sample(n=target_count))
    
    return pd.concat(balanced_data, ignore_index=True)
```

### æ•°æ®åŒæ­¥

#### è‡ªåŠ¨æ›´æ–°ç³»ç»Ÿ
```bash
# å¯åŠ¨è‡ªåŠ¨æ›´æ–°
python scripts/auto_update_system.py --mode monitor --interval 10

# å•æ¬¡æ£€æŸ¥
python scripts/auto_update_system.py --mode check
```

**åŠŸèƒ½**:
- ç›‘æ§éŸ³é¢‘æ–‡ä»¶å˜åŒ–
- è‡ªåŠ¨æ›´æ–°è¯æ±‡è¡¨
- åŒæ­¥é¢„å¤„ç†ç‰¹å¾
- æ›´æ–°ç‰¹å¾ç´¢å¼•

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. è®­ç»ƒä¸æ”¶æ•›
**ç—‡çŠ¶**: æŸå¤±ä¸ä¸‹é™æˆ–å‡†ç¡®ç‡ä¸æå‡

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥å­¦ä¹ ç‡
python -c "
from wavespectra2text.training.config import get_default_config
config = get_default_config('small')
print(f'å­¦ä¹ ç‡: {config[\"learning_rate\"]}')
"

# è°ƒæ•´å­¦ä¹ ç‡
# ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œå°†learning_rateä»1e-5æ”¹ä¸º5e-5
```

#### 2. å†…å­˜ä¸è¶³
**ç—‡çŠ¶**: CUDA out of memory æˆ–ç³»ç»Ÿå¡æ­»

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°‘æ‰¹å¤§å°
python scripts/train.py --scale small  # ä½¿ç”¨å°è§„æ¨¡é…ç½®

# å‡å°‘æ¨¡å‹å¤§å°
# ç¼–è¾‘é…ç½®æ–‡ä»¶
batch_size: 1
hidden_dim: 32
```

#### 3. æ¨ç†é€Ÿåº¦æ…¢
**ç—‡çŠ¶**: æ¨ç†æ—¶é—´è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨é¢‘è°±è¾“å…¥æ¨¡å¼
python scripts/inference.py --model checkpoints/best_model.pth --input spectrogram.npy --mode spectrogram

# ä½¿ç”¨GPU
python scripts/inference.py --model checkpoints/best_model.pth --input audio.wav --device cuda
```

#### 4. è¯†åˆ«å‡†ç¡®ç‡ä½
**ç—‡çŠ¶**: è¯†åˆ«ç»“æœé”™è¯¯ç‡é«˜

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ•°æ®è´¨é‡
python scripts/setup_data.py

# å¢åŠ è®­ç»ƒè½®æ•°
# ç¼–è¾‘é…ç½®æ–‡ä»¶
num_epochs: 200  # ä»50å¢åŠ åˆ°200

# ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
python scripts/train.py --scale large
```

### è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
```bash
# è®­ç»ƒæ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
python scripts/train.py --scale small --verbose

# æ¨ç†æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
python scripts/inference.py --model checkpoints/best_model.pth --input audio.wav --verbose
```

#### 2. æ£€æŸ¥ä¸­é—´ç»“æœ
```python
# æ£€æŸ¥é¢„å¤„ç†ç»“æœ
from wavespectra2text import PreprocessorFactory
preprocessor = PreprocessorFactory.create('spectrogram')
features = preprocessor.process('audio.wav')
print(f'ç‰¹å¾å½¢çŠ¶: {features.shape}')
print(f'ç‰¹å¾èŒƒå›´: [{features.min():.3f}, {features.max():.3f}]')

# æ£€æŸ¥æ¨¡å‹è¾“å‡º
from wavespectra2text import create_model, vocab
model = create_model()
# ... åŠ è½½æ¨¡å‹
output = model(input_tensor)
print(f'è¾“å‡ºå½¢çŠ¶: {output.shape}')
```

#### 3. ä½¿ç”¨æµ‹è¯•è„šæœ¬
```bash
# è¿è¡Œæµ‹è¯•å¥—ä»¶
python tests/run_tests.py

# è¿è¡Œç‰¹å®šæµ‹è¯•
python tests/test_core.py
python tests/test_data.py
python tests/test_training.py
python tests/test_inference.py
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### è®­ç»ƒä¼˜åŒ–

#### 1. ä½¿ç”¨é¢„è®¡ç®—ç‰¹å¾
```bash
# é¢„å¤„ç†æ‰€æœ‰éŸ³é¢‘
python scripts/batch_preprocess.py --audio_dir data/audio --labels_file data/labels.csv

# ä½¿ç”¨é¢„è®¡ç®—æ¨¡å¼è®­ç»ƒï¼ˆæ›´å¿«ï¼‰
python scripts/train.py --scale medium --use_precomputed
```

#### 2. GPUåŠ é€Ÿ
```bash
# æ£€æŸ¥GPUå¯ç”¨æ€§
python -c "import torch; print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"

# ä½¿ç”¨GPUè®­ç»ƒ
python scripts/train.py --scale medium --device cuda
```

#### 3. å¤šè¿›ç¨‹æ•°æ®åŠ è½½
```bash
# ç¼–è¾‘é…ç½®æ–‡ä»¶
num_workers: 4  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
pin_memory: true
```

### æ¨ç†ä¼˜åŒ–

#### 1. æ¨¡å‹é‡åŒ–
```python
import torch

# åŠ è½½æ¨¡å‹
model = torch.load('checkpoints/best_model.pth')

# é‡åŒ–æ¨¡å‹
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# ä¿å­˜é‡åŒ–æ¨¡å‹
torch.save(quantized_model.state_dict(), 'checkpoints/quantized_model.pth')
```

#### 2. æ‰¹å¤„ç†æ¨ç†
```python
from wavespectra2text.core.inference import BatchInference

# æ‰¹é‡æ¨ç†
batch_inference = BatchInference(inference_core)
results = batch_inference.infer_audio_batch(
    audio_files, 
    method='beam',
    beam_size=3,
    show_progress=True
)
```

#### 3. ç¼“å­˜æœºåˆ¶
```python
# å¯ç”¨ç‰¹å¾ç¼“å­˜
from wavespectra2text.data.preprocessing import OfflinePreprocessor

preprocessor = PreprocessorFactory.create('spectrogram')
offline_processor = OfflinePreprocessor(preprocessor, cache_dir='cache/features')

# å¤„ç†æ–‡ä»¶ï¼ˆä¼šè‡ªåŠ¨ç¼“å­˜ï¼‰
features = offline_processor.process_file('audio.wav')
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. æ•°æ®å‡†å¤‡
- ä½¿ç”¨é«˜è´¨é‡çš„éŸ³é¢‘æ–‡ä»¶
- ç¡®ä¿æ ‡ç­¾å‡†ç¡®æ— è¯¯
- ä¿æŒæ•°æ®å¹³è¡¡
- å®šæœŸéªŒè¯æ•°æ®å®Œæ•´æ€§

### 2. æ¨¡å‹è®­ç»ƒ
- ä»å°è§„æ¨¡å¼€å§‹æµ‹è¯•
- ç›‘æ§è®­ç»ƒè¿‡ç¨‹
- ä½¿ç”¨éªŒè¯é›†è¯„ä¼°
- ä¿å­˜æœ€ä½³æ¨¡å‹

### 3. æ¨ç†éƒ¨ç½²
- ä½¿ç”¨é¢‘è°±è¾“å…¥æ¨¡å¼æå‡æ€§èƒ½
- æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
- å¯ç”¨GPUåŠ é€Ÿ
- å®ç°é”™è¯¯å¤„ç†æœºåˆ¶

### 4. ç³»ç»Ÿç»´æŠ¤
- å®šæœŸæ›´æ–°ä¾èµ–åŒ…
- ç›‘æ§ç³»ç»Ÿæ€§èƒ½
- å¤‡ä»½é‡è¦æ•°æ®
- è®°å½•é…ç½®å˜æ›´

## ğŸ“š è¿›é˜¶ä½¿ç”¨

### è‡ªå®šä¹‰é¢„å¤„ç†å™¨
```python
from wavespectra2text.data.preprocessing import AudioPreprocessor
import librosa
import numpy as np

class CustomPreprocessor(AudioPreprocessor):
    def process(self, audio_path):
        # è‡ªå®šä¹‰é¢„å¤„ç†é€»è¾‘
        audio, sr = librosa.load(audio_path, sr=self.sample_rate)
        
        # æ·»åŠ è‡ªå®šä¹‰ç‰¹å¾
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
        chroma = librosa.feature.chroma(y=audio, sr=sr)
        
        # ç»„åˆç‰¹å¾
        features = np.concatenate([mfcc.T, chroma.T], axis=1)
        
        return features
    
    def get_feature_shape(self):
        return (200, 25)  # (time, features)

# æ³¨å†Œè‡ªå®šä¹‰é¢„å¤„ç†å™¨
from wavespectra2text.data.preprocessing import PreprocessorFactory
PreprocessorFactory.register('custom', CustomPreprocessor)
```

### è‡ªå®šä¹‰è®­ç»ƒå™¨
```python
from wavespectra2text.training.trainer import BaseTrainer
import torch.optim as optim

class CustomTrainer(BaseTrainer):
    def _create_optimizer(self):
        return optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay'],
            betas=(0.9, 0.98)
        )
    
    def _create_scheduler(self):
        return optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config['num_epochs']
        )
```

### é›†æˆå¤–éƒ¨ç³»ç»Ÿ
```python
# ä¸å¤–éƒ¨éŸ³é¢‘å¤„ç†ç³»ç»Ÿé›†æˆ
def external_audio_processing(audio_path):
    """å¤–éƒ¨ç³»ç»Ÿçš„éŸ³é¢‘é¢„å¤„ç†"""
    from wavespectra2text import PreprocessorFactory
    
    processor = PreprocessorFactory.create('spectrogram')
    return processor.process(audio_path)

# ä¿å­˜é¢„å¤„ç†ç»“æœ
import numpy as np
spectrogram = external_audio_processing('audio.wav')
np.save('external_spectrogram.npy', spectrogram)

# ä½¿ç”¨é¢„å¤„ç†ç»“æœè¿›è¡Œæ¨ç†
from wavespectra2text import DualInputSpeechRecognizer
recognizer = DualInputSpeechRecognizer('checkpoints/best_model.pth')
result = recognizer.recognize_from_spectrogram('external_spectrogram.npy')
```
