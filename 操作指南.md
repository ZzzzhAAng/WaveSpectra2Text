# WaveSpectra2Text æ“ä½œæŒ‡å—
# åŒè¾“å…¥è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå®Œæ•´ä½¿ç”¨æ‰‹å†Œ

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
3. [è®­ç»ƒæ¨¡å‹](#è®­ç»ƒæ¨¡å‹)
4. [æ¨ç†ä½¿ç”¨](#æ¨ç†ä½¿ç”¨)
5. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
6. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
7. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
8. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository_url>
cd WaveSpectra2Text

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. éªŒè¯å®‰è£…
python -c "import torch, librosa, pandas; print('ç¯å¢ƒå‡†å¤‡å®Œæˆï¼')"
```

### 30ç§’ä½“éªŒ

```bash
# 1. æ£€æŸ¥æ•°æ®
python setup_data.py

# 2. è®­ç»ƒå°æ¨¡å‹ï¼ˆçº¦5åˆ†é’Ÿï¼‰
python train_at_different_scales/train_scale_1.py

# 3. æµ‹è¯•æ¨ç†
python dual_input_inference.py --model checkpoints/best_model.pth --input data/audio/Chinese_Number_01.wav
```

---

## ğŸ“ æ•°æ®å‡†å¤‡

### 1. æ•°æ®ç»“æ„è¦æ±‚

```
data/
â”œâ”€â”€ audio/                    # éŸ³é¢‘æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ Chinese_Number_01.wav # 48kHzéŸ³é¢‘æ–‡ä»¶
â”‚   â”œâ”€â”€ Chinese_Number_02.wav
â”‚   â””â”€â”€ ...
â””â”€â”€ labels.csv               # æ ‡ç­¾æ–‡ä»¶
```

### 2. æ ‡ç­¾æ–‡ä»¶æ ¼å¼

åˆ›å»º `data/labels.csv` æ–‡ä»¶ï¼š

```csv
filename,label
Chinese_Number_01.wav,ä¸€
Chinese_Number_02.wav,äºŒ
Chinese_Number_03.wav,ä¸‰
...
```

### 3. è‡ªåŠ¨æ•°æ®è®¾ç½®

```bash
# è‡ªåŠ¨æ‰«æéŸ³é¢‘æ–‡ä»¶å¹¶ç”Ÿæˆæ ‡ç­¾æ¨¡æ¿
python setup_data.py

# éªŒè¯æ•°æ®å®Œæ•´æ€§
python setup_data.py  # å†æ¬¡è¿è¡Œè¿›è¡ŒéªŒè¯
```

### 4. æ”¯æŒçš„éŸ³é¢‘æ ¼å¼

- **æ¨è**: WAV (48kHz, 16-bit)
- **æ”¯æŒ**: MP3, FLAC, M4A, AAC
- **è¦æ±‚**: æ¸…æ™°çš„ä¸­æ–‡æ•°å­—å‘éŸ³

---

## ğŸ‹ï¸ è®­ç»ƒæ¨¡å‹

### 1. é€‰æ‹©è®­ç»ƒè§„æ¨¡

æ ¹æ®æ•°æ®é›†å¤§å°é€‰æ‹©åˆé€‚çš„è®­ç»ƒè„šæœ¬ï¼š

| æ•°æ®é‡ | è®­ç»ƒè„šæœ¬ | æ¨èé…ç½® | è®­ç»ƒæ—¶é—´ |
|--------|----------|----------|----------|
| 10-50æ ·æœ¬ | `train_scale_1.py` | batch_size=1, hidden_dim=32 | 5-15åˆ†é’Ÿ |
| 50-200æ ·æœ¬ | `train_scale_2.py` | batch_size=2, hidden_dim=64 | 15-30åˆ†é’Ÿ |
| 200-1000æ ·æœ¬ | `train_scale_3.py` | batch_size=4, hidden_dim=128 | 30-60åˆ†é’Ÿ |
| 1000+æ ·æœ¬ | `train_scale_4.py` | batch_size=8, hidden_dim=256 | 1-3å°æ—¶ |

### 2. é…ç½®è®­ç»ƒå‚æ•°

ç¼–è¾‘ `config.json`ï¼š

```json
{
  "experiment_name": "my_experiment",
  "batch_size": 1,
  "learning_rate": 0.00005,
  "num_epochs": 200,
  "hidden_dim": 32,
  "encoder_layers": 2,
  "decoder_layers": 2,
  "dropout": 0.1
}
```

### 3. å¼€å§‹è®­ç»ƒ

```bash
# åŸºç¡€è®­ç»ƒ
python train_at_different_scales/train_scale_1.py

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python train_at_different_scales/train_scale_1.py --config my_config.json

# ä»æ£€æŸ¥ç‚¹æ¢å¤
python train_at_different_scales/train_scale_1.py --resume checkpoints/checkpoint_epoch_50.pth

# ä½¿ç”¨GPUè®­ç»ƒ
python train_at_different_scales/train_scale_1.py --device cuda
```

### 4. ç›‘æ§è®­ç»ƒè¿›åº¦

```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir runs

# åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:6006
```

---

## ğŸ¯ æ¨ç†ä½¿ç”¨

### 1. åŒè¾“å…¥æ¨ç†ç³»ç»Ÿï¼ˆæ¨èï¼‰

#### è‡ªåŠ¨æ¨¡å¼
```bash
# ç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹è¾“å…¥ç±»å‹
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/audio/Chinese_Number_01.wav

# æ‰¹é‡å¤„ç†ç›®å½•
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/audio/ \
  --mode auto
```

#### éŸ³é¢‘è¾“å…¥æ¨¡å¼
```bash
# å®Œæ•´æµç¨‹ï¼šéŸ³é¢‘ â†’ é¢‘è°± â†’ æ¨ç†
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/audio/Chinese_Number_01.wav \
  --mode audio
```

#### é¢‘è°±è¾“å…¥æ¨¡å¼ï¼ˆé«˜æ€§èƒ½ï¼‰
```bash
# é¢„å¤„ç†éŸ³é¢‘ä¸ºé¢‘è°±ç‰¹å¾
python batch_preprocess.py \
  --audio_dir data/audio \
  --labels_file data/labels.csv \
  --output_dir data/features

# å¿«é€Ÿæ¨ç†ï¼šé¢‘è°± â†’ æ¨ç†ï¼ˆé€Ÿåº¦æå‡5-10å€ï¼‰
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/features/Chinese_Number_01.npy \
  --mode spectrogram
```

### 2. ä¼ ç»Ÿæ¨ç†ç³»ç»Ÿ

```bash
# ç¼–ç¨‹æ¥å£ä½¿ç”¨ï¼ˆæ¨èï¼‰
# æ–¹å¼1ï¼šä½¿ç”¨ç»Ÿä¸€æ¨ç†æ ¸å¿ƒ
from inference_core import InferenceCore
core = InferenceCore('checkpoints/best_model.pth')
result = core.infer_from_audio('audio.wav')

# æ–¹å¼2ï¼šå¿«é€Ÿæ¨ç†å‡½æ•°
from inference_core import quick_infer_audio
text = quick_infer_audio('checkpoints/best_model.pth', 'audio.wav')
```

### 3. æ¨ç†ç»“æœè§£è¯»

```
ğŸ¯ åŒè¾“å…¥æ¨¡å¼è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ
è®¾å¤‡: cpu
ğŸ“‚ æ¨¡å‹åŠ è½½æˆåŠŸ: checkpoints/best_model.pth

ğŸµ æ¨¡å¼1: åŸå§‹éŸ³é¢‘è¾“å…¥
æ–‡ä»¶: data/audio/Chinese_Number_01.wav
ğŸ”§ æ­¥éª¤1: éŸ³é¢‘é¢„å¤„ç†
  âœ… é¢‘è°±æå–: (200, 513)
  â±ï¸  é¢„å¤„ç†è€—æ—¶: 2.156ç§’
ğŸ§  æ­¥éª¤2: æ¨¡å‹æ¨ç†
  ğŸ” ç¼–ç å™¨è¾“å‡º: (1, 200, 32)
  ğŸ”¤ è´ªå©ªè§£ç : 'ä¸€'
  ğŸ”¤ æŸæœç´¢: 'ä¸€' (å¾—åˆ†: -1.234)

ğŸ¯ æœ€ç»ˆç»“æœ: 'ä¸€'
â±ï¸  æ¨ç†è€—æ—¶: 0.423ç§’
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. è®­ç»ƒæ€§èƒ½ä¼˜åŒ–

#### ä½¿ç”¨é¢„è®¡ç®—ç‰¹å¾
```bash
# ä¸€æ¬¡æ€§é¢„å¤„ç†æ‰€æœ‰éŸ³é¢‘ï¼ˆæ¨èï¼‰
python batch_preprocess.py \
  --audio_dir data/audio \
  --labels_file data/labels.csv \
  --output_dir data/features

# è®­ç»ƒæ—¶ä½¿ç”¨é¢„è®¡ç®—ç‰¹å¾
python train_at_different_scales/train_scale_1.py --use_precomputed
```

#### GPUåŠ é€Ÿ
```bash
# æ£€æŸ¥GPUå¯ç”¨æ€§
python -c "import torch; print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"

# ä½¿ç”¨GPUè®­ç»ƒ
python train_at_different_scales/train_scale_1.py --device cuda
```

#### å†…å­˜ä¼˜åŒ–
```bash
# å°å†…å­˜ç¯å¢ƒé…ç½®
# ç¼–è¾‘config.json:
{
  "batch_size": 1,
  "hidden_dim": 16,
  "encoder_layers": 1,
  "decoder_layers": 1
}
```

### 2. æ¨ç†æ€§èƒ½ä¼˜åŒ–

#### æ€§èƒ½å¯¹æ¯”æµ‹è¯•
```bash
# æŸ¥çœ‹ä¸¤ç§è¾“å…¥æ¨¡å¼çš„æ€§èƒ½å¯¹æ¯”
python dual_input_inference.py --compare

# è¾“å‡ºç¤ºä¾‹:
# ç‰¹å¾            éŸ³é¢‘è¾“å…¥        é¢‘è°±è¾“å…¥
# é¢„å¤„ç†æ—¶é—´      2-3ç§’          0ç§’
# æ¨ç†æ—¶é—´        0.3-0.5ç§’      0.3-0.5ç§’
# æ€»æ—¶é—´          2.5-3.5ç§’      0.3-0.5ç§’
```

#### æ‰¹é‡æ¨ç†ä¼˜åŒ–
```bash
# é¢„å¤„ç†æ•´ä¸ªç›®å½•
python batch_preprocess.py --audio_dir data/audio --output_dir data/features

# æ‰¹é‡æ¨ç†ï¼ˆä½¿ç”¨é¢‘è°±è¾“å…¥ï¼‰
for file in data/features/*.npy; do
  python dual_input_inference.py --model checkpoints/best_model.pth --input "$file" --mode spectrogram
done
```

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰é¢„å¤„ç†å™¨

```python
# åˆ›å»ºè‡ªå®šä¹‰é¢„å¤„ç†å™¨
from audio_preprocess import PreprocessorFactory

# ä½¿ç”¨Melé¢‘è°±
preprocessor = PreprocessorFactory.create(
    'mel_spectrogram',
    sample_rate=48000,
    n_mels=128,
    max_length=200
)

# æ‰¹é‡å¤„ç†
python batch_preprocess.py \
  --preprocessor mel_spectrogram \
  --n_mels 128
```

### 2. å¤–éƒ¨ç³»ç»Ÿé›†æˆ

```bash
# æŸ¥çœ‹é›†æˆç¤ºä¾‹ä»£ç 
python dual_input_inference.py --demo
```

ç¤ºä¾‹é›†æˆä»£ç ï¼š
```python
# å¤–éƒ¨ç³»ç»Ÿé¢„å¤„ç†
import librosa
import numpy as np

def external_preprocess(audio_path):
    audio, sr = librosa.load(audio_path, sr=48000)
    stft = librosa.stft(audio, n_fft=1024, hop_length=512)
    magnitude = np.abs(stft)
    log_magnitude = np.log1p(magnitude)
    spectrogram = log_magnitude.T
    
    # æ ‡å‡†åŒ–é•¿åº¦åˆ°200
    if len(spectrogram) > 200:
        spectrogram = spectrogram[:200]
    else:
        pad_length = 200 - len(spectrogram)
        spectrogram = np.pad(spectrogram, ((0, pad_length), (0, 0)))
    
    return spectrogram.astype(np.float32)

# ä½¿ç”¨é¢„å¤„ç†ç»“æœ
from dual_input_inference import DualInputSpeechRecognizer

recognizer = DualInputSpeechRecognizer("checkpoints/best_model.pth")
spectrogram = external_preprocess("audio.wav")
result = recognizer.recognize_from_spectrogram_array(spectrogram)
```

### 3. æ¨¡å‹é…ç½®è°ƒä¼˜

```python
# æŸ¥çœ‹å‚æ•°è°ƒä¼˜æŒ‡å—
import json
with open('parameters_tuning_guide.json', 'r', encoding='utf-8') as f:
    guide = json.load(f)
    print(json.dumps(guide, indent=2, ensure_ascii=False))
```

---

## ğŸ” æ•…éšœæ’é™¤

### 1. å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

#### æ•°æ®ç›¸å…³é”™è¯¯
```bash
# é”™è¯¯ï¼šFileNotFoundError: éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨
# è§£å†³ï¼šæ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæƒé™
ls -la data/audio/
python setup_data.py  # é‡æ–°éªŒè¯æ•°æ®

# é”™è¯¯ï¼šæ ‡ç­¾æ–‡ä»¶æ ¼å¼é”™è¯¯
# è§£å†³ï¼šæ£€æŸ¥CSVæ ¼å¼
head -5 data/labels.csv
# ç¡®ä¿åŒ…å«filenameå’Œlabelåˆ—
```

#### ä¾èµ–ç›¸å…³é”™è¯¯
```bash
# é”™è¯¯ï¼šImportError: No module named 'librosa'
pip install librosa soundfile

# é”™è¯¯ï¼šOSError: cannot load library 'libsndfile.so'
# Ubuntu/Debian:
sudo apt-get install libsndfile1
# macOS:
brew install libsndfile

# é”™è¯¯ï¼šRuntimeError: No audio backend is available
pip install soundfile
```

#### å†…å­˜ç›¸å…³é”™è¯¯
```bash
# é”™è¯¯ï¼šRuntimeError: CUDA out of memory
# è§£å†³ï¼šå‡å°‘batch_sizeæˆ–ä½¿ç”¨CPU
python train_scale_1.py --device cpu

# é”™è¯¯ï¼šMemoryError during training
# è§£å†³ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®
# ç¼–è¾‘config.json: "hidden_dim": 16, "batch_size": 1
```

### 2. è°ƒè¯•æŠ€å·§

#### å¯ç”¨è¯¦ç»†æ—¥å¿—
```bash
# è®­ç»ƒæ—¶å¯ç”¨è¯¦ç»†è¾“å‡º
python train_scale_1.py --verbose

# æ¨ç†æ—¶å¯ç”¨è¯¦ç»†è¾“å‡º
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/audio/test.wav \
  --verbose
```

#### æ£€æŸ¥æ¨¡å‹çŠ¶æ€
```python
import torch

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
checkpoint = torch.load('checkpoints/best_model.pth', map_location='cpu')
print(f"è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'Unknown')}")
print(f"éªŒè¯æŸå¤±: {checkpoint.get('best_val_loss', 'Unknown')}")
print(f"æ¨¡å‹é…ç½®: {checkpoint.get('config', {})}")
```

#### éªŒè¯æ•°æ®è´¨é‡
```bash
# æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶è´¨é‡
python -c "
import librosa
import os

for f in os.listdir('data/audio'):
    if f.endswith('.wav'):
        try:
            y, sr = librosa.load(f'data/audio/{f}', sr=None)
            print(f'{f}: {sr}Hz, {len(y)/sr:.2f}ç§’')
        except Exception as e:
            print(f'{f}: é”™è¯¯ - {e}')
"
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ•°æ®å‡†å¤‡æœ€ä½³å®è·µ

#### éŸ³é¢‘è´¨é‡è¦æ±‚
- **é‡‡æ ·ç‡**: 48kHzï¼ˆæ¨èï¼‰æˆ–44.1kHz
- **æ ¼å¼**: WAVæ— æŸæ ¼å¼
- **æ—¶é•¿**: 1-5ç§’ä¸ºæœ€ä½³
- **å†…å®¹**: æ¸…æ™°çš„ä¸­æ–‡æ•°å­—å‘éŸ³
- **ç¯å¢ƒ**: ä½å™ªå£°ç¯å¢ƒå½•åˆ¶

#### æ•°æ®ç»„ç»‡
```bash
# æ¨èçš„ç›®å½•ç»“æ„
data/
â”œâ”€â”€ raw_audio/          # åŸå§‹å½•éŸ³æ–‡ä»¶
â”œâ”€â”€ audio/             # å¤„ç†åçš„è®­ç»ƒéŸ³é¢‘
â”œâ”€â”€ labels.csv         # æ ‡ç­¾æ–‡ä»¶
â”œâ”€â”€ features/          # é¢„è®¡ç®—ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
â””â”€â”€ validation/        # éªŒè¯é›†ï¼ˆå¯é€‰ï¼‰
```

### 2. è®­ç»ƒæœ€ä½³å®è·µ

#### å°æ•°æ®é›†è®­ç»ƒï¼ˆ<50æ ·æœ¬ï¼‰
```json
{
  "batch_size": 1,
  "learning_rate": 0.00005,
  "hidden_dim": 32,
  "encoder_layers": 2,
  "decoder_layers": 2,
  "dropout": 0.1,
  "num_epochs": 200
}
```

#### ä¸­ç­‰æ•°æ®é›†è®­ç»ƒï¼ˆ50-200æ ·æœ¬ï¼‰
```json
{
  "batch_size": 2,
  "learning_rate": 0.0001,
  "hidden_dim": 64,
  "encoder_layers": 3,
  "decoder_layers": 3,
  "dropout": 0.15,
  "num_epochs": 150
}
```

#### è®­ç»ƒç›‘æ§
```bash
# å®æ—¶ç›‘æ§è®­ç»ƒ
tensorboard --logdir runs &
python train_scale_1.py

# å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
# åœ¨config.jsonä¸­è®¾ç½®: "save_every": 20
```

### 3. æ¨ç†æœ€ä½³å®è·µ

#### é€‰æ‹©åˆé€‚çš„æ¨ç†æ¨¡å¼
```bash
# å¼€å‘æµ‹è¯•é˜¶æ®µï¼šä½¿ç”¨éŸ³é¢‘è¾“å…¥æ¨¡å¼
python dual_input_inference.py --mode audio --input test.wav

# ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨é¢‘è°±è¾“å…¥æ¨¡å¼
python batch_preprocess.py --audio_dir production_audio/
python dual_input_inference.py --mode spectrogram --input features/
```

#### æ‰¹é‡å¤„ç†ä¼˜åŒ–
```bash
# 1. é¢„å¤„ç†æ‰€æœ‰éŸ³é¢‘
python batch_preprocess.py --audio_dir data/audio --output_dir data/features

# 2. æ‰¹é‡æ¨ç†
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/features/ \
  --mode spectrogram \
  --output results.csv
```

### 4. éƒ¨ç½²æœ€ä½³å®è·µ

#### æ¨¡å‹ä¼˜åŒ–
```python
# æ¨¡å‹é‡åŒ–ï¼ˆå‡å°‘æ¨¡å‹å¤§å°ï¼‰
import torch

model = torch.load('checkpoints/best_model.pth')
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
torch.save(quantized_model, 'checkpoints/quantized_model.pth')
```

#### æœåŠ¡åŒ–éƒ¨ç½²
```python
# ç®€å•çš„Flask APIç¤ºä¾‹
from flask import Flask, request, jsonify
from dual_input_inference import DualInputSpeechRecognizer

app = Flask(__name__)
recognizer = DualInputSpeechRecognizer("checkpoints/best_model.pth")

@app.route('/recognize', methods=['POST'])
def recognize():
    if 'audio' in request.files:
        # éŸ³é¢‘è¾“å…¥æ¨¡å¼
        audio_file = request.files['audio']
        audio_file.save('temp_audio.wav')
        result = recognizer.recognize_from_audio('temp_audio.wav')
    elif 'spectrogram' in request.json:
        # é¢‘è°±è¾“å…¥æ¨¡å¼
        import numpy as np
        spectrogram = np.array(request.json['spectrogram'])
        result = recognizer.recognize_from_spectrogram_array(spectrogram)
    
    return jsonify(result)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### è·å–å¸®åŠ©
- æŸ¥çœ‹é¡¹ç›®README.md
- è¿è¡Œ `python <script> --help` æŸ¥çœ‹å‘½ä»¤è¡Œå‚æ•°
- æ£€æŸ¥ `tests/` ç›®å½•ä¸­çš„æµ‹è¯•ç”¨ä¾‹

### æ€§èƒ½åŸºå‡†
- **å°æ•°æ®é›†ï¼ˆ10æ ·æœ¬ï¼‰**: è®­ç»ƒ5åˆ†é’Ÿï¼Œå‡†ç¡®ç‡>90%
- **ä¸­ç­‰æ•°æ®é›†ï¼ˆ50æ ·æœ¬ï¼‰**: è®­ç»ƒ15åˆ†é’Ÿï¼Œå‡†ç¡®ç‡>95%
- **æ¨ç†é€Ÿåº¦**: éŸ³é¢‘æ¨¡å¼3ç§’/æ–‡ä»¶ï¼Œé¢‘è°±æ¨¡å¼0.5ç§’/æ–‡ä»¶

### ç³»ç»Ÿè¦æ±‚
- **æœ€ä½**: Python 3.7+, 4GB RAM, CPU
- **æ¨è**: Python 3.8+, 8GB RAM, GPU (å¯é€‰)
- **æ“ä½œç³»ç»Ÿ**: Windows 10+, macOS 10.14+, Ubuntu 18.04+

---

*æœ¬æ“ä½œæŒ‡å—æ¶µç›–äº†WaveSpectra2TextåŒè¾“å…¥è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å®Œæ•´ä½¿ç”¨æµç¨‹ã€‚å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒæ•…éšœæ’é™¤éƒ¨åˆ†æˆ–æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£ã€‚*