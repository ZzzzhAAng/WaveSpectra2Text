# WaveSpectra2Text 操作指南
# 双输入语音识别系统完整使用手册

## 📋 目录

1. [快速开始](#快速开始)
2. [数据准备](#数据准备)
3. [训练模型](#训练模型)
4. [推理使用](#推理使用)
5. [性能优化](#性能优化)
6. [高级功能](#高级功能)
7. [故障排除](#故障排除)
8. [最佳实践](#最佳实践)

---

## 🚀 快速开始

### 环境准备

```bash
# 1. 克隆项目
git clone <repository_url>
cd WaveSpectra2Text

# 2. 安装依赖
pip install -r requirements.txt

# 3. 验证安装
python -c "import torch, librosa, pandas; print('环境准备完成！')"
```

### 30秒体验

```bash
# 1. 检查数据
python setup_data.py

# 2. 训练小模型（约5分钟）
python train_at_different_scales/train_scale_1.py

# 3. 测试推理
python dual_input_inference.py --model checkpoints/best_model.pth --input data/audio/Chinese_Number_01.wav
```

---

## 📁 数据准备

### 1. 数据结构要求

```
data/
├── audio/                    # 音频文件目录
│   ├── Chinese_Number_01.wav # 48kHz音频文件
│   ├── Chinese_Number_02.wav
│   └── ...
└── labels.csv               # 标签文件
```

### 2. 标签文件格式

创建 `data/labels.csv` 文件：

```csv
filename,label
Chinese_Number_01.wav,一
Chinese_Number_02.wav,二
Chinese_Number_03.wav,三
...
```

### 3. 自动数据设置

```bash
# 自动扫描音频文件并生成标签模板
python setup_data.py

# 验证数据完整性
python setup_data.py  # 再次运行进行验证
```

### 4. 支持的音频格式

- **推荐**: WAV (48kHz, 16-bit)
- **支持**: MP3, FLAC, M4A, AAC
- **要求**: 清晰的中文数字发音

---

## 🏋️ 训练模型

### 1. 选择训练规模

根据数据集大小选择合适的训练脚本：

| 数据量 | 训练脚本 | 推荐配置 | 训练时间 |
|--------|----------|----------|----------|
| 10-50样本 | `train_scale_1.py` | batch_size=1, hidden_dim=32 | 5-15分钟 |
| 50-200样本 | `train_scale_2.py` | batch_size=2, hidden_dim=64 | 15-30分钟 |
| 200-1000样本 | `train_scale_3.py` | batch_size=4, hidden_dim=128 | 30-60分钟 |
| 1000+样本 | `train_scale_4.py` | batch_size=8, hidden_dim=256 | 1-3小时 |

### 2. 配置训练参数

编辑 `config.json`：

```json
{
  "experiment_name": "my_experiment",
  "batch_size": 1,
  "learning_rate": 0.00005,
  "num_epochs": 200,
  "hidden_dim": 32,
  "encoder_layers": 2,
  "decoder_layers": 2,
  "dropout": 0.1
}
```

### 3. 开始训练

```bash
# 基础训练
python train_at_different_scales/train_scale_1.py

# 使用自定义配置
python train_at_different_scales/train_scale_1.py --config my_config.json

# 从检查点恢复
python train_at_different_scales/train_scale_1.py --resume checkpoints/checkpoint_epoch_50.pth

# 使用GPU训练
python train_at_different_scales/train_scale_1.py --device cuda
```

### 4. 监控训练进度

```bash
# 启动TensorBoard
tensorboard --logdir runs

# 在浏览器中访问: http://localhost:6006
```

---

## 🎯 推理使用

### 1. 双输入推理系统（推荐）

#### 自动模式
```bash
# 系统自动检测输入类型
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/audio/Chinese_Number_01.wav

# 批量处理目录
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/audio/ \
  --mode auto
```

#### 音频输入模式
```bash
# 完整流程：音频 → 频谱 → 推理
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/audio/Chinese_Number_01.wav \
  --mode audio
```

#### 频谱输入模式（高性能）
```bash
# 预处理音频为频谱特征
python batch_preprocess.py \
  --audio_dir data/audio \
  --labels_file data/labels.csv \
  --output_dir data/features

# 快速推理：频谱 → 推理（速度提升5-10倍）
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/features/Chinese_Number_01.npy \
  --mode spectrogram
```

### 2. 传统推理系统

```bash
# 编程接口使用（推荐）
# 方式1：使用统一推理核心
from inference_core import InferenceCore
core = InferenceCore('checkpoints/best_model.pth')
result = core.infer_from_audio('audio.wav')

# 方式2：快速推理函数
from inference_core import quick_infer_audio
text = quick_infer_audio('checkpoints/best_model.pth', 'audio.wav')
```

### 3. 推理结果解读

```
🎯 双输入模式语音识别系统
设备: cpu
📂 模型加载成功: checkpoints/best_model.pth

🎵 模式1: 原始音频输入
文件: data/audio/Chinese_Number_01.wav
🔧 步骤1: 音频预处理
  ✅ 频谱提取: (200, 513)
  ⏱️  预处理耗时: 2.156秒
🧠 步骤2: 模型推理
  🔍 编码器输出: (1, 200, 32)
  🔤 贪婪解码: '一'
  🔤 束搜索: '一' (得分: -1.234)

🎯 最终结果: '一'
⏱️  推理耗时: 0.423秒
```

---

## ⚡ 性能优化

### 1. 训练性能优化

#### 使用预计算特征
```bash
# 一次性预处理所有音频（推荐）
python batch_preprocess.py \
  --audio_dir data/audio \
  --labels_file data/labels.csv \
  --output_dir data/features

# 训练时使用预计算特征
python train_at_different_scales/train_scale_1.py --use_precomputed
```

#### GPU加速
```bash
# 检查GPU可用性
python -c "import torch; print(f'CUDA可用: {torch.cuda.is_available()}')"

# 使用GPU训练
python train_at_different_scales/train_scale_1.py --device cuda
```

#### 内存优化
```bash
# 小内存环境配置
# 编辑config.json:
{
  "batch_size": 1,
  "hidden_dim": 16,
  "encoder_layers": 1,
  "decoder_layers": 1
}
```

### 2. 推理性能优化

#### 性能对比测试
```bash
# 查看两种输入模式的性能对比
python dual_input_inference.py --compare

# 输出示例:
# 特征            音频输入        频谱输入
# 预处理时间      2-3秒          0秒
# 推理时间        0.3-0.5秒      0.3-0.5秒
# 总时间          2.5-3.5秒      0.3-0.5秒
```

#### 批量推理优化
```bash
# 预处理整个目录
python batch_preprocess.py --audio_dir data/audio --output_dir data/features

# 批量推理（使用频谱输入）
for file in data/features/*.npy; do
  python dual_input_inference.py --model checkpoints/best_model.pth --input "$file" --mode spectrogram
done
```

---

## 🔧 高级功能

### 1. 自定义预处理器

```python
# 创建自定义预处理器
from audio_preprocess import PreprocessorFactory

# 使用Mel频谱
preprocessor = PreprocessorFactory.create(
    'mel_spectrogram',
    sample_rate=48000,
    n_mels=128,
    max_length=200
)

# 批量处理
python batch_preprocess.py \
  --preprocessor mel_spectrogram \
  --n_mels 128
```

### 2. 外部系统集成

```bash
# 查看集成示例代码
python dual_input_inference.py --demo
```

示例集成代码：
```python
# 外部系统预处理
import librosa
import numpy as np

def external_preprocess(audio_path):
    audio, sr = librosa.load(audio_path, sr=48000)
    stft = librosa.stft(audio, n_fft=1024, hop_length=512)
    magnitude = np.abs(stft)
    log_magnitude = np.log1p(magnitude)
    spectrogram = log_magnitude.T
    
    # 标准化长度到200
    if len(spectrogram) > 200:
        spectrogram = spectrogram[:200]
    else:
        pad_length = 200 - len(spectrogram)
        spectrogram = np.pad(spectrogram, ((0, pad_length), (0, 0)))
    
    return spectrogram.astype(np.float32)

# 使用预处理结果
from dual_input_inference import DualInputSpeechRecognizer

recognizer = DualInputSpeechRecognizer("checkpoints/best_model.pth")
spectrogram = external_preprocess("audio.wav")
result = recognizer.recognize_from_spectrogram_array(spectrogram)
```

### 3. 模型配置调优

```python
# 查看参数调优指南
import json
with open('parameters_tuning_guide.json', 'r', encoding='utf-8') as f:
    guide = json.load(f)
    print(json.dumps(guide, indent=2, ensure_ascii=False))
```

---

## 🔍 故障排除

### 1. 常见错误及解决方案

#### 数据相关错误
```bash
# 错误：FileNotFoundError: 音频文件不存在
# 解决：检查文件路径和权限
ls -la data/audio/
python setup_data.py  # 重新验证数据

# 错误：标签文件格式错误
# 解决：检查CSV格式
head -5 data/labels.csv
# 确保包含filename和label列
```

#### 依赖相关错误
```bash
# 错误：ImportError: No module named 'librosa'
pip install librosa soundfile

# 错误：OSError: cannot load library 'libsndfile.so'
# Ubuntu/Debian:
sudo apt-get install libsndfile1
# macOS:
brew install libsndfile

# 错误：RuntimeError: No audio backend is available
pip install soundfile
```

#### 内存相关错误
```bash
# 错误：RuntimeError: CUDA out of memory
# 解决：减少batch_size或使用CPU
python train_scale_1.py --device cpu

# 错误：MemoryError during training
# 解决：使用更小的模型配置
# 编辑config.json: "hidden_dim": 16, "batch_size": 1
```

### 2. 调试技巧

#### 启用详细日志
```bash
# 训练时启用详细输出
python train_scale_1.py --verbose

# 推理时启用详细输出
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/audio/test.wav \
  --verbose
```

#### 检查模型状态
```python
import torch

# 检查模型文件
checkpoint = torch.load('checkpoints/best_model.pth', map_location='cpu')
print(f"训练轮数: {checkpoint.get('epoch', 'Unknown')}")
print(f"验证损失: {checkpoint.get('best_val_loss', 'Unknown')}")
print(f"模型配置: {checkpoint.get('config', {})}")
```

#### 验证数据质量
```bash
# 检查音频文件质量
python -c "
import librosa
import os

for f in os.listdir('data/audio'):
    if f.endswith('.wav'):
        try:
            y, sr = librosa.load(f'data/audio/{f}', sr=None)
            print(f'{f}: {sr}Hz, {len(y)/sr:.2f}秒')
        except Exception as e:
            print(f'{f}: 错误 - {e}')
"
```

---

## 💡 最佳实践

### 1. 数据准备最佳实践

#### 音频质量要求
- **采样率**: 48kHz（推荐）或44.1kHz
- **格式**: WAV无损格式
- **时长**: 1-5秒为最佳
- **内容**: 清晰的中文数字发音
- **环境**: 低噪声环境录制

#### 数据组织
```bash
# 推荐的目录结构
data/
├── raw_audio/          # 原始录音文件
├── audio/             # 处理后的训练音频
├── labels.csv         # 标签文件
├── features/          # 预计算特征（可选）
└── validation/        # 验证集（可选）
```

### 2. 训练最佳实践

#### 小数据集训练（<50样本）
```json
{
  "batch_size": 1,
  "learning_rate": 0.00005,
  "hidden_dim": 32,
  "encoder_layers": 2,
  "decoder_layers": 2,
  "dropout": 0.1,
  "num_epochs": 200
}
```

#### 中等数据集训练（50-200样本）
```json
{
  "batch_size": 2,
  "learning_rate": 0.0001,
  "hidden_dim": 64,
  "encoder_layers": 3,
  "decoder_layers": 3,
  "dropout": 0.15,
  "num_epochs": 150
}
```

#### 训练监控
```bash
# 实时监控训练
tensorboard --logdir runs &
python train_scale_1.py

# 定期保存检查点
# 在config.json中设置: "save_every": 20
```

### 3. 推理最佳实践

#### 选择合适的推理模式
```bash
# 开发测试阶段：使用音频输入模式
python dual_input_inference.py --mode audio --input test.wav

# 生产环境：使用频谱输入模式
python batch_preprocess.py --audio_dir production_audio/
python dual_input_inference.py --mode spectrogram --input features/
```

#### 批量处理优化
```bash
# 1. 预处理所有音频
python batch_preprocess.py --audio_dir data/audio --output_dir data/features

# 2. 批量推理
python dual_input_inference.py \
  --model checkpoints/best_model.pth \
  --input data/features/ \
  --mode spectrogram \
  --output results.csv
```

### 4. 部署最佳实践

#### 模型优化
```python
# 模型量化（减少模型大小）
import torch

model = torch.load('checkpoints/best_model.pth')
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
torch.save(quantized_model, 'checkpoints/quantized_model.pth')
```

#### 服务化部署
```python
# 简单的Flask API示例
from flask import Flask, request, jsonify
from dual_input_inference import DualInputSpeechRecognizer

app = Flask(__name__)
recognizer = DualInputSpeechRecognizer("checkpoints/best_model.pth")

@app.route('/recognize', methods=['POST'])
def recognize():
    if 'audio' in request.files:
        # 音频输入模式
        audio_file = request.files['audio']
        audio_file.save('temp_audio.wav')
        result = recognizer.recognize_from_audio('temp_audio.wav')
    elif 'spectrogram' in request.json:
        # 频谱输入模式
        import numpy as np
        spectrogram = np.array(request.json['spectrogram'])
        result = recognizer.recognize_from_spectrogram_array(spectrogram)
    
    return jsonify(result)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

---

## 📞 技术支持

### 获取帮助
- 查看项目README.md
- 运行 `python <script> --help` 查看命令行参数
- 检查 `tests/` 目录中的测试用例

### 性能基准
- **小数据集（10样本）**: 训练5分钟，准确率>90%
- **中等数据集（50样本）**: 训练15分钟，准确率>95%
- **推理速度**: 音频模式3秒/文件，频谱模式0.5秒/文件

### 系统要求
- **最低**: Python 3.7+, 4GB RAM, CPU
- **推荐**: Python 3.8+, 8GB RAM, GPU (可选)
- **操作系统**: Windows 10+, macOS 10.14+, Ubuntu 18.04+

---

*本操作指南涵盖了WaveSpectra2Text双输入语音识别系统的完整使用流程。如有问题，请参考故障排除部分或查看项目文档。*