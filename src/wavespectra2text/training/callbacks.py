#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒå›è°ƒç³»ç»Ÿ
æä¾›æ—©åœã€æ£€æŸ¥ç‚¹ä¿å­˜ã€æ—¥å¿—è®°å½•ç­‰å›è°ƒåŠŸèƒ½
"""

import os
import torch
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from pathlib import Path


class BaseCallback(ABC):
    """å›è°ƒåŸºç±»"""
    
    def __init__(self):
        self.trainer = None
    
    def set_trainer(self, trainer):
        """è®¾ç½®è®­ç»ƒå™¨å¼•ç”¨"""
        self.trainer = trainer
    
    @abstractmethod
    def on_epoch_begin(self, epoch: int, logs: Dict[str, Any]) -> None:
        """epochå¼€å§‹æ—¶çš„å›è°ƒ"""
        pass
    
    @abstractmethod
    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> None:
        """epochç»“æŸæ—¶çš„å›è°ƒ"""
        pass
    
    def on_train_begin(self, logs: Dict[str, Any]) -> None:
        """è®­ç»ƒå¼€å§‹æ—¶çš„å›è°ƒ"""
        pass
    
    def on_train_end(self, logs: Dict[str, Any]) -> None:
        """è®­ç»ƒç»“æŸæ—¶çš„å›è°ƒ"""
        pass


class EarlyStoppingCallback(BaseCallback):
    """æ—©åœå›è°ƒ"""
    
    def __init__(self, monitor: str = 'val_loss', patience: int = 10, 
                 min_delta: float = 0.0, mode: str = 'min'):
        """
        åˆå§‹åŒ–æ—©åœå›è°ƒ
        
        Args:
            monitor: ç›‘æ§çš„æŒ‡æ ‡åç§°
            patience: è€å¿ƒå€¼ï¼ˆå¤šå°‘ä¸ªepochæ²¡æœ‰æ”¹å–„å°±åœæ­¢ï¼‰
            min_delta: æœ€å°æ”¹å–„é˜ˆå€¼
            mode: 'min' æˆ– 'max'ï¼Œè¡¨ç¤ºæŒ‡æ ‡è¶Šå°è¶Šå¥½è¿˜æ˜¯è¶Šå¤§è¶Šå¥½
        """
        super().__init__()
        self.monitor = monitor
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.best_value = None
        self.wait_count = 0
        self.stopped_epoch = 0
    
    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> None:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ"""
        if self.monitor not in logs:
            return
        
        current_value = logs[self.monitor]
        
        if self.best_value is None:
            self.best_value = current_value
            return
        
        if self.mode == 'min':
            improved = current_value < self.best_value - self.min_delta
        else:
            improved = current_value > self.best_value + self.min_delta
        
        if improved:
            self.best_value = current_value
            self.wait_count = 0
        else:
            self.wait_count += 1
        
        if self.wait_count >= self.patience:
            self.stopped_epoch = epoch
            self.trainer.should_stop = True
            print(f"æ—©åœè§¦å‘ - {self.monitor} è¿ç»­ {self.patience} ä¸ªepochæœªæ”¹å–„")
    
    def on_epoch_begin(self, epoch: int, logs: Dict[str, Any]) -> None:
        """epochå¼€å§‹æ—¶çš„å›è°ƒ"""
        pass


class CheckpointCallback(BaseCallback):
    """æ£€æŸ¥ç‚¹ä¿å­˜å›è°ƒ"""
    
    def __init__(self, filepath: str, monitor: str = 'val_loss', 
                 save_best_only: bool = True, mode: str = 'min',
                 save_every: int = 10):
        """
        åˆå§‹åŒ–æ£€æŸ¥ç‚¹å›è°ƒ
        
        Args:
            filepath: ä¿å­˜è·¯å¾„æ¨¡æ¿
            monitor: ç›‘æ§çš„æŒ‡æ ‡
            save_best_only: æ˜¯å¦åªä¿å­˜æœ€ä½³æ¨¡å‹
            mode: 'min' æˆ– 'max'
            save_every: æ¯å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡
        """
        super().__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.save_best_only = save_best_only
        self.mode = mode
        self.save_every = save_every
        self.best_value = None
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        should_save = False
        
        if self.save_best_only and self.monitor in logs:
            current_value = logs[self.monitor]
            
            if self.best_value is None:
                self.best_value = current_value
                should_save = True
            elif self.mode == 'min' and current_value < self.best_value:
                self.best_value = current_value
                should_save = True
            elif self.mode == 'max' and current_value > self.best_value:
                self.best_value = current_value
                should_save = True
        elif (epoch + 1) % self.save_every == 0:
            should_save = True
        
        if should_save:
            self._save_checkpoint(epoch, logs)
    
    def on_epoch_begin(self, epoch: int, logs: Dict[str, Any]) -> None:
        """epochå¼€å§‹æ—¶çš„å›è°ƒ"""
        pass
    
    def _save_checkpoint(self, epoch: int, logs: Dict[str, Any]) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.trainer.model.state_dict(),
            'optimizer_state_dict': self.trainer.optimizer.state_dict(),
            'scheduler_state_dict': self.trainer.scheduler.state_dict() if self.trainer.scheduler else None,
            'best_val_loss': self.trainer.best_val_loss,
            'train_losses': self.trainer.train_losses,
            'val_losses': self.trainer.val_losses,
            'config': self.trainer.config,
            'logs': logs
        }
        
        # ç”Ÿæˆæ–‡ä»¶å
        if self.save_best_only:
            filepath = self.filepath.replace('{epoch}', 'best')
        else:
            filepath = self.filepath.format(epoch=epoch + 1)
        
        torch.save(checkpoint, filepath)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")


class LoggingCallback(BaseCallback):
    """æ—¥å¿—è®°å½•å›è°ƒ"""
    
    def __init__(self, log_dir: str = 'experiments/logs'):
        """
        åˆå§‹åŒ–æ—¥å¿—å›è°ƒ
        
        Args:
            log_dir: æ—¥å¿—ç›®å½•
        """
        super().__init__()
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
    
    def on_train_begin(self, logs: Dict[str, Any]) -> None:
        """è®­ç»ƒå¼€å§‹æ—¶è®°å½•"""
        print("ğŸš€ è®­ç»ƒå¼€å§‹")
        print(f"ğŸ“Š é…ç½®: {self.trainer.config}")
        print(f"ğŸ“ æ—¥å¿—ç›®å½•: {self.log_dir}")
    
    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> None:
        """è®°å½•epochä¿¡æ¯"""
        print(f"Epoch {epoch + 1}/{self.trainer.config['num_epochs']}")
        print(f"  è®­ç»ƒæŸå¤±: {logs.get('train_loss', 0):.4f}")
        print(f"  è®­ç»ƒå‡†ç¡®ç‡: {logs.get('train_acc', 0):.3f}")
        print(f"  éªŒè¯æŸå¤±: {logs.get('val_loss', 0):.4f}")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {logs.get('val_acc', 0):.3f}")
        print(f"  å­¦ä¹ ç‡: {logs.get('learning_rate', 0):.6f}")
    
    def on_epoch_begin(self, epoch: int, logs: Dict[str, Any]) -> None:
        """epochå¼€å§‹æ—¶çš„å›è°ƒ"""
        pass
    
    def on_train_end(self, logs: Dict[str, Any]) -> None:
        """è®­ç»ƒç»“æŸæ—¶è®°å½•"""
        print("âœ… è®­ç»ƒå®Œæˆ")
        if hasattr(self.trainer, 'stopped_epoch'):
            print(f"ğŸ“ˆ æœ€ä½³éªŒè¯æŸå¤±: {self.trainer.best_val_loss:.4f}")


class TensorBoardCallback(BaseCallback):
    """TensorBoardå›è°ƒ"""
    
    def __init__(self, log_dir: str = 'experiments/runs'):
        """
        åˆå§‹åŒ–TensorBoardå›è°ƒ
        
        Args:
            log_dir: TensorBoardæ—¥å¿—ç›®å½•
        """
        super().__init__()
        self.log_dir = log_dir
        self.writer = None
    
    def on_train_begin(self, logs: Dict[str, Any]) -> None:
        """åˆå§‹åŒ–TensorBoardå†™å…¥å™¨"""
        from torch.utils.tensorboard import SummaryWriter
        self.writer = SummaryWriter(self.log_dir)
    
    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> None:
        """è®°å½•åˆ°TensorBoard"""
        if self.writer:
            for key, value in logs.items():
                if isinstance(value, (int, float)):
                    self.writer.add_scalar(f'Epoch/{key}', value, epoch)
    
    def on_train_end(self, logs: Dict[str, Any]) -> None:
        """å…³é—­TensorBoardå†™å…¥å™¨"""
        if self.writer:
            self.writer.close()
    
    def on_epoch_begin(self, epoch: int, logs: Dict[str, Any]) -> None:
        """epochå¼€å§‹æ—¶çš„å›è°ƒ"""
        pass


class CallbackList:
    """å›è°ƒåˆ—è¡¨ç®¡ç†å™¨"""
    
    def __init__(self, callbacks: list = None):
        """
        åˆå§‹åŒ–å›è°ƒåˆ—è¡¨
        
        Args:
            callbacks: å›è°ƒåˆ—è¡¨
        """
        self.callbacks = callbacks or []
        self.trainer = None
    
    def add_callback(self, callback: BaseCallback) -> None:
        """æ·»åŠ å›è°ƒ"""
        self.callbacks.append(callback)
    
    def set_trainer(self, trainer) -> None:
        """è®¾ç½®è®­ç»ƒå™¨"""
        self.trainer = trainer
        for callback in self.callbacks:
            callback.set_trainer(trainer)
    
    def on_train_begin(self, logs: Dict[str, Any]) -> None:
        """è®­ç»ƒå¼€å§‹å›è°ƒ"""
        for callback in self.callbacks:
            callback.on_train_begin(logs)
    
    def on_train_end(self, logs: Dict[str, Any]) -> None:
        """è®­ç»ƒç»“æŸå›è°ƒ"""
        for callback in self.callbacks:
            callback.on_train_end(logs)
    
    def on_epoch_begin(self, epoch: int, logs: Dict[str, Any]) -> None:
        """epochå¼€å§‹å›è°ƒ"""
        for callback in self.callbacks:
            callback.on_epoch_begin(epoch, logs)
    
    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> None:
        """epochç»“æŸå›è°ƒ"""
        for callback in self.callbacks:
            callback.on_epoch_end(epoch, logs)


if __name__ == "__main__":
    print("ğŸ§ª è®­ç»ƒå›è°ƒç³»ç»Ÿæµ‹è¯•")
    print("=" * 50)
    
    print("ğŸ“‹ å¯ç”¨å›è°ƒ:")
    print("  âœ… EarlyStoppingCallback - æ—©åœæœºåˆ¶")
    print("  âœ… CheckpointCallback - æ£€æŸ¥ç‚¹ä¿å­˜")
    print("  âœ… LoggingCallback - æ—¥å¿—è®°å½•")
    print("  âœ… TensorBoardCallback - TensorBoardè®°å½•")
    print("  âœ… CallbackList - å›è°ƒåˆ—è¡¨ç®¡ç†")
    
    print("\nğŸ’¡ ä½¿ç”¨æ–¹å¼:")
    print("from wavespectra2text.training.callbacks import CallbackList, EarlyStoppingCallback")
    print("callbacks = CallbackList([EarlyStoppingCallback(patience=10)])")
    print("callbacks.set_trainer(trainer)")
