"""
åŒè¾“å…¥æ¨¡å¼æ¨ç†ç³»ç»Ÿ
æ”¯æŒä¸¤ç§è¾“å…¥æ–¹å¼ï¼š
1. åŸå§‹éŸ³é¢‘æ–‡ä»¶ (ç³»ç»Ÿå†…é¢„å¤„ç†)
2. å¯¹åº”åŸå§‹éŸ³é¢‘çš„é¢‘è°±ç‰¹å¾ (è·³è¿‡é¢„å¤„ç†)
"""

import os
import torch
import numpy as np
import librosa
import argparse
from pathlib import Path
import time
import json

from ..core.model import create_model
from ..core.vocab import vocab
from ..data.preprocessing import SpectrogramPreprocessor
from ..core.inference import InferenceCore, BatchInference
import warnings

warnings.filterwarnings('ignore')


class DualInputSpeechRecognizer:
    """åŒè¾“å…¥æ¨¡å¼è¯­éŸ³è¯†åˆ«å™¨"""

    def __init__(self, model_path, device='cpu'):
        """
        åˆå§‹åŒ–åŒè¾“å…¥è¯†åˆ«å™¨

        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        print(f"ğŸš€ åˆå§‹åŒ–åŒè¾“å…¥è¯­éŸ³è¯†åˆ«å™¨")
        print(f"è®¾å¤‡: {device}")

        # ä½¿ç”¨ç»Ÿä¸€æ¨ç†æ ¸å¿ƒ
        self.inference_core = InferenceCore(model_path, device)
        self.batch_inference = BatchInference(self.inference_core)

        # ä¿æŒå…¼å®¹æ€§
        self.device = self.inference_core.device
        self.model = self.inference_core.model

        print(f"âœ… æ”¯æŒä¸¤ç§è¾“å…¥æ¨¡å¼:")
        print(f"  1. åŸå§‹éŸ³é¢‘æ–‡ä»¶ (.wav, .mp3, .flacç­‰)")
        print(f"  2. é¢„å¤„ç†é¢‘è°±ç‰¹å¾ (.npyæ–‡ä»¶)")

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        model_info = self.inference_core.get_model_info()
        print(f"ğŸ“‚ æ¨¡å‹åŠ è½½æˆåŠŸ: {model_info['path']}")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters())}")

    # ç§»é™¤é‡å¤çš„æ¨¡å‹åŠ è½½æ–¹æ³•ï¼Œä½¿ç”¨ç»Ÿä¸€æ¨ç†æ ¸å¿ƒ

    def recognize_from_audio(self, audio_path, show_details=True):
        """
        æ¨¡å¼1: ä»åŸå§‹éŸ³é¢‘æ–‡ä»¶è¯†åˆ«
        å®Œæ•´æµç¨‹: éŸ³é¢‘ â†’ é¢‘è°±æå– â†’ æ¨¡å‹æ¨ç† â†’ æ–‡æœ¬
        """
        if show_details:
            print(f"\nğŸµ æ¨¡å¼1: åŸå§‹éŸ³é¢‘è¾“å…¥")
            print(f"æ–‡ä»¶: {audio_path}")
            print("-" * 50)

        # ä½¿ç”¨ç»Ÿä¸€æ¨ç†æ ¸å¿ƒ
        result = self.inference_core.infer_from_audio(audio_path, method='auto')

        if show_details and result['success']:
            print("ğŸ”§ æ­¥éª¤1: éŸ³é¢‘é¢„å¤„ç†")
            print(f"  âœ… é¢‘è°±æå–: {result['spectrogram_shape']}")
            print(f"  â±ï¸  é¢„å¤„ç†è€—æ—¶: {result['preprocessing_time']:.3f}ç§’")
            print("ğŸ§  æ­¥éª¤2: æ¨¡å‹æ¨ç†")
            print(f"  ğŸ¯ æœ€ç»ˆç»“æœ: '{result['text']}'")
            print(f"  â±ï¸  æ¨ç†è€—æ—¶: {result['inference_time']:.3f}ç§’")

        # è½¬æ¢ç»“æœæ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
        return {
            'text': result['text'],
            'success': result['success'],
            'processing_time': {
                'preprocessing': result.get('preprocessing_time', 0),
                'inference': result.get('inference_time', 0),
                'total': result.get('total_time', 0)
            },
            'input_type': 'audio_file',
            'spectrogram_shape': result.get('spectrogram_shape'),
            'method': result.get('method', 'auto'),
            'error': result.get('error')
        }

    def recognize_from_spectrogram(self, spectrogram_path, show_details=True):
        """
        æ¨¡å¼2: ä»é¢„å¤„ç†é¢‘è°±ç‰¹å¾è¯†åˆ«
        å¿«é€Ÿæµç¨‹: é¢‘è°±ç‰¹å¾ â†’ æ¨¡å‹æ¨ç† â†’ æ–‡æœ¬
        """
        if show_details:
            print(f"\nğŸ“Š æ¨¡å¼2: é¢‘è°±ç‰¹å¾è¾“å…¥")
            print(f"æ–‡ä»¶: {spectrogram_path}")
            print("-" * 50)

        try:
            # åŠ è½½é¢‘è°±ç‰¹å¾
            load_start = time.time()
            spectrogram_features = np.load(spectrogram_path)
            load_time = time.time() - load_start

            if show_details:
                print("ğŸ“‚ æ­¥éª¤1: åŠ è½½é¢‘è°±ç‰¹å¾")
                print(f"  âœ… é¢‘è°±åŠ è½½: {spectrogram_features.shape}")
                print(f"  ğŸ“Š æ•°å€¼èŒƒå›´: [{spectrogram_features.min():.3f}, {spectrogram_features.max():.3f}]")
                print(f"  â±ï¸  åŠ è½½è€—æ—¶: {load_time:.3f}ç§’")
                print("  ğŸš€ è·³è¿‡é¢„å¤„ç†ï¼Œç›´æ¥è¿›å…¥æ¨ç†")

            # ä½¿ç”¨ç»Ÿä¸€æ¨ç†æ ¸å¿ƒ
            result = self.inference_core.infer_from_spectrogram(spectrogram_features, method='auto')

            if show_details and result['success']:
                print("ğŸ§  æ­¥éª¤2: æ¨¡å‹æ¨ç†")
                print(f"  ğŸ¯ æœ€ç»ˆç»“æœ: '{result['text']}'")
                print(f"  â±ï¸  æ¨ç†è€—æ—¶: {result['inference_time']:.3f}ç§’")

            # è½¬æ¢ç»“æœæ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
            return {
                'text': result['text'],
                'success': result['success'],
                'processing_time': {
                    'preprocessing': 0.0,  # è·³è¿‡é¢„å¤„ç†
                    'loading': load_time,
                    'inference': result.get('inference_time', 0),
                    'total': load_time + result.get('inference_time', 0)
                },
                'input_type': 'spectrogram_file',
                'spectrogram_shape': result.get('spectrogram_shape'),
                'method': result.get('method', 'auto'),
                'error': result.get('error')
            }

        except Exception as e:
            return {
                'text': '',
                'success': False,
                'error': str(e),
                'input_type': 'spectrogram_file'
            }

    def recognize_from_spectrogram_array(self, spectrogram_array, show_details=True):
        """
        æ¨¡å¼3: ä»å†…å­˜ä¸­çš„é¢‘è°±æ•°ç»„è¯†åˆ«
        ç›´æ¥è¾“å…¥: numpyæ•°ç»„ â†’ æ¨¡å‹æ¨ç† â†’ æ–‡æœ¬
        """
        if show_details:
            print(f"\nğŸ§® æ¨¡å¼3: å†…å­˜é¢‘è°±æ•°ç»„è¾“å…¥")
            print(f"æ•°ç»„å½¢çŠ¶: {spectrogram_array.shape}")
            print("-" * 50)

        try:
            # éªŒè¯è¾“å…¥æ•°ç»„
            if not isinstance(spectrogram_array, np.ndarray):
                raise ValueError("è¾“å…¥å¿…é¡»æ˜¯numpyæ•°ç»„")

            if show_details:
                print("ğŸ§® æ­¥éª¤1: éªŒè¯é¢‘è°±æ•°ç»„")
                print(f"  âœ… æ•°ç»„å½¢çŠ¶: {spectrogram_array.shape}")
                print(f"  ğŸ“Š æ•°å€¼èŒƒå›´: [{spectrogram_array.min():.3f}, {spectrogram_array.max():.3f}]")
                print("  ğŸš€ ç›´æ¥è¿›å…¥æ¨ç† (æ— éœ€åŠ è½½)")

            # ä½¿ç”¨ç»Ÿä¸€æ¨ç†æ ¸å¿ƒ
            result = self.inference_core.infer_from_spectrogram(spectrogram_array, method='auto')

            if show_details and result['success']:
                print("ğŸ§  æ­¥éª¤2: æ¨¡å‹æ¨ç†")
                print(f"  ğŸ¯ æœ€ç»ˆç»“æœ: '{result['text']}'")
                print(f"  â±ï¸  æ¨ç†è€—æ—¶: {result['inference_time']:.3f}ç§’")

            # è½¬æ¢ç»“æœæ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
            return {
                'text': result['text'],
                'success': result['success'],
                'processing_time': {
                    'preprocessing': 0.0,
                    'loading': 0.0,
                    'inference': result.get('inference_time', 0),
                    'total': result.get('inference_time', 0)
                },
                'input_type': 'spectrogram_array',
                'spectrogram_shape': result.get('spectrogram_shape'),
                'method': result.get('method', 'auto'),
                'error': result.get('error')
            }

        except Exception as e:
            return {
                'text': '',
                'success': False,
                'error': str(e),
                'input_type': 'spectrogram_array'
            }

    # ç§»é™¤é‡å¤çš„æ¨ç†æ–¹æ³•ï¼Œå·²ç»Ÿä¸€åˆ° inference_core æ¨¡å—

    def auto_recognize(self, input_path, show_details=True):
        """
        è‡ªåŠ¨è¯†åˆ«è¾“å…¥ç±»å‹å¹¶å¤„ç†
        æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨åˆ¤æ–­æ˜¯éŸ³é¢‘æ–‡ä»¶è¿˜æ˜¯é¢‘è°±æ–‡ä»¶
        """
        if show_details:
            print(f"\nğŸ¤– è‡ªåŠ¨æ¨¡å¼: {input_path}")

        file_ext = Path(input_path).suffix.lower()

        # éŸ³é¢‘æ–‡ä»¶æ‰©å±•å
        audio_extensions = ['.wav', '.mp3', '.flac', '.m4a', '.ogg']
        # é¢‘è°±æ–‡ä»¶æ‰©å±•å
        spectrogram_extensions = ['.npy', '.npz']

        if file_ext in audio_extensions:
            if show_details:
                print("ğŸµ æ£€æµ‹åˆ°éŸ³é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨éŸ³é¢‘è¾“å…¥æ¨¡å¼")
            return self.recognize_from_audio(input_path, show_details)

        elif file_ext in spectrogram_extensions:
            if show_details:
                print("ğŸ“Š æ£€æµ‹åˆ°é¢‘è°±æ–‡ä»¶ï¼Œä½¿ç”¨é¢‘è°±è¾“å…¥æ¨¡å¼")
            return self.recognize_from_spectrogram(input_path, show_details)

        else:
            return {
                'text': '',
                'success': False,
                'error': f'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}',
                'input_type': 'unknown'
            }


def create_external_spectrogram_demo():
    """åˆ›å»ºå¤–éƒ¨é¢‘è°±ç‰¹å¾å¤„ç†æ¼”ç¤º"""
    print("ğŸ”§ å¤–éƒ¨é¢‘è°±ç‰¹å¾å¤„ç†æ¼”ç¤º")
    print("=" * 60)

    demo_code = '''
# åœºæ™¯: åœ¨å…¶ä»–ç³»ç»Ÿä¸­é¢„å¤„ç†éŸ³é¢‘ï¼Œç„¶åä¼ é€’é¢‘è°±ç‰¹å¾ç»™è¯†åˆ«ç³»ç»Ÿ

# === å¤–éƒ¨ç³»ç»Ÿ (ä¾‹å¦‚: å®æ—¶éŸ³é¢‘å¤„ç†ç³»ç»Ÿ) ===
import librosa
import numpy as np

def external_audio_preprocessing(audio_path):
    """å¤–éƒ¨ç³»ç»Ÿçš„éŸ³é¢‘é¢„å¤„ç† - ä½¿ç”¨ç»Ÿä¸€å·¥å…·"""
    from common_utils import AudioProcessor
    
    # ä½¿ç”¨ç»Ÿä¸€çš„éŸ³é¢‘å¤„ç†å™¨
    processor = AudioProcessor(sample_rate=48000, n_fft=1024, hop_length=512, max_length=200)
    return processor.extract_spectrogram(audio_path)

# å¤–éƒ¨ç³»ç»Ÿå¤„ç†éŸ³é¢‘
audio_file = "external_audio.wav"
spectrogram = external_audio_preprocessing(audio_file)

# ä¿å­˜é¢‘è°±ç‰¹å¾
np.save("external_spectrogram.npy", spectrogram)

# === è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ ===
from dual_input_inference import DualInputSpeechRecognizer

# åˆå§‹åŒ–è¯†åˆ«å™¨
recognizer = DualInputSpeechRecognizer("checkpoints/best_model.pth")

# ç›´æ¥ä»é¢‘è°±ç‰¹å¾è¯†åˆ« (è·³è¿‡é¢„å¤„ç†ï¼Œé€Ÿåº¦æ›´å¿«)
result = recognizer.recognize_from_spectrogram("external_spectrogram.npy")
print(f"è¯†åˆ«ç»“æœ: {result['text']}")

# æˆ–è€…ä½¿ç”¨å†…å­˜æ•°ç»„
result = recognizer.recognize_from_spectrogram_array(spectrogram)
print(f"è¯†åˆ«ç»“æœ: {result['text']}")
    '''

    print("ğŸ’» ä½¿ç”¨ç¤ºä¾‹ä»£ç :")
    print(demo_code)

    return demo_code


def compare_input_modes():
    """å¯¹æ¯”ä¸¤ç§è¾“å…¥æ¨¡å¼çš„æ€§èƒ½"""
    print("\nğŸ“Š è¾“å…¥æ¨¡å¼æ€§èƒ½å¯¹æ¯”")
    print("=" * 60)

    comparison = {
        "ç‰¹å¾": ["é¢„å¤„ç†æ—¶é—´", "æ¨ç†æ—¶é—´", "æ€»æ—¶é—´", "å†…å­˜å ç”¨", "é€‚ç”¨åœºæ™¯"],
        "éŸ³é¢‘è¾“å…¥": ["2-3ç§’", "0.3-0.5ç§’", "2.5-3.5ç§’", "ä¸­ç­‰", "ä¸€èˆ¬ä½¿ç”¨ã€å¼€å‘æµ‹è¯•"],
        "é¢‘è°±è¾“å…¥": ["0ç§’", "0.3-0.5ç§’", "0.3-0.5ç§’", "ä½", "é«˜æ€§èƒ½ã€æ‰¹é‡å¤„ç†ã€å®æ—¶ç³»ç»Ÿ"]
    }

    print(f"{'ç‰¹å¾':<15} {'éŸ³é¢‘è¾“å…¥':<20} {'é¢‘è°±è¾“å…¥':<20}")
    print("-" * 55)

    for i, feature in enumerate(comparison["ç‰¹å¾"]):
        audio_val = comparison["éŸ³é¢‘è¾“å…¥"][i]
        spec_val = comparison["é¢‘è°±è¾“å…¥"][i]
        print(f"{feature:<15} {audio_val:<20} {spec_val:<20}")

    print(f"\nğŸ’¡ é€‰æ‹©å»ºè®®:")
    print(f"  ğŸµ éŸ³é¢‘è¾“å…¥: é€‚åˆä¸€èˆ¬ä½¿ç”¨ï¼Œå®Œæ•´æµç¨‹")
    print(f"  ğŸ“Š é¢‘è°±è¾“å…¥: é€‚åˆé«˜æ€§èƒ½éœ€æ±‚ï¼Œå·²æœ‰é¢„å¤„ç†ç³»ç»Ÿ")
    print(f"  ğŸ¤– è‡ªåŠ¨æ¨¡å¼: æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨é€‰æ‹©")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŒè¾“å…¥æ¨¡å¼è¯­éŸ³è¯†åˆ«')
    parser.add_argument('--model', type=str, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--input', type=str, help='è¾“å…¥æ–‡ä»¶ (éŸ³é¢‘æˆ–é¢‘è°±)')
    parser.add_argument('--mode', type=str, default='auto',
                        choices=['auto', 'audio', 'spectrogram'],
                        help='è¾“å…¥æ¨¡å¼')
    parser.add_argument('--device', type=str, default='cpu', help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--demo', action='store_true', help='æ˜¾ç¤ºæ¼”ç¤ºä»£ç ')
    parser.add_argument('--compare', action='store_true', help='æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”')

    args = parser.parse_args()

    if args.demo:
        create_external_spectrogram_demo()
        return

    if args.compare:
        compare_input_modes()
        return

    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.model or not args.input:
        parser.error("--model å’Œ --input å‚æ•°æ˜¯å¿…éœ€çš„")

    print("ğŸ¯ åŒè¾“å…¥æ¨¡å¼è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ")
    print("=" * 60)

    try:
        # åˆ›å»ºè¯†åˆ«å™¨
        recognizer = DualInputSpeechRecognizer(args.model, args.device)

        # æ ¹æ®æ¨¡å¼å¤„ç†
        if args.mode == 'auto':
            result = recognizer.auto_recognize(args.input)
        elif args.mode == 'audio':
            result = recognizer.recognize_from_audio(args.input)
        elif args.mode == 'spectrogram':
            result = recognizer.recognize_from_spectrogram(args.input)

        # æ˜¾ç¤ºç»“æœ
        if result['success']:
            print(f"\nğŸ‰ è¯†åˆ«æˆåŠŸ!")
            print(f"è¾“å…¥ç±»å‹: {result['input_type']}")
            print(f"è¯†åˆ«ç»“æœ: '{result['text']}'")
            print(f"æ€»è€—æ—¶: {result['processing_time']['total']:.3f}ç§’")

            if 'preprocessing' in result['processing_time']:
                print(f"  é¢„å¤„ç†: {result['processing_time']['preprocessing']:.3f}ç§’")
            if 'loading' in result['processing_time']:
                print(f"  åŠ è½½: {result['processing_time']['loading']:.3f}ç§’")
            if 'inference' in result['processing_time']:
                print(f"  æ¨ç†: {result['processing_time']['inference']:.3f}ç§’")
        else:
            print(f"âŒ è¯†åˆ«å¤±è´¥: {result['error']}")

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()