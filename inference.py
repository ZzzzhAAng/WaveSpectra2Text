#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆç‰ˆæ¨ç†è„šæœ¬
é›†æˆä¿®å¤çš„æŸæœç´¢å’Œæ™ºèƒ½å›é€€æœºåˆ¶
"""

import os
import torch
import numpy as np
import librosa
import argparse
from tqdm import tqdm

from model import create_model
from vocab import vocab
import warnings

warnings.filterwarnings('ignore')


class FinalSpeechRecognizer:
    """æœ€ç»ˆç‰ˆè¯­éŸ³è¯†åˆ«å™¨ - é›†æˆæ‰€æœ‰æ”¹è¿›"""

    def __init__(self, model_path, device='cpu'):
        self.device = torch.device(device)
        self.model = self._load_model(model_path)
        self.model.eval()

        # éŸ³é¢‘å¤„ç†å‚æ•°
        self.sample_rate = 48000
        self.n_fft = 1024
        self.hop_length = 512
        self.max_length = 200

    def _load_model(self, model_path):
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

        checkpoint = torch.load(model_path, map_location=self.device)
        config = checkpoint.get('config', {})

        model = create_model(
            vocab_size=vocab.vocab_size,
            hidden_dim=config.get('hidden_dim', 256),
            encoder_layers=config.get('encoder_layers', 4),
            decoder_layers=config.get('decoder_layers', 4),
            dropout=config.get('dropout', 0.1)
        )

        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)

        print(f"æ¨¡å‹å·²åŠ è½½: {model_path}")
        print(f"è®­ç»ƒepoch: {checkpoint.get('epoch', 'Unknown')}")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {checkpoint.get('best_val_loss', 'Unknown')}")

        return model

    def _extract_spectrogram(self, audio_path):
        """ä»éŸ³é¢‘æ–‡ä»¶æå–é¢‘è°±ç‰¹å¾"""
        audio, sr = librosa.load(audio_path, sr=self.sample_rate)
        stft = librosa.stft(audio, n_fft=self.n_fft, hop_length=self.hop_length)
        magnitude = np.abs(stft)
        log_magnitude = np.log1p(magnitude)
        spectrogram = log_magnitude.T

        if len(spectrogram) > self.max_length:
            spectrogram = spectrogram[:self.max_length]
        else:
            pad_length = self.max_length - len(spectrogram)
            spectrogram = np.pad(spectrogram, ((0, pad_length), (0, 0)), mode='constant')

        return torch.FloatTensor(spectrogram).unsqueeze(0)

    def _greedy_decode(self, encoder_output, max_length=10):
        """è´ªå©ªè§£ç """
        decoded_seq = torch.LongTensor([[vocab.get_sos_idx()]]).to(self.device)

        for step in range(max_length):
            with torch.no_grad():
                output = self.model.decode_step(decoded_seq, encoder_output)
                next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)
                decoded_seq = torch.cat([decoded_seq, next_token], dim=1)

                if next_token.item() == vocab.get_eos_idx():
                    break

        return decoded_seq.squeeze(0)

    def _fixed_beam_search(self, encoder_output, beam_size=3, max_length=10):
        """ä¿®å¤ç‰ˆæŸæœç´¢ - è§£å†³EOSè¿‡æ—©é—®é¢˜"""
        device = encoder_output.device
        beams = [(torch.LongTensor([[vocab.get_sos_idx()]]).to(device), 0.0)]

        for step in range(max_length):
            new_beams = []

            for seq, score in beams:
                if seq[0, -1].item() == vocab.get_eos_idx():
                    # å¯¹è¿‡çŸ­åºåˆ—æ·»åŠ æƒ©ç½š
                    if seq.size(1) < 3:
                        score -= 2.0  # æƒ©ç½šè¿‡çŸ­åºåˆ—
                    new_beams.append((seq, score))
                    continue

                with torch.no_grad():
                    output = self.model.decode_step(seq, encoder_output)
                    logits = output[:, -1, :]

                    # å¯¹è¿‡æ—©çš„EOSæ·»åŠ æƒ©ç½š
                    if seq.size(1) < 3:
                        logits[0, vocab.get_eos_idx()] -= 1.5  # æƒ©ç½šè¿‡æ—©ç»“æŸ

                    probs = torch.softmax(logits, dim=-1)

                top_probs, top_indices = torch.topk(probs, beam_size)

                for i in range(beam_size):
                    new_seq = torch.cat([seq, top_indices[:, i:i + 1]], dim=1)
                    new_score = score + torch.log(top_probs[:, i]).item()

                    # é•¿åº¦å¥–åŠ± - é¼“åŠ±ç”Ÿæˆæ›´é•¿åºåˆ—
                    if top_indices[:, i].item() != vocab.get_eos_idx():
                        new_score += 0.1

                    new_beams.append((new_seq, new_score))

            new_beams.sort(key=lambda x: x[1], reverse=True)
            beams = new_beams[:beam_size]

            if all(seq[0, -1].item() == vocab.get_eos_idx() for seq, _ in beams):
                break

        best_seq, best_score = beams[0]
        return best_seq.squeeze(0), best_score

    def recognize_file(self, audio_path, method='auto', beam_size=3):
        """æ™ºèƒ½è¯†åˆ«æ–‡ä»¶ - æ”¯æŒå¤šç§è§£ç æ–¹æ³•"""
        try:
            spectrogram = self._extract_spectrogram(audio_path).to(self.device)

            with torch.no_grad():
                encoder_output = self.model.encode(spectrogram)

                if method == 'greedy':
                    # å¼ºåˆ¶ä½¿ç”¨è´ªå©ªè§£ç 
                    seq = self._greedy_decode(encoder_output)
                    text = vocab.decode(seq.tolist())
                    return {
                        'text': text,
                        'method': 'greedy',
                        'success': True
                    }

                elif method == 'beam':
                    # å¼ºåˆ¶ä½¿ç”¨æŸæœç´¢
                    seq, score = self._fixed_beam_search(encoder_output, beam_size)
                    text = vocab.decode(seq.tolist())
                    return {
                        'text': text,
                        'method': 'fixed_beam_search',
                        'score': score,
                        'success': True
                    }

                else:  # method == 'auto'
                    # æ™ºèƒ½é€‰æ‹©ï¼šå…ˆå°è¯•ä¿®å¤ç‰ˆæŸæœç´¢
                    beam_seq, beam_score = self._fixed_beam_search(encoder_output, beam_size)
                    beam_text = vocab.decode(beam_seq.tolist())

                    # å¦‚æœæŸæœç´¢ç»“æœåˆç†ï¼Œä½¿ç”¨å®ƒ
                    if beam_text and len(beam_text.strip()) > 0 and not beam_text.strip() == '':
                        return {
                            'text': beam_text,
                            'method': 'fixed_beam_search',
                            'score': beam_score,
                            'success': True
                        }
                    else:
                        # å¦åˆ™å›é€€åˆ°è´ªå©ªè§£ç 
                        greedy_seq = self._greedy_decode(encoder_output)
                        greedy_text = vocab.decode(greedy_seq.tolist())
                        return {
                            'text': greedy_text,
                            'method': 'greedy_fallback',
                            'success': True,
                            'note': 'æŸæœç´¢ä¸ºç©ºï¼Œä½¿ç”¨è´ªå©ªè§£ç '
                        }

        except Exception as e:
            return {
                'text': '',
                'success': False,
                'error': str(e)
            }

    def recognize_batch(self, audio_paths, method='auto', beam_size=3):
        """æ‰¹é‡è¯†åˆ«"""
        results = []

        for audio_path in tqdm(audio_paths, desc="è¯†åˆ«ä¸­"):
            result = self.recognize_file(audio_path, method, beam_size)
            result['file'] = audio_path
            results.append(result)

        return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æœ€ç»ˆç‰ˆè¯­éŸ³è¯†åˆ«æ¨ç†')
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--audio', type=str, help='å•ä¸ªéŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--audio_dir', type=str, help='éŸ³é¢‘æ–‡ä»¶ç›®å½•')
    parser.add_argument('--method', type=str, default='auto',
                        choices=['auto', 'greedy', 'beam'],
                        help='è§£ç æ–¹æ³•: auto(æ™ºèƒ½é€‰æ‹©), greedy(è´ªå©ª), beam(æŸæœç´¢)')
    parser.add_argument('--beam_size', type=int, default=3, help='æŸæœç´¢å¤§å°')
    parser.add_argument('--device', type=str, default='cpu', help='è®¡ç®—è®¾å¤‡')

    args = parser.parse_args()

    # åˆ›å»ºè¯†åˆ«å™¨
    try:
        recognizer = FinalSpeechRecognizer(args.model, args.device)
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return

    if args.audio:
        # è¯†åˆ«å•ä¸ªæ–‡ä»¶
        print(f"è¯†åˆ«æ–‡ä»¶: {args.audio}")
        result = recognizer.recognize_file(args.audio, args.method, args.beam_size)

        if result['success']:
            print(f"è¯†åˆ«ç»“æœ: {result['text']}")
            print(f"ä½¿ç”¨æ–¹æ³•: {result['method']}")
            if 'score' in result:
                print(f"å¾—åˆ†: {result['score']:.4f}")
            if 'note' in result:
                print(f"æ³¨æ„: {result['note']}")
        else:
            print(f"è¯†åˆ«å¤±è´¥: {result['error']}")

    elif args.audio_dir:
        # æ‰¹é‡è¯†åˆ«ç›®å½•ä¸­çš„æ–‡ä»¶
        print(f"æ‰¹é‡è¯†åˆ«ç›®å½•: {args.audio_dir}")
        print(f"è§£ç æ–¹æ³•: {args.method}")

        audio_files = []
        for ext in ['.wav', '.mp3', '.flac', '.m4a']:
            audio_files.extend([
                os.path.join(args.audio_dir, f)
                for f in os.listdir(args.audio_dir)
                if f.lower().endswith(ext)
            ])

        if not audio_files:
            print("æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return

        results = recognizer.recognize_batch(audio_files, args.method, args.beam_size)

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r['success'])
        method_counts = {}
        for r in results:
            method = r.get('method', 'unknown')
            method_counts[method] = method_counts.get(method, 0) + 1

        print(f"\nğŸ“Š è¯†åˆ«ç»Ÿè®¡:")
        print(f"  æ€»æ–‡ä»¶æ•°: {len(results)}")
        print(f"  æˆåŠŸè¯†åˆ«: {success_count}")
        for method, count in method_counts.items():
            print(f"  {method}: {count}")

        # æ‰“å°ç»“æœ
        print(f"\nğŸ“‹ è¯†åˆ«ç»“æœ:")
        for result in results:
            filename = os.path.basename(result['file'])
            status = "âœ…" if result['success'] else "âŒ"
            method = result.get('method', 'unknown')
            print(f"  {status} {filename}: '{result['text']}' ({method})")

    else:
        print("è¯·æŒ‡å®šè¦è¯†åˆ«çš„éŸ³é¢‘æ–‡ä»¶æˆ–ç›®å½•")
        parser.print_help()


if __name__ == "__main__":
    main()