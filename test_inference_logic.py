#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨ç†é€»è¾‘æµ‹è¯• - ä½¿ç”¨è™šæ‹Ÿæ¨¡å‹æµ‹è¯•å®Œæ•´æ¨ç†æµç¨‹
"""

import os
import torch
import numpy as np
import tempfile
import json
from pathlib import Path

def create_dummy_model_checkpoint():
    """åˆ›å»ºè™šæ‹Ÿæ¨¡å‹æ£€æŸ¥ç‚¹ç”¨äºæµ‹è¯•"""
    from model import create_model
    from vocab import vocab
    
    print("ğŸ”§ åˆ›å»ºè™šæ‹Ÿæ¨¡å‹æ£€æŸ¥ç‚¹...")
    
    # åˆ›å»ºå°æ¨¡å‹
    model = create_model(
        vocab_size=vocab.vocab_size,
        hidden_dim=64,
        encoder_layers=2,
        decoder_layers=2,
        dropout=0.1
    )
    
    # éšæœºåˆå§‹åŒ–æƒé‡
    for param in model.parameters():
        if param.dim() > 1:
            torch.nn.init.xavier_uniform_(param)
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹
    checkpoint = {
        'epoch': 50,
        'model_state_dict': model.state_dict(),
        'best_val_loss': 0.5,
        'config': {
            'hidden_dim': 64,
            'encoder_layers': 2,
            'decoder_layers': 2,
            'dropout': 0.1
        }
    }
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    dummy_model_path = 'dummy_model.pth'
    torch.save(checkpoint, dummy_model_path)
    
    print(f"âœ… è™šæ‹Ÿæ¨¡å‹ä¿å­˜åˆ°: {dummy_model_path}")
    return dummy_model_path

def test_inference_with_dummy_model():
    """ä½¿ç”¨è™šæ‹Ÿæ¨¡å‹æµ‹è¯•æ¨ç†"""
    print("ğŸ§ª ä½¿ç”¨è™šæ‹Ÿæ¨¡å‹æµ‹è¯•æ¨ç†...")
    
    try:
        # åˆ›å»ºè™šæ‹Ÿæ¨¡å‹
        dummy_model_path = create_dummy_model_checkpoint()
        
        # å¯¼å…¥æ¨ç†ç±»
        from inference import SpeechRecognizer
        
        # åˆ›å»ºè¯†åˆ«å™¨
        recognizer = SpeechRecognizer(dummy_model_path, device='cpu')
        print("âœ… æ¨ç†å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å•æ–‡ä»¶æ¨ç†
        test_audio = "data/audio/Chinese_Number_01.wav"
        if os.path.exists(test_audio):
            print(f"ğŸµ æµ‹è¯•å•æ–‡ä»¶æ¨ç†: {test_audio}")
            
            # è´ªå©ªè§£ç 
            result_greedy = recognizer.recognize_file(test_audio, use_beam_search=False)
            print(f"  è´ªå©ªè§£ç ç»“æœ: '{result_greedy['text']}' (æˆåŠŸ: {result_greedy['success']})")
            
            # æŸæœç´¢è§£ç 
            result_beam = recognizer.recognize_file(test_audio, use_beam_search=True, beam_size=3)
            print(f"  æŸæœç´¢ç»“æœ: '{result_beam['text']}' (æˆåŠŸ: {result_beam['success']}, å¾—åˆ†: {result_beam.get('score', 'N/A')})")
            
        else:
            print(f"âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_audio}")
        
        # æµ‹è¯•æ‰¹é‡æ¨ç†
        test_files = [f"data/audio/Chinese_Number_0{i}.wav" for i in range(1, 4)]
        existing_files = [f for f in test_files if os.path.exists(f)]
        
        if existing_files:
            print(f"ğŸµ æµ‹è¯•æ‰¹é‡æ¨ç†: {len(existing_files)} ä¸ªæ–‡ä»¶")
            batch_results = recognizer.recognize_batch(existing_files, use_beam_search=False)
            
            for result in batch_results:
                status = "âœ…" if result['success'] else "âŒ"
                filename = os.path.basename(result['file'])
                print(f"  {status} {filename}: '{result['text']}'")
        
        # æµ‹è¯•æ•°æ®é›†è¯„ä¼°
        if os.path.exists('data/labels.csv'):
            print("ğŸ“Š æµ‹è¯•æ•°æ®é›†è¯„ä¼°...")
            try:
                results, accuracy = recognizer.evaluate_on_dataset('data/audio', 'data/labels.csv')
                print(f"  è¯„ä¼°å®Œæˆ: å‡†ç¡®ç‡ {accuracy:.2%}")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœ
                for i, result in enumerate(results[:3]):
                    status = "âœ…" if result['correct'] else "âŒ"
                    print(f"  {status} {result['filename']}: çœŸå®='{result['true_label']}', é¢„æµ‹='{result['predicted']}'")
                
            except Exception as e:
                print(f"  âš ï¸ è¯„ä¼°æµ‹è¯•è·³è¿‡: {e}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(dummy_model_path):
            os.remove(dummy_model_path)
            print("ğŸ§¹ æ¸…ç†è™šæ‹Ÿæ¨¡å‹æ–‡ä»¶")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_inference_components_separately():
    """åˆ†åˆ«æµ‹è¯•æ¨ç†ç»„ä»¶"""
    print("\nğŸ”§ åˆ†åˆ«æµ‹è¯•æ¨ç†ç»„ä»¶...")
    
    try:
        from inference import SpeechRecognizer
        from model import create_model
        from vocab import vocab
        
        # åˆ›å»ºè™šæ‹Ÿè¯†åˆ«å™¨å®ä¾‹ (ä¸åŠ è½½æ¨¡å‹)
        class DummyRecognizer:
            def __init__(self):
                self.device = torch.device('cpu')
                self.sample_rate = 48000
                self.n_fft = 1024
                self.hop_length = 512
                self.max_length = 200
        
        recognizer = DummyRecognizer()
        
        # æµ‹è¯•éŸ³é¢‘é¢„å¤„ç†æ–¹æ³•
        test_audio = "data/audio/Chinese_Number_01.wav"
        if os.path.exists(test_audio):
            # ä½¿ç”¨SpeechRecognizerçš„ç§æœ‰æ–¹æ³•è¿›è¡Œæµ‹è¯•
            real_recognizer = SpeechRecognizer.__new__(SpeechRecognizer)
            real_recognizer.device = torch.device('cpu')
            real_recognizer.sample_rate = 48000
            real_recognizer.n_fft = 1024
            real_recognizer.hop_length = 512
            real_recognizer.max_length = 200
            
            spectrogram = real_recognizer._extract_spectrogram(test_audio)
            print(f"âœ… éŸ³é¢‘é¢„å¤„ç†: {test_audio} â†’ {spectrogram.shape}")
            
            # æµ‹è¯•è§£ç æ–¹æ³•éœ€è¦æ¨¡å‹ï¼Œè·³è¿‡
            print("âš ï¸ è§£ç æ–¹æ³•æµ‹è¯•éœ€è¦çœŸå®æ¨¡å‹ï¼Œè·³è¿‡")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç»„ä»¶æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_inference_usage_guide():
    """åˆ›å»ºæ¨ç†ä½¿ç”¨æŒ‡å—"""
    guide = """
# ğŸ¯ æ¨ç†ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## åŸºæœ¬ç”¨æ³•

### 1. å•æ–‡ä»¶æ¨ç†
```bash
python inference.py --model checkpoints/best_model.pth --audio data/audio/test.wav
```

### 2. æ‰¹é‡æ¨ç†
```bash
python inference.py --model checkpoints/best_model.pth --audio_dir data/audio
```

### 3. æ•°æ®é›†è¯„ä¼°
```bash
python inference.py \\
    --model checkpoints/best_model.pth \\
    --audio_dir data/audio \\
    --labels data/labels.csv \\
    --output results.csv
```

## Python API ç”¨æ³•

```python
from inference import SpeechRecognizer

# åˆ›å»ºè¯†åˆ«å™¨
recognizer = SpeechRecognizer('checkpoints/best_model.pth')

# å•æ–‡ä»¶è¯†åˆ«
result = recognizer.recognize_file('test.wav')
print(f"è¯†åˆ«ç»“æœ: {result['text']}")

# æ‰¹é‡è¯†åˆ«
results = recognizer.recognize_batch(['file1.wav', 'file2.wav'])
for result in results:
    print(f"{result['file']}: {result['text']}")

# æ•°æ®é›†è¯„ä¼°
results, accuracy = recognizer.evaluate_on_dataset('data/audio', 'data/labels.csv')
print(f"å‡†ç¡®ç‡: {accuracy:.2%}")
```

## é«˜çº§é€‰é¡¹

- `--beam_size 5`: æŸæœç´¢å¤§å°
- `--no_beam_search`: ä½¿ç”¨è´ªå©ªè§£ç 
- `--device cuda`: ä½¿ç”¨GPUåŠ é€Ÿ
- `--output results.csv`: ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
"""
    
    with open('INFERENCE_GUIDE.md', 'w', encoding='utf-8') as f:
        f.write(guide)
    
    print("ğŸ“š æ¨ç†ä½¿ç”¨æŒ‡å—å·²åˆ›å»º: INFERENCE_GUIDE.md")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ æ¨ç†é€»è¾‘å®Œæ•´æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("æ¨ç†ç»„ä»¶æµ‹è¯•", test_inference_components_separately),
        ("è™šæ‹Ÿæ¨¡å‹æ¨ç†æµ‹è¯•", test_inference_with_dummy_model),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"âŒ {test_name} æ‰§è¡Œå¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # åˆ›å»ºä½¿ç”¨æŒ‡å—
    create_inference_usage_guide()
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    
    passed = 0
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
        if success:
            passed += 1
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{len(results)} æµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("ğŸ‰ æ¨ç†é€»è¾‘å®Œå…¨æ­£å¸¸ï¼")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("1. ç­‰å¾…æ¨¡å‹è®­ç»ƒå®Œæˆ")
        print("2. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå®é™…æ¨ç†:")
        print("   python inference.py --model checkpoints/best_model.pth --audio data/audio/test.wav")
        print("3. æŸ¥çœ‹ INFERENCE_GUIDE.md äº†è§£è¯¦ç»†ç”¨æ³•")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œä½†æ ¸å¿ƒé€»è¾‘åº”è¯¥æ­£å¸¸")

if __name__ == "__main__":
    main()