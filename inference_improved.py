#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›ç‰ˆæ¨ç†è„šæœ¬
æ·»åŠ æ™ºèƒ½å›é€€æœºåˆ¶ï¼Œè§£å†³æŸæœç´¢ç©ºç»“æœé—®é¢˜
"""

import os
import torch
import numpy as np
import librosa
import argparse
from tqdm import tqdm

from model import create_model
from vocab import vocab
import warnings

warnings.filterwarnings('ignore')


class ImprovedSpeechRecognizer:
    """æ”¹è¿›ç‰ˆè¯­éŸ³è¯†åˆ«å™¨ - å¸¦æ™ºèƒ½å›é€€æœºåˆ¶"""
    
    def __init__(self, model_path, device='cpu'):
        self.device = torch.device(device)
        self.model = self._load_model(model_path)
        self.model.eval()
        
        # éŸ³é¢‘å¤„ç†å‚æ•°
        self.sample_rate = 48000
        self.n_fft = 1024
        self.hop_length = 512
        self.max_length = 200
    
    def _load_model(self, model_path):
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        config = checkpoint.get('config', {})
        
        model = create_model(
            vocab_size=vocab.vocab_size,
            hidden_dim=config.get('hidden_dim', 256),
            encoder_layers=config.get('encoder_layers', 4),
            decoder_layers=config.get('decoder_layers', 4),
            dropout=config.get('dropout', 0.1)
        )
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        
        print(f"æ¨¡å‹å·²åŠ è½½: {model_path}")
        print(f"è®­ç»ƒepoch: {checkpoint.get('epoch', 'Unknown')}")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {checkpoint.get('best_val_loss', 'Unknown')}")
        
        return model
    
    def _extract_spectrogram(self, audio_path):
        """ä»éŸ³é¢‘æ–‡ä»¶æå–é¢‘è°±ç‰¹å¾"""
        audio, sr = librosa.load(audio_path, sr=self.sample_rate)
        stft = librosa.stft(audio, n_fft=self.n_fft, hop_length=self.hop_length)
        magnitude = np.abs(stft)
        log_magnitude = np.log1p(magnitude)
        spectrogram = log_magnitude.T
        
        if len(spectrogram) > self.max_length:
            spectrogram = spectrogram[:self.max_length]
        else:
            pad_length = self.max_length - len(spectrogram)
            spectrogram = np.pad(spectrogram, ((0, pad_length), (0, 0)), mode='constant')
        
        return torch.FloatTensor(spectrogram).unsqueeze(0)
    
    def _greedy_decode(self, encoder_output, max_length=10):
        """è´ªå©ªè§£ç """
        decoded_seq = torch.LongTensor([[vocab.get_sos_idx()]]).to(self.device)
        
        for step in range(max_length):
            with torch.no_grad():
                output = self.model.decode_step(decoded_seq, encoder_output)
                next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)
                decoded_seq = torch.cat([decoded_seq, next_token], dim=1)
                
                if next_token.item() == vocab.get_eos_idx():
                    break
        
        return decoded_seq.squeeze(0)
    
    def _beam_search(self, encoder_output, beam_size=3, max_length=10):
        """æŸæœç´¢è§£ç """
        beams = [(torch.LongTensor([[vocab.get_sos_idx()]]).to(self.device), 0.0)]
        
        for step in range(max_length):
            new_beams = []
            
            for seq, score in beams:
                if seq[0, -1].item() == vocab.get_eos_idx():
                    new_beams.append((seq, score))
                    continue
                
                with torch.no_grad():
                    output = self.model.decode_step(seq, encoder_output)
                    probs = torch.softmax(output[:, -1, :], dim=-1)
                
                top_probs, top_indices = torch.topk(probs, beam_size)
                
                for i in range(beam_size):
                    new_seq = torch.cat([seq, top_indices[:, i:i + 1]], dim=1)
                    new_score = score + torch.log(top_probs[:, i]).item()
                    new_beams.append((new_seq, new_score))
            
            new_beams.sort(key=lambda x: x[1], reverse=True)
            beams = new_beams[:beam_size]
            
            if all(seq[0, -1].item() == vocab.get_eos_idx() for seq, _ in beams):
                break
        
        best_seq, best_score = beams[0]
        return best_seq.squeeze(0), best_score
    
    def recognize_file(self, audio_path, use_beam_search=True, beam_size=3):
        """æ™ºèƒ½è¯†åˆ«æ–‡ä»¶ - å¸¦å›é€€æœºåˆ¶"""
        try:
            spectrogram = self._extract_spectrogram(audio_path).to(self.device)
            
            with torch.no_grad():
                encoder_output = self.model.encode(spectrogram)
                
                # å°è¯•æŸæœç´¢
                if use_beam_search:
                    beam_seq, beam_score = self._beam_search(encoder_output, beam_size)
                    beam_text = vocab.decode(beam_seq.tolist())
                    
                    # å¦‚æœæŸæœç´¢ç»“æœä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œå›é€€åˆ°è´ªå©ªè§£ç 
                    if not beam_text or len(beam_text.strip()) == 0:
                        print(f"âš ï¸  æŸæœç´¢ç»“æœä¸ºç©ºï¼Œå›é€€åˆ°è´ªå©ªè§£ç ")
                        greedy_seq = self._greedy_decode(encoder_output)
                        greedy_text = vocab.decode(greedy_seq.tolist())
                        
                        return {
                            'text': greedy_text,
                            'method': 'greedy_fallback',
                            'beam_score': beam_score,
                            'success': True,
                            'note': 'æŸæœç´¢ä¸ºç©ºï¼Œä½¿ç”¨è´ªå©ªè§£ç '
                        }
                    else:
                        return {
                            'text': beam_text,
                            'method': 'beam_search',
                            'score': beam_score,
                            'success': True
                        }
                else:
                    # ç›´æ¥ä½¿ç”¨è´ªå©ªè§£ç 
                    greedy_seq = self._greedy_decode(encoder_output)
                    greedy_text = vocab.decode(greedy_seq.tolist())
                    
                    return {
                        'text': greedy_text,
                        'method': 'greedy',
                        'success': True
                    }
        
        except Exception as e:
            return {
                'text': '',
                'success': False,
                'error': str(e)
            }
    
    def recognize_batch(self, audio_paths, use_beam_search=True, beam_size=3):
        """æ‰¹é‡è¯†åˆ«"""
        results = []
        
        for audio_path in tqdm(audio_paths, desc="è¯†åˆ«ä¸­"):
            result = self.recognize_file(audio_path, use_beam_search, beam_size)
            result['file'] = audio_path
            results.append(result)
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ”¹è¿›ç‰ˆè¯­éŸ³è¯†åˆ«æ¨ç†')
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--audio', type=str, help='å•ä¸ªéŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--audio_dir', type=str, help='éŸ³é¢‘æ–‡ä»¶ç›®å½•')
    parser.add_argument('--beam_size', type=int, default=3, help='æŸæœç´¢å¤§å°')
    parser.add_argument('--no_beam_search', action='store_true', help='ä½¿ç”¨è´ªå©ªè§£ç ')
    parser.add_argument('--device', type=str, default='cpu', help='è®¡ç®—è®¾å¤‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯†åˆ«å™¨
    try:
        recognizer = ImprovedSpeechRecognizer(args.model, args.device)
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return
    
    use_beam_search = not args.no_beam_search
    
    if args.audio:
        # è¯†åˆ«å•ä¸ªæ–‡ä»¶
        print(f"è¯†åˆ«æ–‡ä»¶: {args.audio}")
        result = recognizer.recognize_file(args.audio, use_beam_search, args.beam_size)
        
        if result['success']:
            print(f"è¯†åˆ«ç»“æœ: {result['text']}")
            print(f"ä½¿ç”¨æ–¹æ³•: {result['method']}")
            if 'score' in result:
                print(f"å¾—åˆ†: {result['score']:.4f}")
            if 'note' in result:
                print(f"æ³¨æ„: {result['note']}")
        else:
            print(f"è¯†åˆ«å¤±è´¥: {result['error']}")
    
    elif args.audio_dir:
        # æ‰¹é‡è¯†åˆ«ç›®å½•ä¸­çš„æ–‡ä»¶
        print(f"æ‰¹é‡è¯†åˆ«ç›®å½•: {args.audio_dir}")
        
        audio_files = []
        for ext in ['.wav', '.mp3', '.flac', '.m4a']:
            audio_files.extend([
                os.path.join(args.audio_dir, f)
                for f in os.listdir(args.audio_dir)
                if f.lower().endswith(ext)
            ])
        
        if not audio_files:
            print("æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return
        
        results = recognizer.recognize_batch(audio_files, use_beam_search, args.beam_size)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r['success'])
        beam_count = sum(1 for r in results if r.get('method') == 'beam_search')
        greedy_count = sum(1 for r in results if r.get('method') in ['greedy', 'greedy_fallback'])
        
        print(f"\nğŸ“Š è¯†åˆ«ç»Ÿè®¡:")
        print(f"  æ€»æ–‡ä»¶æ•°: {len(results)}")
        print(f"  æˆåŠŸè¯†åˆ«: {success_count}")
        print(f"  æŸæœç´¢: {beam_count}")
        print(f"  è´ªå©ªè§£ç : {greedy_count}")
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ“‹ è¯†åˆ«ç»“æœ:")
        for result in results:
            filename = os.path.basename(result['file'])
            status = "âœ…" if result['success'] else "âŒ"
            method = result.get('method', 'unknown')
            print(f"  {status} {filename}: '{result['text']}' ({method})")
    
    else:
        print("è¯·æŒ‡å®šè¦è¯†åˆ«çš„éŸ³é¢‘æ–‡ä»¶æˆ–ç›®å½•")
        parser.print_help()


if __name__ == "__main__":
    main()