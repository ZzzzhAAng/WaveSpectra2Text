# -*- coding: utf-8 -*-
"""
å¤§æ•°æ®é›†è®­ç»ƒè„šæœ¬ - é€‚ç”¨äº10000+æ ·æœ¬
åŸºäºåŸå§‹train.pyï¼Œé’ˆå¯¹å¤§æ•°æ®é›†ä¼˜åŒ–
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import random_split
import numpy as np
from tqdm import tqdm
import argparse
import json
from datetime import datetime

from model import create_model
from data_utils import get_dataloader, check_audio_files
from vocab import vocab


class LargeDatasetTrainer:
    def __init__(self, model, train_loader, val_loader, device, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.config = config

        # ä¼˜åŒ–å™¨ - å¤§æ•°æ®é›†å¯ä»¥ç”¨æ›´é«˜å­¦ä¹ ç‡
        self.optimizer = optim.AdamW(  # AdamWå¯¹å¤§æ•°æ®é›†æ›´å¥½
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay'],
            betas=(0.9, 0.98)  # æ›´é€‚åˆTransformer
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½™å¼¦é€€ç«
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=20,  # 20ä¸ªepochåé‡å¯
            T_mult=2,  # æ¯æ¬¡é‡å¯å‘¨æœŸç¿»å€
            eta_min=1e-6
        )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(
            ignore_index=vocab.get_padding_idx(),
            label_smoothing=0.1
        )

        # æ—¥å¿—
        self.writer = SummaryWriter(f"runs/{config['experiment_name']}")

        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.best_val_loss = float('inf')
        self.train_losses = []
        self.val_losses = []
        self.patience_counter = 0
        self.max_patience = 10

    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = len(self.train_loader)

        progress_bar = tqdm(self.train_loader, desc=f'Epoch {self.epoch + 1}')

        for batch_idx, batch in enumerate(progress_bar):
            # å…¼å®¹æ–°æ—§æ¥å£

            if 'features' in batch:

                spectrograms = batch['features'].to(self.device)

            else:

                spectrograms = (batch['features'] if 'features' in batch else batch['spectrograms']).to(self.device)
            labels = batch['labels'].to(self.device)

            tgt_input = labels[:, :-1]
            tgt_output = labels[:, 1:]

            self.optimizer.zero_grad()

            outputs = self.model(spectrograms, tgt_input)

            loss = self.criterion(
                outputs.reshape(-1, outputs.size(-1)),
                tgt_output.reshape(-1)
            )

            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['grad_clip'])

            self.optimizer.step()

            total_loss += loss.item()

            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{total_loss / (batch_idx + 1):.4f}'
            })

            # è®°å½•åˆ°tensorboard
            global_step = self.epoch * num_batches + batch_idx
            self.writer.add_scalar('Train/Loss', loss.item(), global_step)

        avg_loss = total_loss / num_batches
        self.train_losses.append(avg_loss)

        return avg_loss

    def validate(self):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        total_correct = 0
        total_tokens = 0

        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                # å…¼å®¹æ–°æ—§æ¥å£

                if 'features' in batch:

                    spectrograms = batch['features'].to(self.device)

                else:

                    spectrograms = (batch['features'] if 'features' in batch else batch['spectrograms']).to(self.device)
                labels = batch['labels'].to(self.device)

                tgt_input = labels[:, :-1]
                tgt_output = labels[:, 1:]

                outputs = self.model(spectrograms, tgt_input)

                loss = self.criterion(
                    outputs.reshape(-1, outputs.size(-1)),
                    tgt_output.reshape(-1)
                )

                total_loss += loss.item()

                # è®¡ç®—å‡†ç¡®ç‡
                predictions = outputs.argmax(dim=-1)
                mask = (tgt_output != vocab.get_padding_idx())
                correct = (predictions == tgt_output) & mask
                total_correct += correct.sum().item()
                total_tokens += mask.sum().item()

        avg_loss = total_loss / len(self.val_loader)
        accuracy = total_correct / total_tokens if total_tokens > 0 else 0

        self.val_losses.append(avg_loss)

        self.writer.add_scalar('Val/Loss', avg_loss, self.epoch)
        self.writer.add_scalar('Val/Accuracy', accuracy, self.epoch)

        return avg_loss, accuracy

    def save_checkpoint(self, filename):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'config': self.config
        }

        torch.save(checkpoint, filename)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")

    def train(self, num_epochs):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters())}")
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(self.train_loader.dataset)}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(self.val_loader.dataset)}")

        for epoch in range(num_epochs):
            self.epoch = epoch

            # è®­ç»ƒ
            train_loss = self.train_epoch()

            # éªŒè¯
            val_loss, val_acc = self.validate()

            # å­¦ä¹ ç‡è°ƒåº¦
            old_lr = self.optimizer.param_groups[0]['lr']
            self.scheduler.step()
            new_lr = self.optimizer.param_groups[0]['lr']

            # æ‰“å°ç»“æœ
            print(f"Epoch {epoch + 1}/{num_epochs}:")
            print(f"  Train Loss: {train_loss:.4f}")
            print(f"  Val Loss: {val_loss:.4f}")
            print(f"  Val Acc: {val_acc:.4f}")
            print(f"  LR: {new_lr:.6f}")

            if abs(new_lr - old_lr) > 1e-7:
                print(f"  -> å­¦ä¹ ç‡å˜åŒ–: {old_lr:.6f} -> {new_lr:.6f}")

            # æ—©åœæ£€æŸ¥
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                self.save_checkpoint(f"../checkpoints/best_model.pth")
                print("  -> æ–°çš„æœ€ä½³æ¨¡å‹!")
            else:
                self.patience_counter += 1
                if self.patience_counter >= self.max_patience:
                    print(f"  -> æ—©åœ: éªŒè¯æŸå¤±è¿ç»­{self.max_patience}ä¸ªepochæœªæ”¹å–„")
                    break

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config['save_every'] == 0:
                self.save_checkpoint(f"checkpoints/checkpoint_epoch_{epoch + 1}.pth")

            print("-" * 50)

        print("è®­ç»ƒå®Œæˆ!")
        self.writer.close()


def split_large_dataset(dataset, train_ratio=0.8, val_ratio=0.1):
    """åˆ†å‰²å¤§æ•°æ®é›†"""
    total_size = len(dataset)
    train_size = int(train_ratio * total_size)
    val_size = int(val_ratio * total_size)
    test_size = total_size - train_size - val_size

    train_dataset, val_dataset, test_dataset = random_split(
        dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )

    print(f"æ•°æ®é›†åˆ†å‰²:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬ ({train_ratio * 100:.1f}%)")
    print(f"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬ ({val_ratio * 100:.1f}%)")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬ ({(1 - train_ratio - val_ratio) * 100:.1f}%)")

    return train_dataset, val_dataset, test_dataset


def main():
    """ä¸»å‡½æ•° - å¤§æ•°æ®é›†é…ç½®"""

    # å¤§æ•°æ®é›†ä¼˜åŒ–é…ç½®
    config = {
        'experiment_name': f'large_dataset_speech_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
        'batch_size': 32,  # å¤§æ‰¹å¤§å°
        'learning_rate': 3e-4,  # è¾ƒé«˜å­¦ä¹ ç‡
        'weight_decay': 1e-4,  # é€‚ä¸­æ­£åˆ™åŒ–
        'grad_clip': 1.0,  # æ ‡å‡†æ¢¯åº¦è£å‰ª
        'num_epochs': 200,  # æ›´å¤šè®­ç»ƒè½®æ•°
        'save_every': 20,
        'hidden_dim': 512,  # å¤§æ¨¡å‹
        'encoder_layers': 6,  # æ›´å¤šå±‚
        'decoder_layers': 6,
        'dropout': 0.1,  # è¾ƒå°‘dropout
        'audio_dir': 'data/audio',
        'labels_file': 'data/labels.csv'
    }

    print("ğŸš€ å¤§æ•°æ®é›†è®­ç»ƒè„šæœ¬ (é€‚ç”¨äº10000+æ ·æœ¬)")
    print("é…ç½®ç‰¹ç‚¹:")
    print("  âœ… å¤§æ¨¡å‹ (hidden_dim=512, 6å±‚)")
    print("  âœ… é«˜å­¦ä¹ ç‡ (3e-4)")
    print("  âœ… å¤§æ‰¹å¤§å° (32)")
    print("  âœ… ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦")
    print("  âœ… AdamWä¼˜åŒ–å™¨")
    print("  âœ… æ•°æ®é›†è‡ªåŠ¨åˆ†å‰² (80%/10%/10%)")
    print("=" * 60)

    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶
    if not check_audio_files(config['audio_dir'], config['labels_file']):
        print("é”™è¯¯: éŸ³é¢‘æ–‡ä»¶æ£€æŸ¥å¤±è´¥")
        return

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    os.makedirs('../checkpoints', exist_ok=True)
    os.makedirs('../runs', exist_ok=True)

    try:
        # åˆ›å»ºå®Œæ•´æ•°æ®é›†
        from data_utils import AudioSpectrogramDataset
        from torch.utils.data import DataLoader

        full_dataset = AudioSpectrogramDataset(
            config['audio_dir'],
            config['labels_file']
        )

        # åˆ†å‰²æ•°æ®é›†
        train_dataset, val_dataset, _ = split_large_dataset(full_dataset)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=config['batch_size'],
            shuffle=True,
            num_workers=4,  # å¤šè¿›ç¨‹åŠ è½½
            pin_memory=True if device.type == 'cuda' else False
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=config['batch_size'],
            shuffle=False,
            num_workers=4,
            pin_memory=True if device.type == 'cuda' else False
        )

    except Exception as e:
        print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # åˆ›å»ºå¤§æ¨¡å‹
    model = create_model(
        vocab_size=vocab.vocab_size,
        hidden_dim=config['hidden_dim'],
        encoder_layers=config['encoder_layers'],
        decoder_layers=config['decoder_layers'],
        dropout=config['dropout']
    ).to(device)

    print(f"æ¨¡å‹å¤§å°: {sum(p.numel() for p in model.parameters())} å‚æ•°")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LargeDatasetTrainer(model, train_loader, val_loader, device, config)

    # å¼€å§‹è®­ç»ƒ
    try:
        trainer.train(config['num_epochs'])
    except KeyboardInterrupt:
        print("è®­ç»ƒè¢«ä¸­æ–­")
        trainer.save_checkpoint("checkpoints/interrupted.pth")


if __name__ == "__main__":
    main()