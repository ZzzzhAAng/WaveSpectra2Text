"""
åŒè¾“å…¥æ¨¡å¼æ¨ç†ç³»ç»Ÿ
æ”¯æŒä¸¤ç§è¾“å…¥æ–¹å¼ï¼š
1. åŸå§‹éŸ³é¢‘æ–‡ä»¶ (ç³»ç»Ÿå†…é¢„å¤„ç†)
2. å¯¹åº”åŸå§‹éŸ³é¢‘çš„é¢‘è°±ç‰¹å¾ (è·³è¿‡é¢„å¤„ç†)
"""

import os
import torch
import numpy as np
import librosa
import argparse
from pathlib import Path
import time
import json

from model import create_model
from vocab import vocab
from audio_preprocessing import SpectrogramPreprocessor
import warnings

warnings.filterwarnings('ignore')


class DualInputSpeechRecognizer:
    """åŒè¾“å…¥æ¨¡å¼è¯­éŸ³è¯†åˆ«å™¨"""

    def __init__(self, model_path, device='cpu'):
        """
        åˆå§‹åŒ–åŒè¾“å…¥è¯†åˆ«å™¨

        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = torch.device(device)
        print(f"ğŸš€ åˆå§‹åŒ–åŒè¾“å…¥è¯­éŸ³è¯†åˆ«å™¨")
        print(f"è®¾å¤‡: {self.device}")

        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        self.model = self._load_trained_model(model_path)

        # åˆå§‹åŒ–éŸ³é¢‘é¢„å¤„ç†å™¨ (ä»…ç”¨äºéŸ³é¢‘è¾“å…¥æ¨¡å¼)
        self.preprocessor = SpectrogramPreprocessor(
            sample_rate=48000,
            n_fft=1024,
            hop_length=512,
            max_length=200
        )

        print(f"âœ… æ”¯æŒä¸¤ç§è¾“å…¥æ¨¡å¼:")
        print(f"  1. åŸå§‹éŸ³é¢‘æ–‡ä»¶ (.wav, .mp3, .flacç­‰)")
        print(f"  2. é¢„å¤„ç†é¢‘è°±ç‰¹å¾ (.npyæ–‡ä»¶)")

    def _load_trained_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

        checkpoint = torch.load(model_path, map_location=self.device)
        config = checkpoint.get('config', {})

        model = create_model(
            vocab_size=vocab.vocab_size,
            hidden_dim=config.get('hidden_dim', 256),
            encoder_layers=config.get('encoder_layers', 4),
            decoder_layers=config.get('decoder_layers', 4),
            dropout=config.get('dropout', 0.1)
        )

        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()

        print(f"ğŸ“‚ æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters())}")

        return model

    def recognize_from_audio(self, audio_path, show_details=True):
        """
        æ¨¡å¼1: ä»åŸå§‹éŸ³é¢‘æ–‡ä»¶è¯†åˆ«
        å®Œæ•´æµç¨‹: éŸ³é¢‘ â†’ é¢‘è°±æå– â†’ æ¨¡å‹æ¨ç† â†’ æ–‡æœ¬
        """
        if show_details:
            print(f"\nğŸµ æ¨¡å¼1: åŸå§‹éŸ³é¢‘è¾“å…¥")
            print(f"æ–‡ä»¶: {audio_path}")
            print("-" * 50)

        try:
            start_time = time.time()

            # éªŒè¯éŸ³é¢‘æ–‡ä»¶
            if not os.path.exists(audio_path):
                raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")

            # æ­¥éª¤1: éŸ³é¢‘é¢„å¤„ç† (æå–é¢‘è°±ç‰¹å¾)
            if show_details:
                print("ğŸ”§ æ­¥éª¤1: éŸ³é¢‘é¢„å¤„ç†")

            preprocess_start = time.time()
            spectrogram_features = self.preprocessor.process(audio_path)
            preprocess_time = time.time() - preprocess_start

            if show_details:
                print(f"  âœ… é¢‘è°±æå–: {spectrogram_features.shape}")
                print(f"  â±ï¸  é¢„å¤„ç†è€—æ—¶: {preprocess_time:.3f}ç§’")

            # æ­¥éª¤2: æ¨¡å‹æ¨ç†
            result = self._infer_from_spectrogram(spectrogram_features, show_details)

            # æ·»åŠ é¢„å¤„ç†æ—¶é—´
            result['processing_time']['preprocessing'] = preprocess_time
            result['processing_time']['total'] = time.time() - start_time
            result['input_type'] = 'audio_file'

            return result

        except Exception as e:
            return {
                'text': '',
                'success': False,
                'error': str(e),
                'input_type': 'audio_file'
            }

    def recognize_from_spectrogram(self, spectrogram_path, show_details=True):
        """
        æ¨¡å¼2: ä»é¢„å¤„ç†é¢‘è°±ç‰¹å¾è¯†åˆ«
        å¿«é€Ÿæµç¨‹: é¢‘è°±ç‰¹å¾ â†’ æ¨¡å‹æ¨ç† â†’ æ–‡æœ¬
        """
        if show_details:
            print(f"\nğŸ“Š æ¨¡å¼2: é¢‘è°±ç‰¹å¾è¾“å…¥")
            print(f"æ–‡ä»¶: {spectrogram_path}")
            print("-" * 50)

        try:
            start_time = time.time()

            # éªŒè¯é¢‘è°±æ–‡ä»¶
            if not os.path.exists(spectrogram_path):
                raise FileNotFoundError(f"é¢‘è°±æ–‡ä»¶ä¸å­˜åœ¨: {spectrogram_path}")

            # æ­¥éª¤1: åŠ è½½é¢„å¤„ç†å¥½çš„é¢‘è°±ç‰¹å¾
            if show_details:
                print("ğŸ“‚ æ­¥éª¤1: åŠ è½½é¢‘è°±ç‰¹å¾")

            load_start = time.time()
            spectrogram_features = np.load(spectrogram_path)
            load_time = time.time() - load_start

            # éªŒè¯é¢‘è°±ç‰¹å¾æ ¼å¼
            expected_shape = (200, 513)  # æˆ–å…¶ä»–é¢„æœŸå½¢çŠ¶
            if spectrogram_features.shape != expected_shape:
                print(f"âš ï¸  é¢‘è°±å½¢çŠ¶ {spectrogram_features.shape} ä¸é¢„æœŸ {expected_shape} ä¸åŒ¹é…")
                # å¯ä»¥å°è¯•è°ƒæ•´å½¢çŠ¶æˆ–ç»™å‡ºè­¦å‘Š

            if show_details:
                print(f"  âœ… é¢‘è°±åŠ è½½: {spectrogram_features.shape}")
                print(f"  ğŸ“Š æ•°å€¼èŒƒå›´: [{spectrogram_features.min():.3f}, {spectrogram_features.max():.3f}]")
                print(f"  â±ï¸  åŠ è½½è€—æ—¶: {load_time:.3f}ç§’")
                print("  ğŸš€ è·³è¿‡é¢„å¤„ç†ï¼Œç›´æ¥è¿›å…¥æ¨ç†")

            # æ­¥éª¤2: æ¨¡å‹æ¨ç† (è·³è¿‡é¢„å¤„ç†)
            result = self._infer_from_spectrogram(spectrogram_features, show_details)

            # æ·»åŠ åŠ è½½æ—¶é—´
            result['processing_time']['preprocessing'] = 0.0  # è·³è¿‡é¢„å¤„ç†
            result['processing_time']['loading'] = load_time
            result['processing_time']['total'] = time.time() - start_time
            result['input_type'] = 'spectrogram_file'

            return result

        except Exception as e:
            return {
                'text': '',
                'success': False,
                'error': str(e),
                'input_type': 'spectrogram_file'
            }

    def recognize_from_spectrogram_array(self, spectrogram_array, show_details=True):
        """
        æ¨¡å¼3: ä»å†…å­˜ä¸­çš„é¢‘è°±æ•°ç»„è¯†åˆ«
        ç›´æ¥è¾“å…¥: numpyæ•°ç»„ â†’ æ¨¡å‹æ¨ç† â†’ æ–‡æœ¬
        """
        if show_details:
            print(f"\nğŸ§® æ¨¡å¼3: å†…å­˜é¢‘è°±æ•°ç»„è¾“å…¥")
            print(f"æ•°ç»„å½¢çŠ¶: {spectrogram_array.shape}")
            print("-" * 50)

        try:
            start_time = time.time()

            # éªŒè¯è¾“å…¥æ•°ç»„
            if not isinstance(spectrogram_array, np.ndarray):
                raise ValueError("è¾“å…¥å¿…é¡»æ˜¯numpyæ•°ç»„")

            expected_shape = (200, 513)
            if spectrogram_array.shape != expected_shape:
                print(f"âš ï¸  è¾“å…¥å½¢çŠ¶ {spectrogram_array.shape} ä¸é¢„æœŸ {expected_shape} ä¸åŒ¹é…")

            if show_details:
                print("ğŸ§® æ­¥éª¤1: éªŒè¯é¢‘è°±æ•°ç»„")
                print(f"  âœ… æ•°ç»„å½¢çŠ¶: {spectrogram_array.shape}")
                print(f"  ğŸ“Š æ•°å€¼èŒƒå›´: [{spectrogram_array.min():.3f}, {spectrogram_array.max():.3f}]")
                print("  ğŸš€ ç›´æ¥è¿›å…¥æ¨ç† (æ— éœ€åŠ è½½)")

            # ç›´æ¥æ¨ç†
            result = self._infer_from_spectrogram(spectrogram_array, show_details)

            result['processing_time']['preprocessing'] = 0.0
            result['processing_time']['loading'] = 0.0
            result['processing_time']['total'] = time.time() - start_time
            result['input_type'] = 'spectrogram_array'

            return result

        except Exception as e:
            return {
                'text': '',
                'success': False,
                'error': str(e),
                'input_type': 'spectrogram_array'
            }

    def _infer_from_spectrogram(self, spectrogram_features, show_details=True):
        """ä»é¢‘è°±ç‰¹å¾è¿›è¡Œæ¨ç† (æ ¸å¿ƒæ¨ç†é€»è¾‘)"""
        if show_details:
            print("ğŸ§  æ­¥éª¤2: æ¨¡å‹æ¨ç†")

        inference_start = time.time()

        # è½¬æ¢ä¸ºtensor
        spectrogram_tensor = torch.FloatTensor(spectrogram_features).unsqueeze(0).to(self.device)

        with torch.no_grad():
            # ç¼–ç é˜¶æ®µ
            encoder_output = self.model.encode(spectrogram_tensor)

            if show_details:
                print(f"  ğŸ” ç¼–ç å™¨è¾“å‡º: {encoder_output.shape}")

            # è§£ç é˜¶æ®µ - è´ªå©ªè§£ç 
            greedy_seq = self._greedy_decode(encoder_output)
            greedy_text = vocab.decode(greedy_seq.tolist())

            # è§£ç é˜¶æ®µ - æŸæœç´¢
            beam_seq, beam_score = self._beam_search_decode(encoder_output)
            beam_text = vocab.decode(beam_seq.tolist())

            if show_details:
                print(f"  ğŸ”¤ è´ªå©ªè§£ç : '{greedy_text}'")
                print(f"  ğŸ”¤ æŸæœç´¢: '{beam_text}' (å¾—åˆ†: {beam_score:.3f})")

        inference_time = time.time() - inference_start

        # æ™ºèƒ½é€‰æ‹©æœ€ç»ˆç»“æœ
        final_text = beam_text if beam_text and len(beam_text.strip()) > 0 else greedy_text

        if show_details:
            print(f"\nğŸ¯ æœ€ç»ˆç»“æœ: '{final_text}'")
            print(f"â±ï¸  æ¨ç†è€—æ—¶: {inference_time:.3f}ç§’")

        return {
            'text': final_text,
            'greedy_text': greedy_text,
            'beam_text': beam_text,
            'beam_score': beam_score,
            'processing_time': {
                'inference': inference_time
            },
            'spectrogram_shape': spectrogram_features.shape,
            'success': True
        }

    def _greedy_decode(self, encoder_output, max_length=10):
        """è´ªå©ªè§£ç """
        decoded_seq = torch.LongTensor([[vocab.get_sos_idx()]]).to(self.device)

        for step in range(max_length):
            output = self.model.decode_step(decoded_seq, encoder_output)
            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)
            decoded_seq = torch.cat([decoded_seq, next_token], dim=1)

            if next_token.item() == vocab.get_eos_idx():
                break

        return decoded_seq.squeeze(0)

    def _beam_search_decode(self, encoder_output, beam_size=3, max_length=10):
        """æŸæœç´¢è§£ç """
        device = encoder_output.device
        beams = [(torch.LongTensor([[vocab.get_sos_idx()]]).to(device), 0.0)]

        for step in range(max_length):
            new_beams = []

            for seq, score in beams:
                if seq[0, -1].item() == vocab.get_eos_idx():
                    new_beams.append((seq, score))
                    continue

                output = self.model.decode_step(seq, encoder_output)
                logits = output[:, -1, :]

                # å¯¹è¿‡æ—©EOSæ·»åŠ æƒ©ç½š
                if seq.size(1) < 3:
                    logits[0, vocab.get_eos_idx()] -= 1.0

                probs = torch.softmax(logits, dim=-1)
                top_probs, top_indices = torch.topk(probs, beam_size)

                for i in range(beam_size):
                    new_seq = torch.cat([seq, top_indices[:, i:i + 1]], dim=1)
                    new_score = score + torch.log(top_probs[:, i]).item()
                    new_beams.append((new_seq, new_score))

            new_beams.sort(key=lambda x: x[1], reverse=True)
            beams = new_beams[:beam_size]

            if all(seq[0, -1].item() == vocab.get_eos_idx() for seq, _ in beams):
                break

        best_seq, best_score = beams[0]
        return best_seq.squeeze(0), best_score

    def auto_recognize(self, input_path, show_details=True):
        """
        è‡ªåŠ¨è¯†åˆ«è¾“å…¥ç±»å‹å¹¶å¤„ç†
        æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨åˆ¤æ–­æ˜¯éŸ³é¢‘æ–‡ä»¶è¿˜æ˜¯é¢‘è°±æ–‡ä»¶
        """
        if show_details:
            print(f"\nğŸ¤– è‡ªåŠ¨æ¨¡å¼: {input_path}")

        file_ext = Path(input_path).suffix.lower()

        # éŸ³é¢‘æ–‡ä»¶æ‰©å±•å
        audio_extensions = ['.wav', '.mp3', '.flac', '.m4a', '.ogg']
        # é¢‘è°±æ–‡ä»¶æ‰©å±•å
        spectrogram_extensions = ['.npy', '.npz']

        if file_ext in audio_extensions:
            if show_details:
                print("ğŸµ æ£€æµ‹åˆ°éŸ³é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨éŸ³é¢‘è¾“å…¥æ¨¡å¼")
            return self.recognize_from_audio(input_path, show_details)

        elif file_ext in spectrogram_extensions:
            if show_details:
                print("ğŸ“Š æ£€æµ‹åˆ°é¢‘è°±æ–‡ä»¶ï¼Œä½¿ç”¨é¢‘è°±è¾“å…¥æ¨¡å¼")
            return self.recognize_from_spectrogram(input_path, show_details)

        else:
            return {
                'text': '',
                'success': False,
                'error': f'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}',
                'input_type': 'unknown'
            }


def create_external_spectrogram_demo():
    """åˆ›å»ºå¤–éƒ¨é¢‘è°±ç‰¹å¾å¤„ç†æ¼”ç¤º"""
    print("ğŸ”§ å¤–éƒ¨é¢‘è°±ç‰¹å¾å¤„ç†æ¼”ç¤º")
    print("=" * 60)

    demo_code = '''
# åœºæ™¯: åœ¨å…¶ä»–ç³»ç»Ÿä¸­é¢„å¤„ç†éŸ³é¢‘ï¼Œç„¶åä¼ é€’é¢‘è°±ç‰¹å¾ç»™è¯†åˆ«ç³»ç»Ÿ

# === å¤–éƒ¨ç³»ç»Ÿ (ä¾‹å¦‚: å®æ—¶éŸ³é¢‘å¤„ç†ç³»ç»Ÿ) ===
import librosa
import numpy as np

def external_audio_preprocessing(audio_path):
    """å¤–éƒ¨ç³»ç»Ÿçš„éŸ³é¢‘é¢„å¤„ç†"""
    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„å‚æ•°
    audio, sr = librosa.load(audio_path, sr=48000)
    stft = librosa.stft(audio, n_fft=1024, hop_length=512)
    magnitude = np.abs(stft)
    log_magnitude = np.log1p(magnitude)
    spectrogram = log_magnitude.T

    # æ ‡å‡†åŒ–é•¿åº¦
    if len(spectrogram) > 200:
        spectrogram = spectrogram[:200]
    else:
        pad_length = 200 - len(spectrogram)
        spectrogram = np.pad(spectrogram, ((0, pad_length), (0, 0)))

    return spectrogram.astype(np.float32)

# å¤–éƒ¨ç³»ç»Ÿå¤„ç†éŸ³é¢‘
audio_file = "external_audio.wav"
spectrogram = external_audio_preprocessing(audio_file)

# ä¿å­˜é¢‘è°±ç‰¹å¾
np.save("external_spectrogram.npy", spectrogram)

# === è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ ===
from dual_input_inference import DualInputSpeechRecognizer

# åˆå§‹åŒ–è¯†åˆ«å™¨
recognizer = DualInputSpeechRecognizer("checkpoints/best_model.pth")

# ç›´æ¥ä»é¢‘è°±ç‰¹å¾è¯†åˆ« (è·³è¿‡é¢„å¤„ç†ï¼Œé€Ÿåº¦æ›´å¿«)
result = recognizer.recognize_from_spectrogram("external_spectrogram.npy")
print(f"è¯†åˆ«ç»“æœ: {result['text']}")

# æˆ–è€…ä½¿ç”¨å†…å­˜æ•°ç»„
result = recognizer.recognize_from_spectrogram_array(spectrogram)
print(f"è¯†åˆ«ç»“æœ: {result['text']}")
    '''

    print("ğŸ’» ä½¿ç”¨ç¤ºä¾‹ä»£ç :")
    print(demo_code)

    return demo_code


def compare_input_modes():
    """å¯¹æ¯”ä¸¤ç§è¾“å…¥æ¨¡å¼çš„æ€§èƒ½"""
    print("\nğŸ“Š è¾“å…¥æ¨¡å¼æ€§èƒ½å¯¹æ¯”")
    print("=" * 60)

    comparison = {
        "ç‰¹å¾": ["é¢„å¤„ç†æ—¶é—´", "æ¨ç†æ—¶é—´", "æ€»æ—¶é—´", "å†…å­˜å ç”¨", "é€‚ç”¨åœºæ™¯"],
        "éŸ³é¢‘è¾“å…¥": ["2-3ç§’", "0.3-0.5ç§’", "2.5-3.5ç§’", "ä¸­ç­‰", "ä¸€èˆ¬ä½¿ç”¨ã€å¼€å‘æµ‹è¯•"],
        "é¢‘è°±è¾“å…¥": ["0ç§’", "0.3-0.5ç§’", "0.3-0.5ç§’", "ä½", "é«˜æ€§èƒ½ã€æ‰¹é‡å¤„ç†ã€å®æ—¶ç³»ç»Ÿ"]
    }

    print(f"{'ç‰¹å¾':<15} {'éŸ³é¢‘è¾“å…¥':<20} {'é¢‘è°±è¾“å…¥':<20}")
    print("-" * 55)

    for i, feature in enumerate(comparison["ç‰¹å¾"]):
        audio_val = comparison["éŸ³é¢‘è¾“å…¥"][i]
        spec_val = comparison["é¢‘è°±è¾“å…¥"][i]
        print(f"{feature:<15} {audio_val:<20} {spec_val:<20}")

    print(f"\nğŸ’¡ é€‰æ‹©å»ºè®®:")
    print(f"  ğŸµ éŸ³é¢‘è¾“å…¥: é€‚åˆä¸€èˆ¬ä½¿ç”¨ï¼Œå®Œæ•´æµç¨‹")
    print(f"  ğŸ“Š é¢‘è°±è¾“å…¥: é€‚åˆé«˜æ€§èƒ½éœ€æ±‚ï¼Œå·²æœ‰é¢„å¤„ç†ç³»ç»Ÿ")
    print(f"  ğŸ¤– è‡ªåŠ¨æ¨¡å¼: æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨é€‰æ‹©")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŒè¾“å…¥æ¨¡å¼è¯­éŸ³è¯†åˆ«')
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥æ–‡ä»¶ (éŸ³é¢‘æˆ–é¢‘è°±)')
    parser.add_argument('--mode', type=str, default='auto',
                        choices=['auto', 'audio', 'spectrogram'],
                        help='è¾“å…¥æ¨¡å¼')
    parser.add_argument('--device', type=str, default='cpu', help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--demo', action='store_true', help='æ˜¾ç¤ºæ¼”ç¤ºä»£ç ')
    parser.add_argument('--compare', action='store_true', help='æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”')

    args = parser.parse_args()

    if args.demo:
        create_external_spectrogram_demo()
        return

    if args.compare:
        compare_input_modes()
        return

    print("ğŸ¯ åŒè¾“å…¥æ¨¡å¼è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ")
    print("=" * 60)

    try:
        # åˆ›å»ºè¯†åˆ«å™¨
        recognizer = DualInputSpeechRecognizer(args.model, args.device)

        # æ ¹æ®æ¨¡å¼å¤„ç†
        if args.mode == 'auto':
            result = recognizer.auto_recognize(args.input)
        elif args.mode == 'audio':
            result = recognizer.recognize_from_audio(args.input)
        elif args.mode == 'spectrogram':
            result = recognizer.recognize_from_spectrogram(args.input)

        # æ˜¾ç¤ºç»“æœ
        if result['success']:
            print(f"\nğŸ‰ è¯†åˆ«æˆåŠŸ!")
            print(f"è¾“å…¥ç±»å‹: {result['input_type']}")
            print(f"è¯†åˆ«ç»“æœ: '{result['text']}'")
            print(f"æ€»è€—æ—¶: {result['processing_time']['total']:.3f}ç§’")

            if 'preprocessing' in result['processing_time']:
                print(f"  é¢„å¤„ç†: {result['processing_time']['preprocessing']:.3f}ç§’")
            if 'loading' in result['processing_time']:
                print(f"  åŠ è½½: {result['processing_time']['loading']:.3f}ç§’")
            if 'inference' in result['processing_time']:
                print(f"  æ¨ç†: {result['processing_time']['inference']:.3f}ç§’")
        else:
            print(f"âŒ è¯†åˆ«å¤±è´¥: {result['error']}")

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()