#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ¨ç†è„šæœ¬
æ”¯æŒéŸ³é¢‘å’Œé¢‘è°±ä¸¤ç§è¾“å…¥æ¨¡å¼
"""

import sys
import os
import argparse

from wavespectra2text.inference.recognizer import DualInputSpeechRecognizer


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='WaveSpectra2Text æ¨ç†è„šæœ¬')
    parser.add_argument('--model', type=str, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--input', type=str, help='è¾“å…¥æ–‡ä»¶ (éŸ³é¢‘æˆ–é¢‘è°±)')
    parser.add_argument('--mode', type=str, default='auto',
                        choices=['auto', 'audio', 'spectrogram'],
                        help='è¾“å…¥æ¨¡å¼')
    parser.add_argument('--device', type=str, default='cpu', help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--demo', action='store_true', help='æ˜¾ç¤ºæ¼”ç¤ºä»£ç ')
    parser.add_argument('--compare', action='store_true', help='æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”')

    args = parser.parse_args()

    if args.demo:
        # æ˜¾ç¤ºæ¼”ç¤ºä»£ç 
        print("ğŸ”§ å¤–éƒ¨é¢‘è°±ç‰¹å¾å¤„ç†æ¼”ç¤º")
        print("=" * 60)
        print("ğŸ’» ä½¿ç”¨ç¤ºä¾‹ä»£ç :")
        print()
        print("# åœºæ™¯: åœ¨å…¶ä»–ç³»ç»Ÿä¸­é¢„å¤„ç†éŸ³é¢‘ï¼Œç„¶åä¼ é€’é¢‘è°±ç‰¹å¾ç»™è¯†åˆ«ç³»ç»Ÿ")
        print()
        print("# === å¤–éƒ¨ç³»ç»Ÿ (ä¾‹å¦‚: å®æ—¶éŸ³é¢‘å¤„ç†ç³»ç»Ÿ) ===")
        print("import librosa")
        print("import numpy as np")
        print()
        print("def external_audio_preprocessing(audio_path):")
        print("    \"\"\"å¤–éƒ¨ç³»ç»Ÿçš„éŸ³é¢‘é¢„å¤„ç† - ä½¿ç”¨ç»Ÿä¸€å·¥å…·\"\"\"")
        print("    from wavespectra2text.data.utils import AudioProcessor")
        print()
        print("    # ä½¿ç”¨ç»Ÿä¸€çš„éŸ³é¢‘å¤„ç†å™¨")
        print("    processor = AudioProcessor(sample_rate=48000, n_fft=1024, hop_length=512, max_length=200)")
        print("    return processor.extract_spectrogram(audio_path)")
        print()
        print("# å¤–éƒ¨ç³»ç»Ÿå¤„ç†éŸ³é¢‘")
        print("audio_file = \"external_audio.wav\"")
        print("spectrogram = external_audio_preprocessing(audio_file)")
        print()
        print("# ä¿å­˜é¢‘è°±ç‰¹å¾")
        print("np.save(\"external_spectrogram.npy\", spectrogram)")
        print()
        print("# === è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ ===")
        print("from wavespectra2text.inference.recognizer import DualInputSpeechRecognizer")
        print()
        print("# åˆå§‹åŒ–è¯†åˆ«å™¨")
        print("recognizer = DualInputSpeechRecognizer(\"experiments/checkpoints/best_model.pth\")")
        print()
        print("# ç›´æ¥ä»é¢‘è°±ç‰¹å¾è¯†åˆ« (è·³è¿‡é¢„å¤„ç†ï¼Œé€Ÿåº¦æ›´å¿«)")
        print("result = recognizer.recognize_from_spectrogram(\"external_spectrogram.npy\")")
        print("print(f\"è¯†åˆ«ç»“æœ: {result['text']}\")")
        return

    if args.compare:
        # æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”
        print("ğŸ“Š è¾“å…¥æ¨¡å¼æ€§èƒ½å¯¹æ¯”")
        print("=" * 60)
        print(f"{'ç‰¹å¾':<15} {'éŸ³é¢‘è¾“å…¥':<20} {'é¢‘è°±è¾“å…¥':<20}")
        print("-" * 60)
        
        comparison = {
            "ç‰¹å¾": ["é¢„å¤„ç†æ—¶é—´", "æ¨ç†æ—¶é—´", "æ€»æ—¶é—´", "å†…å­˜å ç”¨", "é€‚ç”¨åœºæ™¯"],
            "éŸ³é¢‘è¾“å…¥": ["2-3ç§’", "0.3-0.5ç§’", "2.5-3.5ç§’", "ä¸­ç­‰", "ä¸€èˆ¬ä½¿ç”¨ã€å¼€å‘æµ‹è¯•"],
            "é¢‘è°±è¾“å…¥": ["0ç§’", "0.3-0.5ç§’", "0.3-0.5ç§’", "ä½", "é«˜æ€§èƒ½ã€æ‰¹é‡å¤„ç†ã€å®æ—¶ç³»ç»Ÿ"]
        }
        
        for i, feature in enumerate(comparison["ç‰¹å¾"]):
            audio_val = comparison["éŸ³é¢‘è¾“å…¥"][i]
            spec_val = comparison["é¢‘è°±è¾“å…¥"][i]
            print(f"{feature:<15} {audio_val:<20} {spec_val:<20}")
        
        print(f"\nğŸ’¡ é€‰æ‹©å»ºè®®:")
        print(f"  ğŸµ éŸ³é¢‘è¾“å…¥: é€‚åˆä¸€èˆ¬ä½¿ç”¨ï¼Œå®Œæ•´æµç¨‹")
        print(f"  ğŸ“Š é¢‘è°±è¾“å…¥: é€‚åˆé«˜æ€§èƒ½éœ€æ±‚ï¼Œå·²æœ‰é¢„å¤„ç†ç³»ç»Ÿ")
        print(f"  ğŸ¤– è‡ªåŠ¨æ¨¡å¼: æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨é€‰æ‹©")
        return

    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.demo and not args.compare and not args.input:
        parser.error("--input å‚æ•°æ˜¯å¿…éœ€çš„")

    print("ğŸ¯ WaveSpectra2Text æ¨ç†ç³»ç»Ÿ")
    print("=" * 60)

    try:
        # åˆ›å»ºè¯†åˆ«å™¨
        recognizer = DualInputSpeechRecognizer(args.model, args.device)

        # æ ¹æ®æ¨¡å¼å¤„ç†
        if args.mode == 'auto':
            result = recognizer.auto_recognize(args.input)
        elif args.mode == 'audio':
            result = recognizer.recognize_from_audio(args.input)
        elif args.mode == 'spectrogram':
            result = recognizer.recognize_from_spectrogram(args.input)

        # æ˜¾ç¤ºç»“æœ
        if result['success']:
            print(f"\nğŸ¯ è¯†åˆ«ç»“æœ: '{result['text']}'")
            
            # å…¼å®¹ä¸åŒçš„æ—¶é—´æ ¼å¼
            if 'total_time' in result:
                total_time = result['total_time']
            elif 'processing_time' in result and 'total' in result['processing_time']:
                total_time = result['processing_time']['total']
            else:
                total_time = 0.0
                
            print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.3f}ç§’")
            print(f"ğŸ“Š ä½¿ç”¨æ¨¡å¼: {result.get('mode', 'unknown')}")
        else:
            print(f"\nâŒ è¯†åˆ«å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
        return


if __name__ == "__main__":
    main()
