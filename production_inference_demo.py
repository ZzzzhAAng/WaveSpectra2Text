#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§ç¯å¢ƒæ¨ç†æ¼”ç¤º
å±•ç¤ºæˆç†Ÿæ¨¡å‹å¦‚ä½•å¤„ç†ä»»æ„æ–°éŸ³é¢‘æ–‡ä»¶çš„å®Œæ•´æµç¨‹
"""

import os
import torch
import numpy as np
import librosa
import argparse
from pathlib import Path
import time

from model import create_model
from vocab import vocab
from audio_preprocessing import SpectrogramPreprocessor
import warnings

warnings.filterwarnings('ignore')


class ProductionSpeechRecognizer:
    """ç”Ÿäº§ç¯å¢ƒè¯­éŸ³è¯†åˆ«å™¨ - å¤„ç†ä»»æ„æ–°éŸ³é¢‘"""
    
    def __init__(self, model_path, device='cpu'):
        """
        åˆå§‹åŒ–ç”Ÿäº§ç¯å¢ƒè¯†åˆ«å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = torch.device(device)
        print(f"ğŸš€ åˆå§‹åŒ–ç”Ÿäº§ç¯å¢ƒè¯­éŸ³è¯†åˆ«å™¨")
        print(f"è®¾å¤‡: {self.device}")
        
        # 1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        self.model = self._load_trained_model(model_path)
        
        # 2. åˆå§‹åŒ–éŸ³é¢‘é¢„å¤„ç†å™¨ (ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´)
        self.preprocessor = SpectrogramPreprocessor(
            sample_rate=48000,
            n_fft=1024,
            hop_length=512,
            max_length=200
        )
        
        print(f"âœ… åˆå§‹åŒ–å®Œæˆï¼Œå‡†å¤‡å¤„ç†æ–°éŸ³é¢‘æ–‡ä»¶")
    
    def _load_trained_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location=self.device)
        config = checkpoint.get('config', {})
        
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  è®­ç»ƒepoch: {checkpoint.get('epoch', 'Unknown')}")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {checkpoint.get('best_val_acc', 'Unknown')}")
        print(f"  æ¨¡å‹é…ç½®: {config}")
        
        # åˆ›å»ºæ¨¡å‹æ¶æ„
        model = create_model(
            vocab_size=vocab.vocab_size,
            hidden_dim=config.get('hidden_dim', 256),
            encoder_layers=config.get('encoder_layers', 4),
            decoder_layers=config.get('decoder_layers', 4),
            dropout=config.get('dropout', 0.1)
        )
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        
        return model
    
    def process_new_audio(self, audio_path, show_details=True):
        """
        å¤„ç†å…¨æ–°çš„éŸ³é¢‘æ–‡ä»¶ - å®Œæ•´æµç¨‹æ¼”ç¤º
        
        Args:
            audio_path: æ–°éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†å¤„ç†è¿‡ç¨‹
        
        Returns:
            è¯†åˆ«ç»“æœå­—å…¸
        """
        if show_details:
            print(f"\nğŸµ å¤„ç†æ–°éŸ³é¢‘æ–‡ä»¶: {audio_path}")
            print("=" * 60)
        
        try:
            start_time = time.time()
            
            # æ­¥éª¤1: éªŒè¯éŸ³é¢‘æ–‡ä»¶
            if not os.path.exists(audio_path):
                raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            
            file_size = os.path.getsize(audio_path) / 1024  # KB
            if show_details:
                print(f"ğŸ“ æ–‡ä»¶ä¿¡æ¯:")
                print(f"  è·¯å¾„: {audio_path}")
                print(f"  å¤§å°: {file_size:.1f} KB")
            
            # æ­¥éª¤2: éŸ³é¢‘é¢„å¤„ç† - æå–é¢‘è°±ç‰¹å¾
            if show_details:
                print(f"\nğŸ”§ æ­¥éª¤1: éŸ³é¢‘é¢„å¤„ç†")
            
            preprocess_start = time.time()
            
            # ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„é¢„å¤„ç†æµç¨‹
            spectrogram_features = self.preprocessor.process(audio_path)
            
            preprocess_time = time.time() - preprocess_start
            
            if show_details:
                print(f"  âœ… é¢‘è°±æå–å®Œæˆ")
                print(f"  ğŸ“Š é¢‘è°±å½¢çŠ¶: {spectrogram_features.shape}")
                print(f"  ğŸ“Š æ•°æ®ç±»å‹: {spectrogram_features.dtype}")
                print(f"  ğŸ“Š æ•°å€¼èŒƒå›´: [{spectrogram_features.min():.3f}, {spectrogram_features.max():.3f}]")
                print(f"  â±ï¸  é¢„å¤„ç†è€—æ—¶: {preprocess_time:.3f}ç§’")
            
            # æ­¥éª¤3: æ¨¡å‹æ¨ç†
            if show_details:
                print(f"\nğŸ§  æ­¥éª¤2: æ¨¡å‹æ¨ç†")
            
            inference_start = time.time()
            
            # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
            spectrogram_tensor = torch.FloatTensor(spectrogram_features).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                # ç¼–ç é˜¶æ®µ: é¢‘è°± -> éšè—è¡¨ç¤º
                encoder_output = self.model.encode(spectrogram_tensor)
                
                if show_details:
                    print(f"  ğŸ” ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {encoder_output.shape}")
                
                # è§£ç é˜¶æ®µ: éšè—è¡¨ç¤º -> æ–‡æœ¬åºåˆ—
                # ä½¿ç”¨è´ªå©ªè§£ç 
                decoded_sequence = self._greedy_decode(encoder_output)
                
                # ä¹Ÿå¯ä»¥ä½¿ç”¨æŸæœç´¢ (æ›´é«˜è´¨é‡ä½†æ›´æ…¢)
                beam_sequence, beam_score = self._beam_search_decode(encoder_output)
                
                if show_details:
                    print(f"  ğŸ” è´ªå©ªè§£ç åºåˆ—: {decoded_sequence.tolist()}")
                    print(f"  ğŸ” æŸæœç´¢åºåˆ—: {beam_sequence.tolist()}")
            
            inference_time = time.time() - inference_start
            
            # æ­¥éª¤4: åºåˆ—è§£ç ä¸ºæ–‡æœ¬
            if show_details:
                print(f"\nğŸ“ æ­¥éª¤3: æ–‡æœ¬è§£ç ")
            
            # å°†tokenåºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬
            greedy_text = vocab.decode(decoded_sequence.tolist())
            beam_text = vocab.decode(beam_sequence.tolist())
            
            if show_details:
                print(f"  ğŸ”¤ è´ªå©ªè§£ç æ–‡æœ¬: '{greedy_text}'")
                print(f"  ğŸ”¤ æŸæœç´¢æ–‡æœ¬: '{beam_text}' (å¾—åˆ†: {beam_score:.3f})")
            
            # é€‰æ‹©æœ€ç»ˆç»“æœ (å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©è´ªå©ªæˆ–æŸæœç´¢)
            final_text = beam_text if beam_text and len(beam_text.strip()) > 0 else greedy_text
            
            total_time = time.time() - start_time
            
            if show_details:
                print(f"\nğŸ¯ æœ€ç»ˆç»“æœ:")
                print(f"  è¯†åˆ«æ–‡æœ¬: '{final_text}'")
                print(f"  â±ï¸  æ€»è€—æ—¶: {total_time:.3f}ç§’")
                print(f"    - é¢„å¤„ç†: {preprocess_time:.3f}ç§’")
                print(f"    - æ¨ç†: {inference_time:.3f}ç§’")
            
            return {
                'text': final_text,
                'greedy_text': greedy_text,
                'beam_text': beam_text,
                'beam_score': beam_score,
                'processing_time': {
                    'total': total_time,
                    'preprocessing': preprocess_time,
                    'inference': inference_time
                },
                'spectrogram_shape': spectrogram_features.shape,
                'success': True
            }
            
        except Exception as e:
            if show_details:
                print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            
            return {
                'text': '',
                'success': False,
                'error': str(e)
            }
    
    def _greedy_decode(self, encoder_output, max_length=10):
        """è´ªå©ªè§£ç """
        decoded_seq = torch.LongTensor([[vocab.get_sos_idx()]]).to(self.device)
        
        for step in range(max_length):
            output = self.model.decode_step(decoded_seq, encoder_output)
            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)
            decoded_seq = torch.cat([decoded_seq, next_token], dim=1)
            
            if next_token.item() == vocab.get_eos_idx():
                break
        
        return decoded_seq.squeeze(0)
    
    def _beam_search_decode(self, encoder_output, beam_size=3, max_length=10):
        """æŸæœç´¢è§£ç """
        device = encoder_output.device
        beams = [(torch.LongTensor([[vocab.get_sos_idx()]]).to(device), 0.0)]
        
        for step in range(max_length):
            new_beams = []
            
            for seq, score in beams:
                if seq[0, -1].item() == vocab.get_eos_idx():
                    new_beams.append((seq, score))
                    continue
                
                output = self.model.decode_step(seq, encoder_output)
                logits = output[:, -1, :]
                
                # å¯¹è¿‡æ—©EOSæ·»åŠ æƒ©ç½š
                if seq.size(1) < 3:
                    logits[0, vocab.get_eos_idx()] -= 1.0
                
                probs = torch.softmax(logits, dim=-1)
                top_probs, top_indices = torch.topk(probs, beam_size)
                
                for i in range(beam_size):
                    new_seq = torch.cat([seq, top_indices[:, i:i + 1]], dim=1)
                    new_score = score + torch.log(top_probs[:, i]).item()
                    new_beams.append((new_seq, new_score))
            
            new_beams.sort(key=lambda x: x[1], reverse=True)
            beams = new_beams[:beam_size]
            
            if all(seq[0, -1].item() == vocab.get_eos_idx() for seq, _ in beams):
                break
        
        best_seq, best_score = beams[0]
        return best_seq.squeeze(0), best_score
    
    def batch_process_directory(self, audio_dir, output_file=None):
        """æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶"""
        print(f"\nğŸ“ æ‰¹é‡å¤„ç†ç›®å½•: {audio_dir}")
        
        # æ”¯æŒçš„éŸ³é¢‘æ ¼å¼
        audio_extensions = ['.wav', '.mp3', '.flac', '.m4a', '.ogg']
        
        # æ‰¾åˆ°æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
        audio_files = []
        for ext in audio_extensions:
            audio_files.extend(Path(audio_dir).glob(f"*{ext}"))
            audio_files.extend(Path(audio_dir).glob(f"*{ext.upper()}"))
        
        if not audio_files:
            print(f"âŒ åœ¨ {audio_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return []
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        
        results = []
        for i, audio_file in enumerate(audio_files):
            print(f"\nå¤„ç† {i+1}/{len(audio_files)}: {audio_file.name}")
            
            result = self.process_new_audio(str(audio_file), show_details=False)
            result['filename'] = audio_file.name
            results.append(result)
            
            if result['success']:
                print(f"  âœ… '{result['text']}' ({result['processing_time']['total']:.2f}s)")
            else:
                print(f"  âŒ å¤±è´¥: {result['error']}")
        
        # ä¿å­˜ç»“æœ
        if output_file:
            import pandas as pd
            df = pd.DataFrame(results)
            df.to_csv(output_file, index=False, encoding='utf-8')
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç»Ÿè®¡
        success_count = sum(1 for r in results if r['success'])
        avg_time = np.mean([r['processing_time']['total'] for r in results if r['success']])
        
        print(f"\nğŸ“Š æ‰¹é‡å¤„ç†ç»Ÿè®¡:")
        print(f"  æˆåŠŸ: {success_count}/{len(results)}")
        print(f"  å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’/æ–‡ä»¶")
        
        return results


def create_test_audio_examples():
    """åˆ›å»ºæµ‹è¯•éŸ³é¢‘æ–‡ä»¶ç¤ºä¾‹"""
    print("ğŸµ åˆ›å»ºæµ‹è¯•éŸ³é¢‘æ–‡ä»¶ç¤ºä¾‹")
    
    # è¿™é‡Œå¯ä»¥åˆ›å»ºä¸€äº›æµ‹è¯•éŸ³é¢‘æ–‡ä»¶
    # å®é™…ä½¿ç”¨æ—¶ï¼Œç”¨æˆ·ä¼šæä¾›çœŸå®çš„éŸ³é¢‘æ–‡ä»¶
    test_dir = "test_audio"
    os.makedirs(test_dir, exist_ok=True)
    
    print(f"ğŸ“ æµ‹è¯•ç›®å½•: {test_dir}")
    print("ğŸ’¡ è¯·å°†æ‚¨çš„éŸ³é¢‘æ–‡ä»¶æ”¾å…¥æ­¤ç›®å½•è¿›è¡Œæµ‹è¯•")
    
    return test_dir


def demo_complete_workflow():
    """æ¼”ç¤ºå®Œæ•´çš„å·¥ä½œæµç¨‹"""
    print("ğŸ¯ ç”Ÿäº§ç¯å¢ƒæ¨ç†å®Œæ•´æµç¨‹æ¼”ç¤º")
    print("=" * 80)
    
    # æ¨¡æ‹Ÿåœºæ™¯ï¼šç”¨æˆ·æœ‰ä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæƒ³è¦è¯†åˆ«æ–°çš„éŸ³é¢‘æ–‡ä»¶
    
    print("ğŸ“‹ åœºæ™¯æè¿°:")
    print("  - æ‚¨å·²ç»è®­ç»ƒå¥½äº†ä¸€ä¸ªè¯­éŸ³è¯†åˆ«æ¨¡å‹")
    print("  - ç°åœ¨æœ‰æ–°çš„éŸ³é¢‘æ–‡ä»¶éœ€è¦è¯†åˆ«")
    print("  - è¿™äº›éŸ³é¢‘æ–‡ä»¶ä¸åœ¨åŸå§‹è®­ç»ƒæ•°æ®ä¸­")
    print("  - éœ€è¦è·å¾—è¯†åˆ«ç»“æœ")
    
    print(f"\nğŸ”§ ç³»ç»Ÿå·¥ä½œæµç¨‹:")
    print("  1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("  2. å¯¹æ–°éŸ³é¢‘è¿›è¡Œé¢„å¤„ç† (æå–é¢‘è°±ç‰¹å¾)")
    print("  3. ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†")
    print("  4. è§£ç å¾—åˆ°æ–‡æœ¬ç»“æœ")
    
    # å®é™…æ¼”ç¤ºä»£ç 
    workflow_code = '''
# å®Œæ•´çš„ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æµç¨‹

# 1. åˆå§‹åŒ–è¯†åˆ«å™¨
recognizer = ProductionSpeechRecognizer(
    model_path="checkpoints/best_model.pth",  # è®­ç»ƒå¥½çš„æ¨¡å‹
    device="cpu"  # æˆ– "cuda"
)

# 2. å¤„ç†å•ä¸ªæ–°éŸ³é¢‘æ–‡ä»¶
result = recognizer.process_new_audio("path/to/new_audio.wav")

if result['success']:
    print(f"è¯†åˆ«ç»“æœ: {result['text']}")
    print(f"å¤„ç†æ—¶é—´: {result['processing_time']['total']:.3f}ç§’")
else:
    print(f"è¯†åˆ«å¤±è´¥: {result['error']}")

# 3. æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
results = recognizer.batch_process_directory(
    audio_dir="path/to/audio_directory",
    output_file="recognition_results.csv"
)
    '''
    
    print(f"\nğŸ’» ä½¿ç”¨ä»£ç ç¤ºä¾‹:")
    print(workflow_code)


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description='ç”Ÿäº§ç¯å¢ƒè¯­éŸ³è¯†åˆ«æ¼”ç¤º')
    parser.add_argument('--model', type=str, required=True, help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--audio', type=str, help='å•ä¸ªéŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--audio_dir', type=str, help='éŸ³é¢‘æ–‡ä»¶ç›®å½•')
    parser.add_argument('--output', type=str, help='è¾“å‡ºç»“æœæ–‡ä»¶')
    parser.add_argument('--device', type=str, default='cpu', help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--demo', action='store_true', help='æ˜¾ç¤ºå®Œæ•´æµç¨‹æ¼”ç¤º')
    
    args = parser.parse_args()
    
    if args.demo:
        demo_complete_workflow()
        return
    
    try:
        # åˆ›å»ºè¯†åˆ«å™¨
        recognizer = ProductionSpeechRecognizer(args.model, args.device)
        
        if args.audio:
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            result = recognizer.process_new_audio(args.audio, show_details=True)
            
            if result['success']:
                print(f"\nğŸ‰ è¯†åˆ«æˆåŠŸ!")
                print(f"æœ€ç»ˆç»“æœ: '{result['text']}'")
            else:
                print(f"\nâŒ è¯†åˆ«å¤±è´¥: {result['error']}")
        
        elif args.audio_dir:
            # æ‰¹é‡å¤„ç†
            results = recognizer.batch_process_directory(args.audio_dir, args.output)
            
            print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
            
        else:
            print("è¯·æŒ‡å®š --audio æˆ– --audio_dir å‚æ•°")
            parser.print_help()
    
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()