#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§è®­ç»ƒè„šæœ¬
é›†æˆå­¦ä¹ ç‡è°ƒåº¦ã€éªŒè¯é›†åˆ†å‰²ã€æ›´å¥½çš„ç›‘æ§ç­‰æ”¹è¿›
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import argparse
import json
from datetime import datetime
from sklearn.model_selection import train_test_split
import pandas as pd

from model import create_model
from data_utils import get_dataloader
from vocab import vocab


class AdvancedTrainer:
    """é«˜çº§è®­ç»ƒå™¨ - é›†æˆå¤šç§æ”¹è¿›"""
    
    def __init__(self, model, train_loader, val_loader, device, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.config = config
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(  # ä½¿ç”¨AdamWæ›¿ä»£Adam
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay'],
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½™å¼¦é€€ç«
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=config.get('scheduler_T0', 10),  # é‡å¯å‘¨æœŸ
            T_mult=config.get('scheduler_T_mult', 2),  # å‘¨æœŸå€æ•°
            eta_min=config.get('scheduler_eta_min', 1e-6)  # æœ€å°å­¦ä¹ ç‡
        )
        
        # å­¦ä¹ ç‡é¢„çƒ­
        self.warmup_epochs = config.get('warmup_epochs', 5)
        self.warmup_scheduler = optim.lr_scheduler.LinearLR(
            self.optimizer,
            start_factor=0.1,
            total_iters=self.warmup_epochs
        )
        
        # æŸå¤±å‡½æ•° - æ ‡ç­¾å¹³æ»‘
        self.criterion = nn.CrossEntropyLoss(
            ignore_index=vocab.get_padding_idx(),
            label_smoothing=config.get('label_smoothing', 0.1)
        )
        
        # æ—¥å¿—å’Œç›‘æ§
        self.writer = SummaryWriter(f"runs/{config['experiment_name']}")
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.patience_counter = 0
        self.train_losses = []
        self.val_losses = []
        self.val_accuracies = []
        
        # æ¢¯åº¦ç›‘æ§
        self.log_gradients = config.get('log_gradients', False)
        
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_correct = 0
        total_tokens = 0
        num_batches = len(self.train_loader)
        
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch + 1}')
        
        for batch_idx, batch in enumerate(progress_bar):
            # å…¼å®¹æ–°æ—§æ¥å£
            if 'features' in batch:
                spectrograms = batch['features'].to(self.device)
            else:
                spectrograms = batch['spectrograms'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
            tgt_input = labels[:, :-1]
            tgt_output = labels[:, 1:]
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(spectrograms, tgt_input)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(
                outputs.reshape(-1, outputs.size(-1)),
                tgt_output.reshape(-1)
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config['grad_clip']
            )
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            predictions = outputs.argmax(dim=-1)
            mask = (tgt_output != vocab.get_padding_idx())
            correct = (predictions == tgt_output) & mask
            total_correct += correct.sum().item()
            total_tokens += mask.sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            current_acc = total_correct / total_tokens if total_tokens > 0 else 0
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{current_acc:.3f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}',
                'grad_norm': f'{grad_norm:.3f}'
            })
            
            # è®°å½•åˆ°tensorboard
            global_step = epoch * num_batches + batch_idx
            self.writer.add_scalar('Train/Loss_Step', loss.item(), global_step)
            self.writer.add_scalar('Train/Learning_Rate', self.optimizer.param_groups[0]['lr'], global_step)
            self.writer.add_scalar('Train/Gradient_Norm', grad_norm, global_step)
            
            # æ¢¯åº¦ç›‘æ§
            if self.log_gradients and batch_idx % 10 == 0:
                self._log_gradients(global_step)
        
        avg_loss = total_loss / num_batches
        accuracy = total_correct / total_tokens if total_tokens > 0 else 0
        
        return avg_loss, accuracy
    
    def validate_epoch(self, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0
        total_correct = 0
        total_tokens = 0
        num_batches = len(self.val_loader)
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                # å…¼å®¹æ–°æ—§æ¥å£
                if 'features' in batch:
                    spectrograms = batch['features'].to(self.device)
                else:
                    spectrograms = batch['spectrograms'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                tgt_input = labels[:, :-1]
                tgt_output = labels[:, 1:]
                
                outputs = self.model(spectrograms, tgt_input)
                loss = self.criterion(
                    outputs.reshape(-1, outputs.size(-1)),
                    tgt_output.reshape(-1)
                )
                
                total_loss += loss.item()
                predictions = outputs.argmax(dim=-1)
                mask = (tgt_output != vocab.get_padding_idx())
                correct = (predictions == tgt_output) & mask
                total_correct += correct.sum().item()
                total_tokens += mask.sum().item()
        
        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
        accuracy = total_correct / total_tokens if total_tokens > 0 else 0
        
        return avg_loss, accuracy
    
    def _log_gradients(self, step):
        """è®°å½•æ¢¯åº¦ä¿¡æ¯"""
        total_norm = 0
        param_count = 0
        
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
                
                # è®°å½•æ¯å±‚çš„æ¢¯åº¦èŒƒæ•°
                self.writer.add_scalar(f'Gradients/{name}', param_norm, step)
        
        total_norm = total_norm ** (1. / 2)
        self.writer.add_scalar('Gradients/Total_Norm', total_norm, step)
    
    def save_checkpoint(self, epoch, val_loss, val_acc, filename):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'best_val_acc': self.best_val_acc,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'config': self.config
        }
        
        torch.save(checkpoint, filename)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")
    
    def train(self, num_epochs):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"å¼€å§‹é«˜çº§è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters())}")
        print(f"è®­ç»ƒé›†å¤§å°: {len(self.train_loader.dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(self.val_loader.dataset)}")
        
        for epoch in range(num_epochs):
            # å­¦ä¹ ç‡è°ƒåº¦
            if epoch < self.warmup_epochs:
                self.warmup_scheduler.step()
            else:
                self.scheduler.step()
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            self.train_losses.append(train_loss)
            
            # éªŒè¯
            val_loss, val_acc = self.validate_epoch(epoch)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            
            # è®°å½•åˆ°tensorboard
            self.writer.add_scalar('Train/Loss_Epoch', train_loss, epoch)
            self.writer.add_scalar('Train/Accuracy', train_acc, epoch)
            self.writer.add_scalar('Validation/Loss', val_loss, epoch)
            self.writer.add_scalar('Validation/Accuracy', val_acc, epoch)
            
            print(f"Epoch {epoch + 1}/{num_epochs}:")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.3f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.3f}")
            print(f"  å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.best_val_loss = val_loss
                self.patience_counter = 0
                self.save_checkpoint(epoch, val_loss, val_acc, "checkpoints/best_model.pth")
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {val_acc:.3f}")
            else:
                self.patience_counter += 1
            
            # å®šæœŸä¿å­˜
            if (epoch + 1) % self.config['save_every'] == 0:
                self.save_checkpoint(epoch, val_loss, val_acc, f"checkpoints/checkpoint_epoch_{epoch + 1}.pth")
            
            # æ—©åœæ£€æŸ¥
            patience = self.config.get('patience', 20)
            if self.patience_counter >= patience:
                print(f"éªŒè¯å‡†ç¡®ç‡è¿ç»­ {patience} ä¸ªepochæœªæ”¹å–„ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                break
        
        print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.3f}")
        self.writer.close()


def create_train_val_split(labels_file, val_ratio=0.2, random_state=42):
    """åˆ›å»ºè®­ç»ƒéªŒè¯é›†åˆ†å‰²"""
    df = pd.read_csv(labels_file)
    
    # æŒ‰ç±»åˆ«åˆ†å±‚åˆ†å‰²
    train_df, val_df = train_test_split(
        df, 
        test_size=val_ratio,
        random_state=random_state,
        stratify=df['label']
    )
    
    # ä¿å­˜åˆ†å‰²åçš„æ–‡ä»¶
    train_file = labels_file.replace('.csv', '_train.csv')
    val_file = labels_file.replace('.csv', '_val.csv')
    
    train_df.to_csv(train_file, index=False, encoding='utf-8')
    val_df.to_csv(val_file, index=False, encoding='utf-8')
    
    print(f"æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬ -> {train_file}")
    print(f"  éªŒè¯é›†: {len(val_df)} æ ·æœ¬ -> {val_file}")
    
    return train_file, val_file


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é«˜çº§è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--val_ratio', type=float, default=0.2, help='éªŒè¯é›†æ¯”ä¾‹')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    print("ğŸš€ é«˜çº§è®­ç»ƒè„šæœ¬")
    print("=" * 60)
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"å®éªŒåç§°: {config['experiment_name']}")
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºç›®å½•
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('runs', exist_ok=True)
    
    try:
        # åˆ›å»ºè®­ç»ƒéªŒè¯é›†åˆ†å‰²
        train_labels_file, val_labels_file = create_train_val_split(
            config['labels_file'], 
            val_ratio=args.val_ratio
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = get_dataloader(
            audio_dir=config['audio_dir'],
            labels_file=train_labels_file,
            batch_size=config['batch_size'],
            shuffle=True,
            mode='auto'
        )
        
        val_loader = get_dataloader(
            audio_dir=config['audio_dir'],
            labels_file=val_labels_file,
            batch_size=config['batch_size'],
            shuffle=False,
            mode='auto'
        )
        
        # åˆ›å»ºæ¨¡å‹
        model = create_model(
            vocab_size=vocab.vocab_size,
            hidden_dim=config['hidden_dim'],
            encoder_layers=config['encoder_layers'],
            decoder_layers=config['decoder_layers'],
            dropout=config['dropout']
        ).to(device)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = AdvancedTrainer(model, train_loader, val_loader, device, config)
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train(config['num_epochs'])
        
    except KeyboardInterrupt:
        print("\nè®­ç»ƒè¢«ä¸­æ–­")
        trainer.save_checkpoint(-1, float('inf'), 0, "checkpoints/interrupted.pth")
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()