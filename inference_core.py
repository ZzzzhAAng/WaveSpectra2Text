#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ¨ç†æ ¸å¿ƒæ¨¡å—
è§£å†³inference.pyå’Œdual_input_inference.pyä¹‹é—´çš„ä»£ç å†—ä½™
æä¾›ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½ã€è§£ç ç®—æ³•å’Œæ¨ç†æ¥å£
"""

import os
import torch
import numpy as np
from typing import Dict, Tuple, Optional, Union
from pathlib import Path

from model import create_model
from vocab import vocab
from common_utils import AudioProcessor


class InferenceCore:
    """ç»Ÿä¸€çš„æ¨ç†æ ¸å¿ƒç±» - åŒ…å«æ‰€æœ‰å…±åŒçš„æ¨ç†é€»è¾‘"""
    
    def __init__(self, model_path: str, device: str = 'cpu'):
        """
        åˆå§‹åŒ–æ¨ç†æ ¸å¿ƒ
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = torch.device(device)
        self.model = self._load_model(model_path)
        self.model.eval()
        
        # éŸ³é¢‘å¤„ç†å‚æ•°
        self.audio_config = {
            'sample_rate': 48000,
            'n_fft': 1024,
            'hop_length': 512,
            'max_length': 200
        }
        
        # åˆ›å»ºéŸ³é¢‘å¤„ç†å™¨
        self.audio_processor = AudioProcessor(**self.audio_config)
    
    def _load_model(self, model_path: str) -> torch.nn.Module:
        """
        ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½æ–¹æ³•
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            model: åŠ è½½çš„æ¨¡å‹
        """
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        config = checkpoint.get('config', {})
        
        model = create_model(
            vocab_size=vocab.vocab_size,
            hidden_dim=config.get('hidden_dim', 256),
            encoder_layers=config.get('encoder_layers', 4),
            decoder_layers=config.get('decoder_layers', 4),
            dropout=config.get('dropout', 0.1)
        )
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        
        # å­˜å‚¨æ¨¡å‹ä¿¡æ¯
        self.model_info = {
            'path': model_path,
            'epoch': checkpoint.get('epoch', 'Unknown'),
            'best_val_loss': checkpoint.get('best_val_loss', 'Unknown'),
            'config': config
        }
        
        return model
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.model_info.copy()
    
    def extract_spectrogram_from_audio(self, audio_path: Union[str, Path]) -> np.ndarray:
        """
        ä»éŸ³é¢‘æ–‡ä»¶æå–é¢‘è°±ç‰¹å¾
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            spectrogram: é¢‘è°±ç‰¹å¾
        """
        return self.audio_processor.extract_spectrogram(audio_path)
    
    def greedy_decode(self, encoder_output: torch.Tensor, max_length: int = 10) -> torch.Tensor:
        """
        è´ªå©ªè§£ç ç®—æ³•
        
        Args:
            encoder_output: ç¼–ç å™¨è¾“å‡º
            max_length: æœ€å¤§è§£ç é•¿åº¦
            
        Returns:
            decoded_seq: è§£ç åºåˆ—
        """
        decoded_seq = torch.LongTensor([[vocab.get_sos_idx()]]).to(self.device)
        
        for step in range(max_length):
            with torch.no_grad():
                output = self.model.decode_step(decoded_seq, encoder_output)
                next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)
                decoded_seq = torch.cat([decoded_seq, next_token], dim=1)
                
                if next_token.item() == vocab.get_eos_idx():
                    break
        
        return decoded_seq.squeeze(0)
    
    def beam_search_decode(self, encoder_output: torch.Tensor, 
                          beam_size: int = 3, max_length: int = 10,
                          use_length_penalty: bool = True) -> Tuple[torch.Tensor, float]:
        """
        æŸæœç´¢è§£ç ç®—æ³•
        
        Args:
            encoder_output: ç¼–ç å™¨è¾“å‡º
            beam_size: æŸæœç´¢å¤§å°
            max_length: æœ€å¤§è§£ç é•¿åº¦
            use_length_penalty: æ˜¯å¦ä½¿ç”¨é•¿åº¦æƒ©ç½š
            
        Returns:
            best_seq: æœ€ä½³åºåˆ—
            best_score: æœ€ä½³å¾—åˆ†
        """
        device = encoder_output.device
        beams = [(torch.LongTensor([[vocab.get_sos_idx()]]).to(device), 0.0)]
        
        for step in range(max_length):
            new_beams = []
            
            for seq, score in beams:
                if seq[0, -1].item() == vocab.get_eos_idx():
                    # å¯¹è¿‡çŸ­åºåˆ—æ·»åŠ æƒ©ç½š
                    if use_length_penalty and seq.size(1) < 3:
                        score -= 1.0
                    new_beams.append((seq, score))
                    continue
                
                with torch.no_grad():
                    output = self.model.decode_step(seq, encoder_output)
                    logits = output[:, -1, :]
                    
                    # å¯¹è¿‡æ—©çš„EOSæ·»åŠ æƒ©ç½š
                    if use_length_penalty and seq.size(1) < 3:
                        logits[0, vocab.get_eos_idx()] -= 1.0
                    
                    probs = torch.softmax(logits, dim=-1)
                    top_probs, top_indices = torch.topk(probs, beam_size)
                    
                    for i in range(beam_size):
                        new_seq = torch.cat([seq, top_indices[:, i:i + 1]], dim=1)
                        new_score = score + torch.log(top_probs[:, i]).item()
                        
                        # é•¿åº¦å¥–åŠ±
                        if use_length_penalty and top_indices[:, i].item() != vocab.get_eos_idx():
                            new_score += 0.1
                        
                        new_beams.append((new_seq, new_score))
            
            new_beams.sort(key=lambda x: x[1], reverse=True)
            beams = new_beams[:beam_size]
            
            if all(seq[0, -1].item() == vocab.get_eos_idx() for seq, _ in beams):
                break
        
        best_seq, best_score = beams[0]
        return best_seq.squeeze(0), best_score
    
    def infer_from_spectrogram(self, spectrogram: np.ndarray, 
                              method: str = 'auto', 
                              beam_size: int = 3) -> Dict:
        """
        ä»é¢‘è°±ç‰¹å¾è¿›è¡Œæ¨ç†
        
        Args:
            spectrogram: é¢‘è°±ç‰¹å¾
            method: è§£ç æ–¹æ³• ('greedy', 'beam', 'auto')
            beam_size: æŸæœç´¢å¤§å°
            
        Returns:
            result: æ¨ç†ç»“æœå­—å…¸
        """
        import time
        
        start_time = time.time()
        
        # è½¬æ¢ä¸ºtensor
        if isinstance(spectrogram, np.ndarray):
            spectrogram_tensor = torch.FloatTensor(spectrogram).unsqueeze(0).to(self.device)
        else:
            spectrogram_tensor = spectrogram.to(self.device)
        
        with torch.no_grad():
            # ç¼–ç 
            encoder_output = self.model.encode(spectrogram_tensor)
            
            # è§£ç 
            if method == 'greedy':
                seq = self.greedy_decode(encoder_output)
                text = vocab.decode(seq.tolist())
                
                result = {
                    'text': text,
                    'method': 'greedy',
                    'success': True
                }
                
            elif method == 'beam':
                seq, score = self.beam_search_decode(encoder_output, beam_size)
                text = vocab.decode(seq.tolist())
                
                result = {
                    'text': text,
                    'method': 'beam_search',
                    'score': score,
                    'success': True
                }
                
            else:  # method == 'auto'
                # æ™ºèƒ½é€‰æ‹©ï¼šå…ˆå°è¯•æŸæœç´¢
                beam_seq, beam_score = self.beam_search_decode(encoder_output, beam_size)
                beam_text = vocab.decode(beam_seq.tolist())
                
                # å¦‚æœæŸæœç´¢ç»“æœåˆç†ï¼Œä½¿ç”¨å®ƒ
                if beam_text and len(beam_text.strip()) > 0:
                    result = {
                        'text': beam_text,
                        'method': 'beam_search',
                        'score': beam_score,
                        'success': True
                    }
                else:
                    # å¦åˆ™å›é€€åˆ°è´ªå©ªè§£ç 
                    greedy_seq = self.greedy_decode(encoder_output)
                    greedy_text = vocab.decode(greedy_seq.tolist())
                    
                    result = {
                        'text': greedy_text,
                        'method': 'greedy_fallback',
                        'success': True,
                        'note': 'æŸæœç´¢ä¸ºç©ºï¼Œä½¿ç”¨è´ªå©ªè§£ç '
                    }
        
        # æ·»åŠ æ¨ç†æ—¶é—´
        inference_time = time.time() - start_time
        result.update({
            'inference_time': inference_time,
            'spectrogram_shape': spectrogram.shape if isinstance(spectrogram, np.ndarray) else spectrogram_tensor.shape
        })
        
        return result
    
    def infer_from_audio(self, audio_path: Union[str, Path], 
                        method: str = 'auto', 
                        beam_size: int = 3) -> Dict:
        """
        ä»éŸ³é¢‘æ–‡ä»¶è¿›è¡Œæ¨ç†
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            method: è§£ç æ–¹æ³•
            beam_size: æŸæœç´¢å¤§å°
            
        Returns:
            result: æ¨ç†ç»“æœå­—å…¸
        """
        import time
        
        start_time = time.time()
        
        try:
            # æå–é¢‘è°±ç‰¹å¾
            preprocess_start = time.time()
            spectrogram = self.extract_spectrogram_from_audio(audio_path)
            preprocess_time = time.time() - preprocess_start
            
            # è¿›è¡Œæ¨ç†
            result = self.infer_from_spectrogram(spectrogram, method, beam_size)
            
            # æ·»åŠ é¢„å¤„ç†æ—¶é—´
            result.update({
                'preprocessing_time': preprocess_time,
                'total_time': time.time() - start_time,
                'input_type': 'audio_file',
                'audio_path': str(audio_path)
            })
            
            return result
            
        except Exception as e:
            return {
                'text': '',
                'success': False,
                'error': str(e),
                'input_type': 'audio_file',
                'audio_path': str(audio_path)
            }


class BatchInference:
    """æ‰¹é‡æ¨ç†å·¥å…·ç±»"""
    
    def __init__(self, inference_core: InferenceCore):
        """
        åˆå§‹åŒ–æ‰¹é‡æ¨ç†
        
        Args:
            inference_core: æ¨ç†æ ¸å¿ƒå®ä¾‹
        """
        self.core = inference_core
    
    def infer_audio_batch(self, audio_paths: list, 
                         method: str = 'auto', 
                         beam_size: int = 3,
                         show_progress: bool = True) -> list:
        """
        æ‰¹é‡éŸ³é¢‘æ¨ç†
        
        Args:
            audio_paths: éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            method: è§£ç æ–¹æ³•
            beam_size: æŸæœç´¢å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            results: æ¨ç†ç»“æœåˆ—è¡¨
        """
        results = []
        
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(audio_paths, desc="æ‰¹é‡æ¨ç†")
            except ImportError:
                iterator = audio_paths
                print(f"å¼€å§‹æ‰¹é‡æ¨ç† {len(audio_paths)} ä¸ªæ–‡ä»¶...")
        else:
            iterator = audio_paths
        
        for audio_path in iterator:
            result = self.core.infer_from_audio(audio_path, method, beam_size)
            result['file'] = str(audio_path)
            results.append(result)
        
        return results
    
    def infer_spectrogram_batch(self, spectrograms: list, 
                               method: str = 'auto', 
                               beam_size: int = 3,
                               show_progress: bool = True) -> list:
        """
        æ‰¹é‡é¢‘è°±æ¨ç†
        
        Args:
            spectrograms: é¢‘è°±ç‰¹å¾åˆ—è¡¨æˆ–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            method: è§£ç æ–¹æ³•
            beam_size: æŸæœç´¢å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            results: æ¨ç†ç»“æœåˆ—è¡¨
        """
        results = []
        
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(spectrograms, desc="æ‰¹é‡æ¨ç†")
            except ImportError:
                iterator = spectrograms
                print(f"å¼€å§‹æ‰¹é‡æ¨ç† {len(spectrograms)} ä¸ªé¢‘è°±...")
        else:
            iterator = spectrograms
        
        for i, spectrogram in enumerate(iterator):
            if isinstance(spectrogram, (str, Path)):
                # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼ŒåŠ è½½é¢‘è°±
                spectrogram_data = np.load(spectrogram)
                result = self.core.infer_from_spectrogram(spectrogram_data, method, beam_size)
                result['spectrogram_file'] = str(spectrogram)
            else:
                # å¦‚æœæ˜¯numpyæ•°ç»„
                result = self.core.infer_from_spectrogram(spectrogram, method, beam_size)
                result['spectrogram_index'] = i
            
            results.append(result)
        
        return results


# ä¾¿æ·å‡½æ•°
def create_inference_core(model_path: str, device: str = 'cpu') -> InferenceCore:
    """åˆ›å»ºæ¨ç†æ ¸å¿ƒå®ä¾‹"""
    return InferenceCore(model_path, device)


def quick_infer_audio(model_path: str, audio_path: str, 
                     method: str = 'auto', device: str = 'cpu') -> str:
    """å¿«é€ŸéŸ³é¢‘æ¨ç† - è¿”å›è¯†åˆ«æ–‡æœ¬"""
    core = InferenceCore(model_path, device)
    result = core.infer_from_audio(audio_path, method)
    return result.get('text', '')


def quick_infer_spectrogram(model_path: str, spectrogram: np.ndarray, 
                           method: str = 'auto', device: str = 'cpu') -> str:
    """å¿«é€Ÿé¢‘è°±æ¨ç† - è¿”å›è¯†åˆ«æ–‡æœ¬"""
    core = InferenceCore(model_path, device)
    result = core.infer_from_spectrogram(spectrogram, method)
    return result.get('text', '')


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª ç»Ÿä¸€æ¨ç†æ ¸å¿ƒæ¨¡å—æµ‹è¯•")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿæµ‹è¯•ï¼ˆéœ€è¦å®é™…æ¨¡å‹æ–‡ä»¶ï¼‰
    print("ğŸ“‹ åŠŸèƒ½åˆ—è¡¨:")
    print("  âœ… ç»Ÿä¸€æ¨¡å‹åŠ è½½")
    print("  âœ… è´ªå©ªè§£ç ç®—æ³•")
    print("  âœ… æŸæœç´¢è§£ç ç®—æ³•")
    print("  âœ… éŸ³é¢‘æ–‡ä»¶æ¨ç†")
    print("  âœ… é¢‘è°±ç‰¹å¾æ¨ç†")
    print("  âœ… æ‰¹é‡æ¨ç†æ”¯æŒ")
    print("  âœ… æ™ºèƒ½è§£ç ç­–ç•¥")
    
    print("\nğŸ’¡ ä½¿ç”¨æ–¹å¼:")
    print("from inference_core import InferenceCore")
    print("core = InferenceCore('model.pth')")
    print("result = core.infer_from_audio('audio.wav')")
    print("print(result['text'])")